{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook hopefully (I'm only human) contains every step needed to generate intron-exon data because it is way too big for github.  It is a Frankenstein's monster of a notebook in that the code in this comes from a bunch of different notebooks so it's possible I missed something somewhere.  A lot of style choices are not terribly consistent because of that, too.\n",
    "\n",
    "GTF and Fasta files can be found on this page: https://www.gencodegenes.org/human/\n",
    "\n",
    "GTF file is the 'Basic gene annotation' with CHR as the 'regions' and can be direct downloaded here: https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_47/gencode.v47.basic.annotation.gtf.gz\n",
    "\n",
    "I unzipped and saved it as 'basic_annotations.gtf'\n",
    "\n",
    "The fasta file is 'Genome sequence (GRCh38.p14)' with ALL as the 'regions' and can be direct downloaded here: https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_47/GRCh38.p14.genome.fa.gz\n",
    "\n",
    "I unzipped and saved it as 'chr_genome.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 01:10:21.974832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-17 01:10:21.993167: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-17 01:10:21.998951: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-17 01:10:22.013136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 01:10:22.742385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes out a new fasta that uses the full sequences of the 22 numbered chromosomes and the X and Y chromosomes.  chrM has no introns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_chroms = set([f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"])\n",
    "\n",
    "input_fasta = datasets_path + \"chr_genome.fa\"  # Removed chrM because there aren't introns on it and I didn't want to confuse the training data\n",
    "output_fasta = datasets_path + \"trim_chr_genome.fa\"\n",
    "\n",
    "keep = False\n",
    "'''\n",
    "Removes non-chromosome entries in the human genome data\n",
    "If line is a record key (starswith(\">\")), it compares first 'word' in the record key to valid_chroms\n",
    "If comparison is true, it keeps writing lines until it finds another record key line to make the comparison\n",
    "'''\n",
    "with open(input_fasta, \"r\") as fin, open(output_fasta, \"w\") as fout:\n",
    "    for line in fin:\n",
    "        if line.startswith(\">\"):\n",
    "            chrom_name = line.strip()[1:].split()[0]\n",
    "            keep = (chrom_name in valid_chroms)\n",
    "\n",
    "            if keep:\n",
    "                fout.write(line)\n",
    "                print(chrom_name)\n",
    "        else:\n",
    "            if keep:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads gtf annotations with proper 0 index numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gtf_annotations(gtf_file):\n",
    "    \"\"\"\n",
    "    Loads GTF into a pandas DataFrame and converts cstart and cend to zero-based indexing.\n",
    "    \"\"\"\n",
    "    gtf_data = pd.read_csv(\n",
    "        gtf_file, sep='\\t', comment='#', header=None,\n",
    "        names=['seqname', 'source', 'feature', 'cstart', 'cend', \n",
    "               'score', 'strand', 'frame', 'attribute']\n",
    "    )\n",
    "    # Convert to zero-based indexing for cstart\n",
    "    gtf_data['cstart'] = gtf_data['cstart'] - 1\n",
    "    return gtf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTF search function that was nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_gtf_by_range(gtf_df, seqname, pos_min, pos_max, require_both=False):\n",
    "    \"\"\"\n",
    "    Search a GTF annotations DataFrame for rows matching a given sequence name and \n",
    "    having cstart and/or cend values within a specified range.\n",
    "\n",
    "    Parameters:\n",
    "      gtf_df (pd.DataFrame): DataFrame containing GTF annotations. Must include at least \n",
    "                             the columns 'seqname', 'cstart', and 'cend'.\n",
    "      seqname (str): The chromosome or scaffold name to filter by (e.g., 'chr1' or '1').\n",
    "      pos_min (int): The lower bound of the position range (inclusive).\n",
    "      pos_max (int): The upper bound of the position range (inclusive).\n",
    "      require_both (bool): \n",
    "           - If False (default), returns rows where either 'cstart' OR 'cend' falls within the range.\n",
    "           - If True, returns only rows where BOTH 'cstart' and 'cend' fall within the range.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: A DataFrame containing only the rows that match the criteria.\n",
    "    \"\"\"\n",
    "    # Filter by seqname first.\n",
    "    df = gtf_df[gtf_df['seqname'] == seqname]\n",
    "    \n",
    "    if require_both:\n",
    "        condition = (\n",
    "            (df['cstart'] >= pos_min) & (df['cstart'] <= pos_max) &\n",
    "            (df['cend']   >= pos_min) & (df['cend']   <= pos_max)\n",
    "        )\n",
    "    else:\n",
    "        condition = (\n",
    "            ((df['cstart'] >= pos_min) & (df['cstart'] <= pos_max)) |\n",
    "            ((df['cend']   >= pos_min) & (df['cend']   <= pos_max))\n",
    "        )\n",
    "        \n",
    "    return df[condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the function that calculates intron cstart and cend based on the exon cstart and cend values in the basic annotation gtf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_introns(gtf_df):\n",
    "    \"\"\"\n",
    "    Given a pandas DataFrame of GTF records (with columns: 'seqname', 'feature', 'start',\n",
    "    'end', 'strand', and 'attribute') in which the start (called cstart) has been converted\n",
    "    to 0-indexed values, calculate the introns for each gene.\n",
    "    \n",
    "    This function assumes that the attribute field contains a gene identifier in a form like:\n",
    "       gene_id \"XYZ\";\n",
    "    as it does in the hg38 gtf and groups features by gene_id. For each gene, it collects the exon \n",
    "    intervals, merges overlapping exons (forms the union of exonic regions) and then computes each intron \n",
    "    as the gap between consecutive merged exons. For plus-strand genes the intron is reported\n",
    "    as (previous_exon.end, next_exon.start), while for minus-strand genes the order is reversed so\n",
    "    that the cstart value is higher than the cend.\n",
    "    \n",
    "    Returns:\n",
    "        A new DataFrame with one row per intron, having columns:\n",
    "          - seqname\n",
    "          - feature (with value \"intron\")\n",
    "          - cstart (the start coordinate in 0-index system; note that for minus strand this is numerically higher)\n",
    "          - cend   (the end coordinate)\n",
    "          - strand\n",
    "    \"\"\"\n",
    "    \n",
    "    # helper to extract gene_id from the attribute string\n",
    "    def get_gene_id(attr):\n",
    "        # look for a pattern like: gene_id \"XYZ\";\n",
    "        m = re.search(r'gene_id\\s+\"([^\"]+)\"', attr)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Add a gene_id column (if not already present)\n",
    "    if 'gene_id' not in gtf_df.columns:\n",
    "        gtf_df = gtf_df.copy()  # avoid modifying the original dataframe\n",
    "        gtf_df['gene_id'] = gtf_df['attribute'].apply(get_gene_id)\n",
    "    \n",
    "    intron_records = []\n",
    "    \n",
    "    # Group rows by gene_id (each gene should have one gene-level record, and one or more exon records)\n",
    "    for gene_id, group in gtf_df.groupby('gene_id'):\n",
    "        # Skip groups with no gene_id (if any)\n",
    "        if gene_id is None:\n",
    "            continue\n",
    "        \n",
    "        # Identify the gene-level information (if available)\n",
    "        gene_rows = group[group['feature'] == 'gene']\n",
    "        if not gene_rows.empty:\n",
    "            # Use the gene row to get the chromosome and strand.\n",
    "            seqname = gene_rows.iloc[0]['seqname']\n",
    "            strand  = gene_rows.iloc[0]['strand']\n",
    "            gene_start = gene_rows.iloc[0]['cstart']\n",
    "            gene_end   = gene_rows.iloc[0]['cend']\n",
    "        else:\n",
    "            # Fall back on the first exon if no gene record is available.\n",
    "            seqname = group.iloc[0]['seqname']\n",
    "            strand  = group.iloc[0]['strand']\n",
    "            gene_start = None\n",
    "            gene_end = None\n",
    "        \n",
    "        # Get all exon rows for this gene\n",
    "        exon_rows = group[group['feature'] == 'exon']\n",
    "        if exon_rows.empty:\n",
    "            continue\n",
    "        \n",
    "        # Build a list of exon intervals (each as a tuple (start, end))\n",
    "        # Optionally we could filter to exons that fall within the gene boundaries.\n",
    "        exon_intervals = list(zip(exon_rows['cstart'], exon_rows['cend']))\n",
    "        \n",
    "        # Sort by start (genomic order)\n",
    "        exon_intervals = sorted(exon_intervals, key=lambda x: x[0])\n",
    "        \n",
    "        # Merge overlapping or adjacent exons.\n",
    "        # (For example, if two exons overlap because of alternative splicing, we want the union.)\n",
    "        merged_exons = []\n",
    "        for interval in exon_intervals:\n",
    "            if not merged_exons:\n",
    "                merged_exons.append(list(interval))\n",
    "            else:\n",
    "                last = merged_exons[-1]\n",
    "                # If the current exon overlaps or touches the previous one, merge them.\n",
    "                if interval[0] <= last[1]:\n",
    "                    last[1] = max(last[1], interval[1])\n",
    "                else:\n",
    "                    merged_exons.append(list(interval))\n",
    "        \n",
    "        # If there are fewer than two merged exons, then there is no intron.\n",
    "        if len(merged_exons) < 2:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # For each adjacent pair of merged exons, define an intron between them.\n",
    "        for i in range(len(merged_exons) - 1):\n",
    "            # The intron is the gap between the end of exon i and the start of exon i+1.\n",
    "            intron_start = merged_exons[i][1]\n",
    "            intron_end   = merged_exons[i+1][0]\n",
    "            # Only add if there is a gap.\n",
    "            if intron_end > intron_start:\n",
    "                intron_records.append({\n",
    "                    'seqname': seqname,\n",
    "                    'feature': 'intron',\n",
    "                    'cstart': intron_start,\n",
    "                    'cend': intron_end,\n",
    "                    'strand': strand\n",
    "                })\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(intron_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling and applying calculate_introns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_data = load_gtf_annotations(datasets_path + 'basic_annotations.gtf')\n",
    "annotation_data = annotation_data[annotation_data[\"seqname\"]!=\"chrM\"]\n",
    "introns = calculate_introns(annotation_data)\n",
    "\n",
    "trimmed_annotation_data = annotation_data[[\"seqname\", \"feature\", \"cstart\", \"cend\", \"strand\"]]\n",
    "\n",
    "IntronExonDF = pd.concat([trimmed_annotation_data, introns])\n",
    "\n",
    "# IntronExonDF.to_csv(datasets_path + 'IntronExonDF.csv', index=False)\n",
    "# introns.to_csv(datasets_path + 'BetterIntrons.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a fix to an old version of calculate_introns, included to keep the pipeline working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_columns_if_needed(df, col_a, col_b):\n",
    "    \"\"\"\n",
    "    Turns out the (-) strand lists cstart as smaller than cend.  This fixes the output from\n",
    "    the above function that calculated intron boundaries.\n",
    "    For each row in the dataframe, if the value in col_a is greater than the value in col_b,\n",
    "    swap the two values.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        col_a (str): The name of the first column.\n",
    "        col_b (str): The name of the second column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with swapped values where needed.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask where the value in col_a is greater than col_b.\n",
    "    mask = df[col_a] > df[col_b]\n",
    "    \n",
    "    # Swap the values in col_a and col_b for rows where mask is True.\n",
    "    df.loc[mask, [col_a, col_b]] = df.loc[mask, [col_b, col_a]].values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the print statement in the next cell to confirm the merge worked by comparing to https://genome.ucsc.edu/cgi-bin/hgGateway  \n",
    "\n",
    "On the human genome UCSC browser, due to indexing 1 on their end and 0 in python, cstart here is the last base to the right of the feature on the browser.\n",
    "\n",
    "cend looks like the correct spot, but only because python excludes the last base which cancels out the off by 1 issue\n",
    "\n",
    "Printing line by line swaps back into 1 indexing in a .txt file so locations are accurate as long as line 1 has the first base. That might not always be the case if I put a print statement somewhere without thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FixedIntronExonDF = swap_columns_if_needed(IntronExonDF, 'cstart', 'cend')\n",
    "Trimmed_Intron_Exon_DF = FixedIntronExonDF[((FixedIntronExonDF[\"feature\"]==\"exon\") | (FixedIntronExonDF[\"feature\"]==\"intron\"))]\n",
    "Trimmed_Intron_Exon_DF = Trimmed_Intron_Exon_DF[[\"seqname\", \"feature\", \"cstart\", \"cend\", \"strand\"]]\n",
    "\n",
    "print(Trimmed_Intron_Exon_DF.sample(10))\n",
    "\n",
    "Trimmed_Intron_Exon_DF.to_csv(datasets_path + \"FinalIntronExonDF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next five code cells are used to parallel process the GTF and Fasta data into tfrecord.gz files.\n",
    "\n",
    "First cell defines utility functions and a function that converts fasta and gtf data into encoded arrays with DNA location info as annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gtf_annotations(gtf_file):\n",
    "    \"\"\"\n",
    "    Loads GTF into a pandas DataFrame and converts cstart and cend to zero-based indexing.\n",
    "    \"\"\"\n",
    "    gtf_data = pd.read_csv(\n",
    "        gtf_file, sep='\\t', comment='#', header=None,\n",
    "        names=['seqname', 'source', 'feature', 'cstart', 'cend', \n",
    "               'score', 'strand', 'frame', 'attribute']\n",
    "    )\n",
    "    # Convert to zero-based indexing for cstart\n",
    "    gtf_data['cstart'] = gtf_data['cstart'] - 1\n",
    "    return gtf_data\n",
    "\n",
    "\n",
    "# def compute_chunk_indices(fasta_file, chunk_size):\n",
    "#     \"\"\"\n",
    "#     Creates a list of (record_id, cstart, cend) for each chunk in the FASTA.\n",
    "#     This version does not take window shifts\n",
    "#     \"\"\"\n",
    "#     print('Running compute_chunk_indices')\n",
    "    \n",
    "#     fa = Fasta(fasta_file)  # for indexed random access\n",
    "#     chunk_indices = []\n",
    "#     for record_id in fa.keys():\n",
    "#         seq_len = len(fa[record_id])\n",
    "#         for cstart in range(0, seq_len, chunk_size):\n",
    "#             cend = min(cstart + chunk_size, seq_len)\n",
    "#             chunk_indices.append((record_id, cstart, cend))\n",
    "#     return chunk_indices\n",
    "\n",
    "\n",
    "def compute_chunk_indices(fasta_file, chunk_size, shifts=[0]):\n",
    "    \"\"\"\n",
    "    Creates a list of (record_id, cstart, cend) for each chunk in the FASTA,\n",
    "    applying additional shifts to augment the dataset.  Shifts takes a list of window shift values.  \n",
    "    \"\"\"\n",
    "    print('Running compute_chunk_indices with data augmentation')\n",
    "    fa = Fasta(fasta_file)  # for indexed random access\n",
    "    chunk_indices = []\n",
    "    for record_id in fa.keys():\n",
    "        seq_len = len(fa[record_id])\n",
    "        for shift in shifts:\n",
    "            # Start at the given shift, then step by chunk_size\n",
    "            for cstart in range(shift, seq_len, chunk_size):\n",
    "                cend = min(cstart + chunk_size, seq_len)\n",
    "                chunk_indices.append((record_id, cstart, cend))\n",
    "    return chunk_indices\n",
    "\n",
    "\n",
    "def one_hot_encode_reference(sequence):\n",
    "    \"\"\"\n",
    "    Returns a list of 4-element lists for each base (A, C, G, T).\n",
    "    \"\"\"\n",
    "    n_base_encoder = {\n",
    "        'A': [1, 0, 0, 0],\n",
    "        'C': [0, 1, 0, 0],\n",
    "        'G': [0, 0, 1, 0],\n",
    "        'T': [0, 0, 0, 1],\n",
    "        'N': [0, 0, 0, 0],  \n",
    "        }\n",
    "    return [n_base_encoder.get(nuc, [0, 0, 0, 0]) for nuc in sequence]\n",
    "\n",
    "\n",
    "def label_sequence_local(sequence_length, annotations, window=50):\n",
    "    \"\"\"\n",
    "    Builds a label matrix with partial credit for boundary annotations and a background channel\n",
    "    that is set to 0 if any full annotation (value 1) is present in the other channels, or 1\n",
    "    if only partial credit is present.\n",
    "    \n",
    "    For each annotation (e.g. an exon start), the target values are assigned as follows:\n",
    "      - At the annotated base: 1.0.\n",
    "      - At positions 1 base away: 0.5.\n",
    "      - At positions at distance d (for d=2,...,window):\n",
    "            credit = 0.5 - 0.01*(d-1)\n",
    "        so that at distance 50 the credit is 0.5 - 0.01*(50-1) = 0.01.\n",
    "    \n",
    "    When two annotations of the same channel are nearby, each base’s final target is the maximum\n",
    "    credit received from any annotation.\n",
    "    \n",
    "    Channels are assigned as:\n",
    "      - Column 0: non‑coding (background).  \n",
    "          * It is set to 0 when any of the other channels equals 1.\n",
    "          * If none of the other channels have a full (1.0) annotation, background is left at 1.\n",
    "      - Column 1: intron cstart (annotation at cstart for feature \"intron\")\n",
    "      - Column 2: intron cend (annotation at cend-1 for feature \"intron\")\n",
    "      - Column 3: exon cstart (annotation at cstart for feature \"exon\")\n",
    "      - Column 4: exon cend (annotation at cend-1 for feature \"exon\")\n",
    "    \n",
    "    Parameters:\n",
    "      sequence_length (int): Length of the sequence.\n",
    "      annotations (DataFrame): Must have columns 'cstart', 'cend', and 'feature'.\n",
    "      window (int): How far (in bases) the partial credit is spread (default 50).\n",
    "    \n",
    "    Returns:\n",
    "      np.ndarray: A (sequence_length x 5) label matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create binary arrays for the four annotation channels.\n",
    "    exon_cstart_binary   = np.zeros(sequence_length)\n",
    "    exon_cend_binary     = np.zeros(sequence_length)\n",
    "    intron_cstart_binary = np.zeros(sequence_length)\n",
    "    intron_cend_binary   = np.zeros(sequence_length)\n",
    "    \n",
    "    # Mark exact positions from annotations.\n",
    "    for _, row in annotations.iterrows():\n",
    "        cs = int(row['cstart'])\n",
    "        ce = int(row['cend'])\n",
    "        feat = row['feature'].strip().lower()\n",
    "        if feat == 'exon':\n",
    "            if 0 <= cs < sequence_length:\n",
    "                exon_cstart_binary[cs] = 1\n",
    "            if 0 <= ce - 1 < sequence_length:\n",
    "                exon_cend_binary[ce - 1] = 1\n",
    "        elif feat == 'intron':\n",
    "            if 0 <= cs < sequence_length:\n",
    "                intron_cstart_binary[cs] = 1\n",
    "            if 0 <= ce - 1 < sequence_length:\n",
    "                intron_cend_binary[ce - 1] = 1\n",
    "\n",
    "    def smooth_binary(binary_arr, window):\n",
    "        \"\"\"\n",
    "        Given a binary array (with 1’s at annotated positions), create a custom \n",
    "        \"smoothed\" array where an annotation at position i contributes:\n",
    "          - 1.0 at position i,\n",
    "          - 0.5 at positions i ± 1,\n",
    "          - and for positions i ± d (with 2 <= d <= window):\n",
    "                0.5 - 0.01*(d-1)\n",
    "        Contributions from multiple annotations are combined via max().\n",
    "        \"\"\"\n",
    "        L = len(binary_arr)\n",
    "        smooth_arr = np.zeros(L)\n",
    "        # Find indices where an annotation is present.\n",
    "        annotation_indices = np.where(binary_arr == 1)[0]\n",
    "        for idx in annotation_indices:\n",
    "            # Annotated base gets full credit.\n",
    "            smooth_arr[idx] = 1.0\n",
    "            # Spread out to left and right.\n",
    "            for d in range(1, window + 1):\n",
    "                credit = 0.5 - (0.5 / window) * (d - 1)\n",
    "                # Ensure credit is not negative.\n",
    "                credit = max(credit, 0)\n",
    "                left = idx - d\n",
    "                right = idx + d\n",
    "                if left >= 0:\n",
    "                    smooth_arr[left] = max(smooth_arr[left], credit)\n",
    "                if right < L:\n",
    "                    smooth_arr[right] = max(smooth_arr[right], credit)\n",
    "        return smooth_arr\n",
    "\n",
    "    # Smooth each binary channel.\n",
    "    exon_cstart_smooth   = smooth_binary(exon_cstart_binary, window)\n",
    "    exon_cend_smooth     = smooth_binary(exon_cend_binary, window)\n",
    "    intron_cstart_smooth = smooth_binary(intron_cstart_binary, window)\n",
    "    intron_cend_smooth   = smooth_binary(intron_cend_binary, window)\n",
    "    \n",
    "    # Build the full label matrix.\n",
    "    # Columns: [non-coding, intron cstart, intron cend, exon cstart, exon cend]\n",
    "    labels = np.zeros((sequence_length, 5))\n",
    "    labels[:, 1] = intron_cstart_smooth\n",
    "    labels[:, 2] = intron_cend_smooth\n",
    "    labels[:, 3] = exon_cstart_smooth\n",
    "    labels[:, 4] = exon_cend_smooth\n",
    "    \n",
    "    # For each base:\n",
    "    # - If any annotation channel is exactly 1, set background to 0.\n",
    "    # - Otherwise (only partial credits present), leave background as 1.\n",
    "    max_annotation = np.max(labels[:, 1:], axis=1)\n",
    "    # Where max_annotation is 1, background = 0; otherwise background = 1.\n",
    "    labels[:, 0] = np.where(max_annotation == 1, 0, 1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def pad_labels(labels, target_length):\n",
    "    \"\"\"\n",
    "    Pads a NumPy label array (shape: [current_length, 5]) up to target_length\n",
    "    by adding rows of [1, 0, 0, 0, 0] (representing non-coding).\n",
    "    \"\"\"\n",
    "    current_length = len(labels)\n",
    "    if current_length < target_length:\n",
    "        pad_length = target_length - current_length\n",
    "        # Create an array with pad_length rows of the non-coding label.\n",
    "        pad_array = np.tile(np.array([[1, 0, 0, 0, 0]]), (pad_length, 1))\n",
    "        labels = np.concatenate([labels, pad_array], axis=0)\n",
    "        labels = labels.tolist()\n",
    "    return labels\n",
    "\n",
    "\n",
    "def pad_encoded_seq(encoded_seq, target_length):\n",
    "    \"\"\"\n",
    "    Pads sequence of shape (seq_len, 5) up to (target_length, 5) with zeros.\n",
    "    \"\"\"\n",
    "    seq_len = len(encoded_seq)\n",
    "    pad_size = target_length - seq_len\n",
    "    if pad_size > 0:\n",
    "        encoded_seq += [[0, 0, 0, 0, 0]] * pad_size\n",
    "    return encoded_seq\n",
    "\n",
    "\n",
    "def build_chunk_data_for_indices(fasta_file, gtf_df, subset_indices, skip_empty=True, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    For each chunk (record_id, cstart, cend) in the FASTA:\n",
    "      - Extract the reference sequence\n",
    "      - For each strand (+ and -):\n",
    "          1) Create a [chunk_size x 5] input: 4 channels for bases, 1 for strand\n",
    "          2) Label the chunk (0..chunk_size-1) using the GTF annotations that\n",
    "             fall on this chunk AND on this strand\n",
    "          3) If skip_empty=True and all labels are background, skip\n",
    "          4) Yield (X, y) or store it somewhere\n",
    "\n",
    "    Returns: generator of (X, y, record_id, chunk_cstart, chunk_cend, strand, chunk_size)\n",
    "    \"\"\"\n",
    "    print('running build_chunk_data_for_indices')\n",
    "    \n",
    "    fa = Fasta(fasta_file)\n",
    "\n",
    "    # Pre-group GTF by (seqname, strand) to speed up filtering\n",
    "    grouped_gtf = {}\n",
    "    for (seqname, strand), sub_df in gtf_df.groupby(['seqname', 'strand']):\n",
    "        grouped_gtf[(seqname, strand)] = sub_df\n",
    "\n",
    "        \n",
    "    for (record_id, cstart, cend) in subset_indices:\n",
    "        # Read the reference chunk\n",
    "        seq = str(fa[record_id][cstart:cend])  # raw bases from reference\n",
    "        base_encoded_4 = one_hot_encode_reference(seq)  # shape => (chunk_len, 4)\n",
    "\n",
    "        chunk_len = len(base_encoded_4)  # could be < chunk_size if at the cend of the chromosome\n",
    "\n",
    "        for strand_symbol in ['+', '-']:\n",
    "            # append the 5th channel for strand\n",
    "            # 1 for +, 0 for -:\n",
    "            strand_flag = 1 if strand_symbol == '+' else 0\n",
    "            encoded_seq_5 = [row + [strand_flag] for row in base_encoded_4]\n",
    "\n",
    "            # Filter GTF for (record_id, strand_symbol)\n",
    "            if (record_id, strand_symbol) not in grouped_gtf:\n",
    "                # No annotations for that contig+strand => all labels=[[1, 0, 0, 0, 0]]\n",
    "                labels = [[1, 0, 0, 0, 0]]*chunk_len\n",
    "            else:\n",
    "                sub_df = grouped_gtf[(record_id, strand_symbol)]\n",
    "                # Keep only rows that overlap [cstart, cend)\n",
    "                # Then shift 'cstart' and 'cend' to local coordinates\n",
    "                overlap = sub_df[\n",
    "                    (sub_df['cstart'] < cend) & \n",
    "                    (sub_df['cend'] > cstart)\n",
    "                ].copy()\n",
    "\n",
    "                if len(overlap) == 0:\n",
    "                    labels = [[1, 0, 0, 0, 0]]*chunk_len\n",
    "                else:\n",
    "                    # Shift coords so that cstart => 0\n",
    "                    overlap['cstart'] = overlap['cstart'] - cstart\n",
    "                    overlap['cend']   = overlap['cend']   - cstart\n",
    "\n",
    "                    # Now label them in local chunk coords\n",
    "                    labels = label_sequence_local(chunk_len, overlap, window=100)\n",
    "                    labels = labels.tolist()\n",
    "\n",
    "            # Optionally skip if all labels=0 and skip_empty=True\n",
    "            if skip_empty and all(lbl == [1, 0, 0, 0, 0] for lbl in labels):\n",
    "                continue\n",
    "\n",
    "            # Pad up to chunk_size if needed\n",
    "            encoded_seq_5 = pad_encoded_seq(encoded_seq_5, chunk_size)\n",
    "            labels = pad_labels(labels, chunk_size)\n",
    "\n",
    "            # Convert to np.array for deep learning frameworks\n",
    "            X = np.array(encoded_seq_5, dtype=np.float32)   # shape [chunk_size, 5]\n",
    "            y = np.array(labels, dtype=np.float32)            # shape [chunk_size]\n",
    "            \n",
    "            # Passing chunk_size through for unzipping the tfrecord later\n",
    "            chunk_size = chunk_size\n",
    "\n",
    "            # Yield or store\n",
    "            yield (X, y, record_id, cstart, cend, strand_symbol, chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second cell defines functions to optimize output of first cell's last function for use in a tfrecord file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_feature_list(value_list):\n",
    "    \"\"\"\n",
    "    Utility to convert a list of floats into a FloatList. Floats are needed for backpropagation, \n",
    "    activation functions, and loss calculations and are thus the default type in TF and PyTorch.\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value_list))\n",
    "\n",
    "\n",
    "def int_feature_list(value_list):\n",
    "    \"\"\"\n",
    "    Utility to convert a list of ints into an Int64List.  Ints fine because this is classification\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value_list))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"\n",
    "    Utility for a single string/bytes feature. For efficient string storage\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.encode('utf-8')\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def serialize_chunk_example(X, y, record_id, cstart, cend, strand_symbol, chunk_size = 5000):\n",
    "    \"\"\"\n",
    "    Converts a single chunk's data into a tf.train.Example protobuf.\n",
    "\n",
    "    :param X: np.array(float32) of shape [chunk_size, 5]\n",
    "    :param y: np.array(int32) of shape [chunk_size]\n",
    "    :param record_id: str (e.g. chromosome name)\n",
    "    :param cstart, cend: int\n",
    "    :param strand_symbol: '+' or '-'\n",
    "\n",
    "    Flattens X and y for storage. Requires parse/reshape at read time.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunk_size = chunk_size\n",
    "    \n",
    "    # Flattens X to 1D and stores it in row-major order.\n",
    "    X_flat = X.flatten().tolist()\n",
    "    \n",
    "    # y is already 1D, but ensure it's a list of int\n",
    "    y_list = y.flatten().tolist()\n",
    "    \n",
    "    # Builds a dictionary of features using utility functions above \n",
    "    # to cast into types preferred by tensorflow\n",
    "    feature_dict = {        \n",
    "        'X':           float_feature_list(X_flat),\n",
    "        'y':           float_feature_list(y_list),\n",
    "        'record_id':   bytes_feature(record_id),\n",
    "        'cstart':      int_feature_list([cstart]),\n",
    "        'cend':        int_feature_list([cend]),\n",
    "        'strand':      bytes_feature(strand_symbol),\n",
    "        'chunk_size':  int_feature_list([chunk_size]),\n",
    "    }\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third cell defines a function that performs row/example generation and breaks writing process into threads.  \n",
    "\n",
    "Uses functions defined in first and second cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_shard_with_threads(\n",
    "    shard_id,\n",
    "    shard_path,\n",
    "    num_shards,\n",
    "    all_indices,\n",
    "    fasta_file,\n",
    "    gtf_df,\n",
    "    compression_type=\"GZIP\",\n",
    "    skip_empty=True,\n",
    "    max_threads_per_process=4,\n",
    "    chunk_size_input = 5000\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes data for a specific shard using threads for concurrent writes.\n",
    "    Data is processed incrementally to avoid loading everything into memory.\n",
    "    \"\"\"\n",
    "    print('Running write_to_shard_with_threads')\n",
    "    \n",
    "    # Filter indices for this shard\n",
    "    subset_indices = [\n",
    "        idx for i, idx in enumerate(all_indices)\n",
    "        if i % num_shards == shard_id\n",
    "    ]\n",
    "    print(f'Shard subset indices gathered for shard {shard_id}')\n",
    "    \n",
    "    # Passing chunk_size through for the unzipping step later\n",
    "    chunk_size_in = chunk_size_input\n",
    "    \n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    writer = tf.io.TFRecordWriter(shard_path, options=options)\n",
    "    lock = threading.Lock()  # Ensure thread-safe writes\n",
    "\n",
    "    def thread_worker(subset_indices_split):\n",
    "        \"\"\"Thread worker to process and write chunks.\"\"\"\n",
    "        for X, y, record_id, cstart, cend, strand_symbol, chunk_size in build_chunk_data_for_indices(\n",
    "            fasta_file, gtf_df, subset_indices_split, skip_empty=skip_empty, chunk_size=chunk_size_in\n",
    "            ):\n",
    "            try:\n",
    "                # Serialize the chunk\n",
    "                example_str = serialize_chunk_example(X, y, record_id, cstart, cend, strand_symbol, chunk_size)\n",
    "                with lock:  # Ensure thread-safe writes\n",
    "                    writer.write(example_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing chunk: {e}\")\n",
    "\n",
    "    # Divide the subset_indices into splits for each thread\n",
    "    subset_splits = [\n",
    "        subset_indices[i::max_threads_per_process] for i in range(max_threads_per_process)\n",
    "    ]\n",
    "\n",
    "    # Start threads\n",
    "    with cf.ThreadPoolExecutor(max_threads_per_process) as thread_executor:\n",
    "        thread_futures = [\n",
    "            thread_executor.submit(thread_worker, subset_split)\n",
    "            for subset_split in subset_splits\n",
    "        ]\n",
    "        \n",
    "        # After executor finishes\n",
    "        for future in thread_futures:\n",
    "            future.result() # Returns None but terminates thread if using 'with' didn't\n",
    "            worker_number = thread_futures.index(future)\n",
    "            print(f'Thread executor {worker_number} completed for process executor {shard_id}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth cell splits data generation into multiple processes and feeds options to lower functions.\n",
    "\n",
    "Uses functions defined in first and third cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord_in_shards_hybrid(\n",
    "    shard_prefix,\n",
    "    fasta_file,\n",
    "    gtf_df,\n",
    "    num_shards=4,\n",
    "    compression_type=\"GZIP\",\n",
    "    max_processes=4,\n",
    "    max_threads_per_process=4,\n",
    "    chunk_size=5000,\n",
    "    skip_empty=True,\n",
    "    shifts = [0]\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes data in multiple TFRecord shards using multiprocessing for shards\n",
    "    and threading within each shard (hybrid).\n",
    "    \n",
    "    Var_name reminders:\n",
    "    :param shard_prefix: Base path for shards, e.g., \"my_chunks\"\n",
    "    :param build_chunk_generator: Generator yielding (X, y, record_id, cstart, cend, strand)\n",
    "    :param num_shards: Number of TFRecord shards to create\n",
    "    :param compression_type: 'GZIP', 'ZLIB', or None\n",
    "    :param max_processes: Number of processes to use for parallel writing\n",
    "    :param max_threads_per_process: Number of threads to use within each process\n",
    "    \"\"\"\n",
    "    print('Running write_tfrecord_in_shards_hybrid')\n",
    "    \n",
    "    # Compute all chunk indices first\n",
    "    all_indices = compute_chunk_indices(fasta_file, chunk_size, shifts=shifts)\n",
    "    print('all_indices calculated')\n",
    "    \n",
    "    # Create shard paths\n",
    "    shard_paths = []\n",
    "    for shard_id in range(num_shards):\n",
    "        shard_path = f\"{shard_prefix}-{shard_id:04d}.tfrecord\"\n",
    "        if compression_type == \"GZIP\":\n",
    "            shard_path += \".gz\"  # Naming convention\n",
    "        shard_paths.append(shard_path)\n",
    "    print(shard_paths)\n",
    "\n",
    "    # Spawn multiple processes (one per shard, or up to max_processes)\n",
    "    with cf.ProcessPoolExecutor(max_workers=max_processes) as process_executor:\n",
    "        futures = []\n",
    "        for shard_id in range(num_shards):\n",
    "            proc = process_executor.submit(\n",
    "                write_to_shard_with_threads,\n",
    "                shard_id,\n",
    "                shard_paths[shard_id],\n",
    "                num_shards,\n",
    "                all_indices,\n",
    "                fasta_file,\n",
    "                gtf_df,\n",
    "                compression_type,\n",
    "                skip_empty,\n",
    "                max_threads_per_process,\n",
    "                chunk_size\n",
    "            )\n",
    "            futures.append(proc)\n",
    "            print(f'Process executor {shard_id} has started')\n",
    "\n",
    "        # After all shards finish\n",
    "        for future in futures:\n",
    "            future.result() # Returns None but terminates thread if using 'with' didn't\n",
    "            shard_number = futures.index(future)\n",
    "            print(f\"Process executor {shard_number} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell is used to parallel process and write tfrecord.gz shards.  Turns out cells consider themselves as their own module so using main() notation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    my_fasta = datasets_path + 'trim_chr_genome.fa'\n",
    "    my_gtf_df = pd.read_csv(datasets_path + \"FinalIntronExonDF.csv\")\n",
    "    output_directory = datasets_path + \"Final_Optimized_TFRecord_Shards\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # change these every time. The shifts is for data augmentation\n",
    "    # naming convention to work with the shuffle function: list 4 digit shift values lowest to highest\n",
    "    # ex: [0000, 2500, 4000] becomes /000025004000_inex_shard\n",
    "    # It's more recommended to do a single shift at a time as done here\n",
    "    # Mixing multiple shifted datasets can be done later on\n",
    "    shifts = [3334]\n",
    "    my_prefix = output_directory + '/3334_inex_shard'\n",
    "    \n",
    "    write_tfrecord_in_shards_hybrid(\n",
    "        shard_prefix=my_prefix, \n",
    "        fasta_file=my_fasta, \n",
    "        gtf_df=my_gtf_df, \n",
    "        num_shards=4, \n",
    "        compression_type=\"GZIP\", \n",
    "        max_processes=4, \n",
    "        max_threads_per_process=2, \n",
    "        chunk_size=5000, \n",
    "        skip_empty=True,\n",
    "        shifts=shifts\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clunky shuffle method in the following 3 code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chunk_example(serialized_example):\n",
    "    \"\"\"\n",
    "    Parses a single serialized tf.train.Example back into tensors.\n",
    "    Used in testing datasets and in piping tfrecords to DL Algorithms\n",
    "    \"\"\"\n",
    "    feature_spec = {\n",
    "        'X':          tf.io.VarLenFeature(tf.float32),\n",
    "        'y':          tf.io.VarLenFeature(tf.float32),\n",
    "        'record_id':  tf.io.FixedLenFeature([], tf.string),\n",
    "        'cstart':     tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'cend':       tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'strand':     tf.io.FixedLenFeature([], tf.string),\n",
    "        'chunk_size': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    }\n",
    "    \n",
    "    parsed = tf.io.parse_single_example(serialized_example, feature_spec)\n",
    "    \n",
    "    # chunk_size is shape [1]\n",
    "    chunk_size = parsed['chunk_size'][0]\n",
    "    \n",
    "    # Convert sparse to dense\n",
    "    X_flat = tf.sparse.to_dense(parsed['X'])\n",
    "    y_flat = tf.sparse.to_dense(parsed['y'])\n",
    "\n",
    "    # Reshape X to [chunk_size, 5]\n",
    "    X_reshaped = tf.reshape(X_flat, [chunk_size, 5])\n",
    "    # Reshape y to [chunk_size], probably redundant\n",
    "    y_reshaped = tf.reshape(y_flat, [chunk_size, 5])\n",
    "    \n",
    "    record_id = parsed['record_id']\n",
    "    cstart    = parsed['cstart'][0]\n",
    "    cend      = parsed['cend'][0]\n",
    "    strand    = parsed['strand']\n",
    "    \n",
    "    return X_reshaped, y_reshaped, record_id, cstart, cend, strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_from_tfrecords(\n",
    "    tfrecord_pattern,\n",
    "    batch_size=28,\n",
    "    compression_type='GZIP',\n",
    "    shuffle_buffer=66000,\n",
    "):\n",
    "    '''\n",
    "    Builds shuffled dataset from tfrecords.  Returns unparsed serialized\n",
    "    dataset that is not human readable.  \n",
    "    '''\n",
    "\n",
    "    # Loads in records in a round robin fashion for slightly increased mixing\n",
    "    files = tf.data.Dataset.list_files(tfrecord_pattern, shuffle=True)\n",
    "    dataset = files.interleave(\n",
    "        lambda fname: tf.data.TFRecordDataset(fname, compression_type=compression_type),\n",
    "        cycle_length=4,        # how many files to read in parallel\n",
    "        block_length=1,         # how many records to read from each file before switching\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "    \n",
    "    # Shuffle at the record level\n",
    "    dataset = dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
    "\n",
    "    # Shuffle at batch level\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.shuffle(8*batch_size, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.unbatch()\n",
    "\n",
    "    # Prefetch for efficient access\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran this manually 10 times to get a good shuffle for the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "tfrecord_pattern = datasets_path + \"Shuffling/Shuffle_9/shuffled_shard_*.tfrecord.gz\"\n",
    "ds = build_dataset_from_tfrecords(tfrecord_pattern,\n",
    "                                  batch_size=32, compression_type='GZIP',\n",
    "                                  shuffle_buffer=50000)\n",
    "\n",
    "'''Commented out so I don't accidentally try to rewrite anything'''\n",
    "# output_path = datasets_path + \"Shuffling/Shuffle_10\"\n",
    "# if not os.path.exists(output_path):\n",
    "#     os.makedirs(output_path)\n",
    "\n",
    "# num_shards = 4\n",
    "# writers = [\n",
    "#     tf.io.TFRecordWriter(f\"{output_path}/shuffled_shard_{i}.tfrecord.gz\", options=options)\n",
    "#     for i in range(num_shards)\n",
    "# ]\n",
    "\n",
    "# # Write out round-robin to each shard\n",
    "# for i, serialized_example in enumerate(ds):\n",
    "#     shard_index = i % num_shards\n",
    "#     writers[shard_index].write(serialized_example.numpy())\n",
    "\n",
    "# # Close all writers\n",
    "# for w in writers:\n",
    "#     w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better modular method: Split shards from parallel process into 24 tiny shards each, then write out a dataset that grabs one record from each tiny shard in a random order.\n",
    "\n",
    "First, split shards into tiny shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tfrecords(input_directory, output_directory, num_splits=24):\n",
    "    \"\"\"\n",
    "    Splits each TFRecord in the input_directory (assumed to be gzipped TFRecords)\n",
    "    into 'num_splits' smaller TFRecords. The goal is to randomize the order of the\n",
    "    smaller TFRecords when grabbing a row to produce highly shuffled data.\n",
    "    \n",
    "    Each output file is named based on the original filename's components.\n",
    "    \n",
    "    Parameters:\n",
    "      input_directory (str): Relative or absolute path to the input TFRecords.\n",
    "      output_directory (str): Relative or absolute path where the smaller shards will be saved.\n",
    "      num_splits (int): Number of splits (shards) to create per input file.\n",
    "    \"\"\"\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    input_path = os.path.join(current_directory, input_directory)\n",
    "    output_path = os.path.join(current_directory, output_directory)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    input_file_names = os.listdir(input_path)\n",
    "    input_file_paths = [os.path.join(input_path, file) for file in input_file_names]\n",
    "\n",
    "    for file in input_file_paths:\n",
    "        # Example filename: \"1000_inex_shard-0002.tfrecord.gz\"\n",
    "        basename = os.path.basename(file)\n",
    "        # Split at \"_inex_shard-\"\n",
    "        set_index, remainder = basename.split(\"_inex_shard-\")\n",
    "        # Get the first 4 digits from remainder (ignoring the extension)\n",
    "        sub_index = remainder[:4]\n",
    "        # Use the final digit of the sub-index.\n",
    "        final_digit = sub_index[-1]\n",
    "        \n",
    "        # First pass: Count records without loading them all.\n",
    "        total_records = 0\n",
    "        for _ in tf.data.TFRecordDataset(file, compression_type=\"GZIP\"):\n",
    "            total_records += 1\n",
    "\n",
    "        # Compute even splits: base chunk size and distribute any remainder.\n",
    "        chunk_size = total_records // num_splits\n",
    "        remainder_count = total_records % num_splits\n",
    "\n",
    "        # Pre-calculate boundaries for each split.\n",
    "        boundaries = []\n",
    "        start = 0\n",
    "        for i in range(num_splits):\n",
    "            extra = 1 if i < remainder_count else 0\n",
    "            end = start + chunk_size + extra\n",
    "            boundaries.append(end)\n",
    "            start = end\n",
    "\n",
    "        # Open all the TFRecord writers.\n",
    "        writers = []\n",
    "        for i in range(num_splits):\n",
    "            sub_sub_index = f\"{i:02d}\"\n",
    "            new_filename = f\"{set_index}_{final_digit}_{sub_sub_index}_tiny_inex_shard.tfrecord.gz\"\n",
    "            new_filepath = os.path.join(output_path, new_filename)\n",
    "            options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "            writer = tf.io.TFRecordWriter(new_filepath, options=options)\n",
    "            writers.append(writer)\n",
    "\n",
    "        # Second pass: Write records to the appropriate shard in a streaming fashion.\n",
    "        current_index = 0\n",
    "        current_shard = 0\n",
    "        for record in tf.data.TFRecordDataset(file, compression_type=\"GZIP\"):\n",
    "            if current_index >= boundaries[current_shard]:\n",
    "                current_shard += 1\n",
    "            writers[current_shard].write(record.numpy())\n",
    "            current_index += 1\n",
    "\n",
    "        # Close all writers.\n",
    "        for writer in writers:\n",
    "            writer.close()\n",
    "\n",
    "        print(f\"Processed {basename}: {total_records} records split into {num_splits} tiny shards.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_directory = datasets_path + \"Augmented TFRecords 3/\"\n",
    "# output_directory = datasets_path + \"Shuffle Shards/\"\n",
    "# split_tfrecords(input_directory, output_directory, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the dataset taking one sample from each tiny shard in a random order.  Generates a new random order each time after all tiny shards are visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_shuffled_records(input_dir, allowed_indices):\n",
    "    \"\"\"\n",
    "    Lazily iterates over TFRecord files in input_dir whose filenames start with one of the allowed_indices.\n",
    "    Each round, it shuffles the list of file iterators and yields one record per file.\n",
    "    Files that are exhausted are removed from future rounds.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing the TFRecord files.\n",
    "        allowed_indices (list): List of allowed starting indices (as strings or integers).\n",
    "    \n",
    "    Yields:\n",
    "        A TFRecord (as a tf.Tensor) from one of the files.\n",
    "    \"\"\"\n",
    "    # List file paths that start with one of the allowed indices.\n",
    "    file_paths = [os.path.join(input_dir, fname)\n",
    "                  for fname in os.listdir(input_dir)\n",
    "                  if any(fname.startswith(str(idx)) for idx in allowed_indices)]\n",
    "    \n",
    "    if not file_paths:\n",
    "        raise ValueError(\"No TFRecord files found matching allowed indices.\")\n",
    "    \n",
    "    # Create a list of (file_path, iterator) tuples.\n",
    "    file_iterators = [(fp, iter(tf.data.TFRecordDataset(fp, compression_type=\"GZIP\")))\n",
    "                      for fp in file_paths]\n",
    "    \n",
    "    # Continue until all iterators are exhausted.\n",
    "    while file_iterators:\n",
    "        random.shuffle(file_iterators)\n",
    "        next_file_iterators = []\n",
    "        for fp, iterator in file_iterators:\n",
    "            try:\n",
    "                record = next(iterator)\n",
    "                yield record\n",
    "                next_file_iterators.append((fp, iterator))\n",
    "            except StopIteration:\n",
    "                print(f\"File {fp} is exhausted and will be skipped.\")\n",
    "        file_iterators = next_file_iterators\n",
    "\n",
    "def write_shuffled_records_to_single_tfrecord(input_dir, allowed_indices, output_filepath):\n",
    "    \"\"\"\n",
    "    Writes all records produced by stream_shuffled_records into one big gzip-compressed TFRecord file.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing the source TFRecord files.\n",
    "        allowed_indices (list): List of allowed starting indices.\n",
    "        output_filepath (str): Full path to the output TFRecord file.\n",
    "    \"\"\"\n",
    "    # Set up the TFRecord writer with gzip compression.\n",
    "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "    writer = tf.io.TFRecordWriter(output_filepath, options=options)\n",
    "    \n",
    "    record_count = 0\n",
    "    # Stream through the records.\n",
    "    for record in stream_shuffled_records(input_dir, allowed_indices):\n",
    "        writer.write(record.numpy())\n",
    "        record_count += 1\n",
    "        # Print a status update every 1000 records.\n",
    "        if record_count % 1000 == 0:\n",
    "            print(f\"{record_count} records written...\")\n",
    "    \n",
    "    writer.close()\n",
    "    print(f\"Finished writing {record_count} records to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the directory containing your shuffled tiny shards.\n",
    "    input_directory = datasets_path + \"Shuffle Shards\"  # Adjust as needed.\n",
    "    \n",
    "    # Define the allowed starting indices (adjust to your needs).\n",
    "    allowed_indices = [\"0000\", \"2500\"]  # Example indices.\n",
    "    \n",
    "    # Define the output filepath for the big combined TFRecord.\n",
    "    output_filename = datasets_path + \"AugDataSets/00002500_shuffled.tfrecord.gz\"\n",
    "    current_directory = os.getcwd()\n",
    "    output_filepath = os.path.join(current_directory, output_filename)\n",
    "    \n",
    "    # Run the function to write the big TFRecord.\n",
    "    write_shuffled_records_to_single_tfrecord(input_directory, allowed_indices, output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells print tfrecord data to check quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_from_tfrecords(\n",
    "    tfrecord_pattern,\n",
    "    batch_size=32,\n",
    "    compression_type='GZIP',\n",
    "    shuffle_buffer=75000\n",
    "):\n",
    "    '''\n",
    "    Imports tfrecord and shuffles it then parses it and returns a\n",
    "    human readable dataset.  \n",
    "    Two goals: \n",
    "        1. To confirm tfrecord(s) is/are saved properly\n",
    "        2. To view list of record_ids in the batch to see if dataset \n",
    "            is sufficiently shuffled.  Ideally, a good spread of chrN\n",
    "            shows up.\n",
    "    '''\n",
    "    # Loads in records in a round robin fashion for slightly increased mixing\n",
    "    files = tf.data.Dataset.list_files(tfrecord_pattern, shuffle=True)\n",
    "    dataset = files.interleave(\n",
    "        lambda fname: tf.data.TFRecordDataset(fname, compression_type=compression_type),\n",
    "        cycle_length=4,        # how many files to read in parallel\n",
    "        block_length=1,         # how many records to read from each file before switching\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "    \n",
    "    # Shuffle at the record level\n",
    "    dataset = dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
    "\n",
    "    # Shuffle at batch level\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.shuffle(8*batch_size, reshuffle_each_iteration=True)\n",
    "\n",
    "    # Unbatch for parsing and parse\n",
    "    dataset = dataset.unbatch()    \n",
    "    dataset = dataset.map(parse_chunk_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Rebatch parsed and prefetch for efficient reading\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_pattern = datasets_path + \"Shuffling/Shuffle_10/shuffled_shard_*.tfrecord.gz\"\n",
    "\n",
    "ds = test_dataset_from_tfrecords(tfrecord_pattern,\n",
    "                                  batch_size=32, compression_type='GZIP',\n",
    "                                  shuffle_buffer=50000)\n",
    "\n",
    "for X_batch, y_batch, record_id_batch, cstart_batch, cend_batch, strand_batch in ds.take(1):\n",
    "    print(\"X shape:\", X_batch.shape)\n",
    "    print(\"y shape:\", y_batch.shape)\n",
    "    print(\"record_id:\", record_id_batch)\n",
    "    print(\"cstart:\", cstart_batch)\n",
    "    print(\"cend:\", cend_batch)\n",
    "    print(\"strand:\", strand_batch)\n",
    "    # Might not work as written due to changes since writing this\n",
    "    # for i in range(5000):\n",
    "    #     print(f\"Data: {X_batch[0][i]},   {y_batch[0][i]} :Label\")\n",
    "    # print(f\"chr: {record_id_batch[0]}, cstart: {cstart_batch[0]}, cend: {cend_batch[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two code cells can be used to generate Test, Validate, Train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvt_split_tfrecords(original_pattern, train_path, val_path, test_path, train_frac=0.8, val_frac=0.10):\n",
    "    \"\"\"\n",
    "    Splits TFRecord files into separate train, validation, and test sets *without parsing*.\n",
    "    Reads raw serialized records and writes them into new TFRecord files.\n",
    "    \"\"\"\n",
    "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "    # Create TFRecord writers\n",
    "    train_writer = tf.io.TFRecordWriter(train_path, options=options)\n",
    "    val_writer = tf.io.TFRecordWriter(val_path, options=options)\n",
    "    test_writer = tf.io.TFRecordWriter(test_path, options=options)\n",
    "\n",
    "    # List the original TFRecord files\n",
    "    dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(original_pattern), compression_type='GZIP')\n",
    "    \n",
    "    num_records = 0\n",
    "    for _ in dataset:\n",
    "        num_records += 1\n",
    "    print(f\"Total records found: {num_records}\")\n",
    "\n",
    "    # Compute split sizes\n",
    "    train_size = int(train_frac * num_records)\n",
    "    val_size   = int(val_frac * num_records)\n",
    "    test_size  = num_records - train_size - val_size  # Ensuring all records are accounted for\n",
    "\n",
    "    print(f\"Splitting into -> Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "\n",
    "    # Iterate over records and write them to appropriate files\n",
    "    train_count, val_count, test_count = 0, 0, 0\n",
    "    dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(original_pattern), compression_type='GZIP')\n",
    "    dataset = dataset.shuffle(25000, reshuffle_each_iteration=True)\n",
    "\n",
    "    for i, raw_record in enumerate(dataset):\n",
    "        if i < train_size:\n",
    "            train_writer.write(raw_record.numpy())\n",
    "            train_count += 1\n",
    "        elif i < train_size + val_size:\n",
    "            val_writer.write(raw_record.numpy())\n",
    "            val_count += 1\n",
    "        else:\n",
    "            test_writer.write(raw_record.numpy())\n",
    "            test_count += 1\n",
    "\n",
    "    # Close writers\n",
    "    train_writer.close()\n",
    "    val_writer.close()\n",
    "    test_writer.close()\n",
    "\n",
    "    print(f\"Final Split Counts -> Train: {train_count}, Val: {val_count}, Test: {test_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the testvaltrain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = datasets_path + \"AugDataSets/New/\"\n",
    "# paths = os.listdir(directory)\n",
    "# for filename in paths:\n",
    "#     pattern = directory + filename\n",
    "#     tvt_split_tfrecords(\n",
    "#         original_pattern=pattern,\n",
    "#         train_path=\"TestValTrain/train_\" + filename,\n",
    "#         val_path=\"TestValTrain/val_\" + filename,\n",
    "#         test_path=\"TestValTrain/test_\" + filename,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom not-quite-label-smoothing was hard written into the data.  This can remove that, generating fully binary versions of datasets passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_binary(x, y):\n",
    "    \"\"\"\n",
    "    Converts y so that any value not exactly 1 becomes 0.\n",
    "    Both x and y are expected to be tensors of shape (chunk_size, 5).\n",
    "    \"\"\"\n",
    "    y_binary = tf.cast(tf.equal(y, 1.0), y.dtype)\n",
    "    return x, y_binary\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a list of floats.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a list of ints.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string (or byte string).\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.encode('utf-8')\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def serialize_example_with_metadata_no_convert(x, y, record_id, cstart, cend, strand):\n",
    "    \"\"\"\n",
    "    Serializes a single example into a tf.train.Example.\n",
    "    Expects that y is already binary.\n",
    "    x and y are tensors of shape (chunk_size, 5). All metadata is preserved.\n",
    "    \"\"\"\n",
    "    # Flatten the X and y tensors into lists.\n",
    "    x_flat = tf.reshape(x, [-1]).numpy().tolist()\n",
    "    y_flat = tf.reshape(y, [-1]).numpy().tolist()\n",
    "    \n",
    "    # Determine chunk_size from the first dimension of x.\n",
    "    chunk_size = int(x.shape[0])\n",
    "    \n",
    "    # Convert metadata to Python types.\n",
    "    record_id_val = record_id.numpy() if isinstance(record_id, tf.Tensor) else record_id\n",
    "    strand_val = strand.numpy() if isinstance(strand, tf.Tensor) else strand\n",
    "\n",
    "    # cstart and cend are tensors of shape [1].\n",
    "    cstart_val = cstart.numpy() if isinstance(cstart, tf.Tensor) else cstart\n",
    "    cend_val   = cend.numpy()   if isinstance(cend, tf.Tensor)   else cend\n",
    "    cstart_int = int(cstart_val[0]) if isinstance(cstart_val, (list, tuple, np.ndarray)) else int(cstart_val)\n",
    "    cend_int   = int(cend_val[0])   if isinstance(cend_val, (list, tuple, np.ndarray))   else int(cend_val)\n",
    "    \n",
    "    feature = {\n",
    "        'X': _float_feature(x_flat),\n",
    "        'y': _float_feature(y_flat),\n",
    "        'record_id': _bytes_feature(record_id_val),\n",
    "        'cstart': _int64_feature([cstart_int]),\n",
    "        'cend': _int64_feature([cend_int]),\n",
    "        'strand': _bytes_feature(strand_val),\n",
    "        'chunk_size': _int64_feature([chunk_size])\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def parse_chunk_example(serialized_example):\n",
    "    \"\"\"\n",
    "    Parses a single serialized tf.train.Example back into tensors.\n",
    "    Assumes the TFRecord contains metadata fields:\n",
    "      - 'X': VarLenFeature(tf.float32)\n",
    "      - 'y': VarLenFeature(tf.float32)\n",
    "      - 'record_id': FixedLenFeature([], tf.string)\n",
    "      - 'cstart': FixedLenFeature([1], tf.int64)\n",
    "      - 'cend': FixedLenFeature([1], tf.int64)\n",
    "      - 'strand': FixedLenFeature([], tf.string)\n",
    "      - 'chunk_size': FixedLenFeature([1], tf.int64)\n",
    "    \"\"\"\n",
    "    feature_spec = {\n",
    "        'X':          tf.io.VarLenFeature(tf.float32),\n",
    "        'y':          tf.io.VarLenFeature(tf.float32),\n",
    "        'record_id':  tf.io.FixedLenFeature([], tf.string),\n",
    "        'cstart':     tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'cend':       tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'strand':     tf.io.FixedLenFeature([], tf.string),\n",
    "        'chunk_size': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    }\n",
    "    \n",
    "    parsed = tf.io.parse_single_example(serialized_example, feature_spec)\n",
    "    \n",
    "    # Extract chunk_size (a scalar)\n",
    "    chunk_size = parsed['chunk_size'][0]\n",
    "    \n",
    "    # Convert sparse tensors to dense and reshape.\n",
    "    X_flat = tf.sparse.to_dense(parsed['X'])\n",
    "    y_flat = tf.sparse.to_dense(parsed['y'])\n",
    "    X_reshaped = tf.reshape(X_flat, [chunk_size, 5])\n",
    "    y_reshaped = tf.reshape(y_flat, [chunk_size, 5])\n",
    "    \n",
    "    record_id = parsed['record_id']\n",
    "    cstart = parsed['cstart'][0]\n",
    "    cend = parsed['cend'][0]\n",
    "    strand = parsed['strand']\n",
    "    \n",
    "    return X_reshaped, y_reshaped, record_id, cstart, cend, strand\n",
    "\n",
    "def convert_and_write_tfrecord(input_tfrecord, output_tfrecord, compression_type=\"GZIP\"):\n",
    "    \"\"\"\n",
    "    Reads an existing TFRecord (with smoothed labels), converts the labels to binary,\n",
    "    and writes out a new TFRecord file with the same metadata.\n",
    "    \n",
    "    Args:\n",
    "      input_tfrecord: Path to the original TFRecord file.\n",
    "      output_tfrecord: Path where the new TFRecord (with binary labels) will be saved.\n",
    "      compression_type: Compression type used in the TFRecord (e.g., \"GZIP\").\n",
    "    \"\"\"\n",
    "    # Create a dataset from the input TFRecord.\n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        input_tfrecord,\n",
    "        compression_type=compression_type,\n",
    "        num_parallel_reads=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Parse each example.\n",
    "    dataset = dataset.map(parse_chunk_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Convert labels to binary.\n",
    "    def convert_sample(x, y, record_id, cstart, cend, strand):\n",
    "        x, y_binary = convert_labels_to_binary(x, y)\n",
    "        return x, y_binary, record_id, cstart, cend, strand\n",
    "    \n",
    "    dataset = dataset.map(convert_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Write out each converted sample to the new TFRecord file.\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    with tf.io.TFRecordWriter(output_tfrecord, options=options) as writer:\n",
    "        for sample in dataset:\n",
    "            # sample is a tuple: (X, y_binary, record_id, cstart, cend, strand)\n",
    "            X, y_binary, record_id, cstart, cend, strand = sample\n",
    "            serialized_example = serialize_example_with_metadata_no_convert(\n",
    "                X, y_binary, record_id, cstart, cend, strand)\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out to prevent overwrites/rewrites\n",
    "# Convert and write new TFRecord files for train, validation, and test splits.\n",
    "# convert_and_write_tfrecord(datasets_path + \"TestValTrain/train.tfrecord.gz\", datasets_path + \"TestValTrain/train_binary.tfrecord.gz\")\n",
    "# convert_and_write_tfrecord(datasets_path + \"TestValTrain/val.tfrecord.gz\", datasets_path + \"TestValTrain/val_binary.tfrecord.gz\")\n",
    "# convert_and_write_tfrecord(datasets_path + \"TestValTrain/test.tfrecord.gz\", datasets_path + \"TestValTrain/test_binary.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this and we haven't even started deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
