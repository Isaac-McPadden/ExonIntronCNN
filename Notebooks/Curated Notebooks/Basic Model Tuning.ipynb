{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the custom metrics, required because of label sparseness.  The background column (0 index in Python) which is more than 99% 1's would have too strong an effect on any scoring.  The final metric does score the background column which rapidly reaches an F1 score of 0.98 or 0.99 during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mregister_keras_serializable()\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomNoBackgroundF1Score\u001b[39;00m(metrics\u001b[38;5;241m.\u001b[39mMetric):\n",
      "\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_background_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m        Custom F1 score metric that only considers non-dominant classes (ignoring index 0).\u001b[39;00m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m            **kwargs: Additional keyword arguments.\u001b[39;00m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundF1Score(metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', threshold=0.5, name='no_background_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers non-dominant classes (ignoring index 0).\n",
    "        \n",
    "        This version is designed for multi-encoded labels where:\n",
    "          - The dominant class (index 0) is represented as a hard label [1, 0, 0, ...]\n",
    "          - For non-dominant classes (indices 1 to num_classes-1), only an exact label of 1 is considered positive.\n",
    "            (Any partial credit/smoothed values below 1 are treated as 0.)\n",
    "          - Predictions are thresholded (default threshold = 0.5) to decide 1 vs. 0.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "\n",
    "        # Create state variables to accumulate counts for each class.\n",
    "        # We use a vector of length num_classes but we will update only indices 1...num_classes-1.\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the metric state.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, num_classes). These are multi-encoded labels.\n",
    "                    For non-dominant classes, a label is considered positive only if it is exactly 1.\n",
    "            y_pred: Tensor of shape (batch_size, num_classes) with predictions (e.g. probabilities).\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Flatten all dimensions except the last one (which should be num_classes).\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "        \n",
    "        # We want to ignore the dominant class (index 0) and work on classes 1...num_classes-1.\n",
    "        # Assume y_true and y_pred are both of shape (batch_size, num_classes).\n",
    "        y_true_non_dominant = y_true[:, 1:]\n",
    "        y_pred_non_dominant = y_pred[:, 1:]\n",
    "        \n",
    "        # For ground truth: treat a class as positive only if its value is exactly 1.\n",
    "        one_value = tf.cast(1.0, dtype=y_true_non_dominant.dtype)\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_non_dominant, one_value), tf.int32)\n",
    "        # For predictions: apply thresholding.\n",
    "        y_pred_bin = tf.cast(y_pred_non_dominant >= self.threshold, tf.int32)\n",
    "        \n",
    "        # (Optionally) apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.int32)\n",
    "            sample_weight = tf.reshape(sample_weight, (-1, 1))\n",
    "            y_true_bin = y_true_bin * sample_weight\n",
    "            y_pred_bin = y_pred_bin * sample_weight\n",
    "        \n",
    "        # Compute per-class true positives, false positives, and false negatives for non-dominant classes.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_bin) * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        \n",
    "        # Our state variables have length num_classes. We want to update only indices 1... with our computed values.\n",
    "        zeros = tf.zeros([1], dtype=tf.float32)\n",
    "        tp_update = tf.concat([zeros, tp], axis=0)\n",
    "        fp_update = tf.concat([zeros, fp], axis=0)\n",
    "        fn_update = tf.concat([zeros, fn], axis=0)\n",
    "        \n",
    "        self.true_positives.assign_add(tp_update)\n",
    "        self.false_positives.assign_add(fp_update)\n",
    "        self.false_negatives.assign_add(fn_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score over the non-dominant classes (indices 1...num_classes-1).\n",
    "        \"\"\"\n",
    "        # Select non-dominant classes only.\n",
    "        tp = self.true_positives[1:]\n",
    "        fp = self.false_positives[1:]\n",
    "        fn = self.false_negatives[1:]\n",
    "        \n",
    "        precision = tf.math.divide_no_nan(tp, tp + fp)\n",
    "        recall = tf.math.divide_no_nan(tp, tp + fn)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            support = tp + fn\n",
    "            weighted_f1 = tf.reduce_sum(f1 * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro\n",
    "            return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n",
    "            \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the metric, so it can be recreated later.\n",
    "        \"\"\"\n",
    "        config = super(CustomNoBackgroundF1Score, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'average': self.average,\n",
    "            'threshold': self.threshold,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomConditionalF1Score(metrics.Metric):\n",
    "    def __init__(self, threshold=0.5, average='weighted', filter_mode='either', name='conditional_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that computes the F1 score only for target columns (columns 1-4).\n",
    "        Additionally, only rows meeting a filtering criterion are included in the calculation.\n",
    "        \n",
    "        Args:\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default = 0.5).\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            filter_mode (str): Determines which rows to include based on the target columns.\n",
    "                               Options:\n",
    "                                  - 'pred': Only rows where y_pred (after thresholding) has at least one 1.\n",
    "                                  - 'true': Only rows where y_true (exactly equal to 1) has at least one 1.\n",
    "                                  - 'either': Rows where either y_true or y_pred has at least one 1.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \n",
    "        Note:\n",
    "            This metric only tracks columns 1-4 (0-indexed). Column 0 (the dominant background class)\n",
    "            is ignored completely.\n",
    "        \"\"\"\n",
    "        metric_name = f'{name}_{filter_mode}'\n",
    "        \n",
    "        super(CustomConditionalF1Score, self).__init__(name=metric_name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "        \n",
    "        if filter_mode not in ['pred', 'true', 'either']:\n",
    "            raise ValueError(\"filter_mode must be 'pred', 'true', or 'either'\")\n",
    "        self.filter_mode = filter_mode\n",
    "        \n",
    "        # We are tracking only 4 target columns (columns 1 to 4).\n",
    "        self.num_target_columns = 4\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Reshape inputs so that the last dimension is the number of classes.\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "        # Only consider columns 1-4 (ignoring index 0).\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        \n",
    "        # For ground truth, treat a label as positive only if its value is exactly 1.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_subset, 1.0), tf.int32)\n",
    "        # For predictions, apply the threshold to decide 1 vs. 0.\n",
    "        y_pred_bin = tf.cast(y_pred_subset >= self.threshold, tf.int32)\n",
    "        \n",
    "        # Compute a row-level mask based on the filter_mode.\n",
    "        if self.filter_mode == 'pred':\n",
    "            mask = tf.reduce_any(tf.equal(y_pred_bin, 1), axis=1)\n",
    "        elif self.filter_mode == 'true':\n",
    "            mask = tf.reduce_any(tf.equal(y_true_bin, 1), axis=1)\n",
    "        else:  # 'either'\n",
    "            mask = tf.logical_or(\n",
    "                tf.reduce_any(tf.equal(y_pred_bin, 1), axis=1),\n",
    "                tf.reduce_any(tf.equal(y_true_bin, 1), axis=1)\n",
    "            )\n",
    "        \n",
    "        # Apply the mask so only selected rows are used for the metric update.\n",
    "        y_true_filtered = tf.boolean_mask(y_true_bin, mask)\n",
    "        y_pred_filtered = tf.boolean_mask(y_pred_bin, mask)\n",
    "        \n",
    "        # Optionally apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.reshape(sample_weight, [-1, 1])\n",
    "            y_true_filtered = y_true_filtered * sample_weight\n",
    "            y_pred_filtered = y_pred_filtered * sample_weight\n",
    "        \n",
    "        # Compute per-column true positives, false positives, and false negatives.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_filtered * y_pred_filtered, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_filtered) * y_pred_filtered, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_filtered * (1 - y_pred_filtered), tf.float32), axis=0)\n",
    "        \n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        precision = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
    "        recall = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            support = self.true_positives + self.false_negatives\n",
    "            return tf.reduce_sum(f1 * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "        else:  # 'macro'\n",
    "            return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = super(CustomConditionalF1Score, self).get_config()\n",
    "        config.update({\n",
    "            'threshold': self.threshold,\n",
    "            'average': self.average,\n",
    "            'filter_mode': self.filter_mode,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomFalsePositiveDistance(metrics.Metric):\n",
    "    def __init__(self, num_classes, threshold=0.5, window=100, name='false_positive_distance', **kwargs):\n",
    "        \"\"\"\n",
    "        Metric that accumulates a running average “distance” error for false positive predictions,\n",
    "        ignoring the dominant (background) class (index 0).\n",
    "\n",
    "        For each false positive (i.e. a prediction >= threshold when the strict label is not 1),\n",
    "        the distance is computed from the raw label value (which encodes proximity to an actual annotation)\n",
    "        as follows:\n",
    "\n",
    "            distance = 1 + ((max_credit - v) * (window / max_credit))\n",
    "\n",
    "        where:\n",
    "            - v is the raw label value at that position,\n",
    "            - max_credit is the maximum smoothing credit (0.5 in our scheme), so that if v == 0.5 the distance is 1,\n",
    "              and if v == 0 the distance is 1 + window (i.e. 101 for window=100).\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            threshold (float): Threshold on y_pred to decide a positive.\n",
    "            window (int): Window size used in the smoothing scheme.\n",
    "            name (str): Name of the metric.\n",
    "        \"\"\"\n",
    "        super(CustomFalsePositiveDistance, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        self.window = float(window)\n",
    "        self.max_credit = 0.5  # Based on smoothing scheme.\n",
    "\n",
    "        # State variables to accumulate total distance and count of false positives.\n",
    "        self.total_distance = self.add_weight(\n",
    "            name='total_distance', initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positive_count = self.add_weight(\n",
    "            name='false_positive_count', initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        For non-dominant classes (indices 1:), this method:\n",
    "          - thresholds predictions,\n",
    "          - identifies false positives (prediction is positive while strict label != 1),\n",
    "          - computes the distance error from the raw (smoothed) label value, and\n",
    "          - accumulates the sum of distances and count of false positives.\n",
    "        \"\"\"\n",
    "        # Ensure shape (batch_size, num_classes)\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "\n",
    "        # Ignore the dominant/background class (index 0)\n",
    "        y_true_non = y_true[:, 1:]\n",
    "        y_pred_non = y_pred[:, 1:]\n",
    "\n",
    "        # Threshold predictions\n",
    "        y_pred_bin = tf.cast(y_pred_non >= self.threshold, tf.float32)\n",
    "\n",
    "        # For strict classification, a label is positive only if it is exactly 1.\n",
    "        # So a false positive is when y_pred_bin==1 but y_true (strict) is not 1.\n",
    "        # (This is similar to the F1 metric, i.e. smoothing values are treated as negatives.)\n",
    "        false_positive_mask = tf.logical_and(\n",
    "            tf.equal(y_pred_bin, 1.0),\n",
    "            tf.not_equal(y_true_non, 1.0)\n",
    "        )\n",
    "        false_positive_mask = tf.cast(false_positive_mask, tf.float32)\n",
    "\n",
    "        # Compute distance per element.\n",
    "        # In our smoothing scheme:\n",
    "        #   - At a true annotation (v = 1), we wouldn’t count a false positive.\n",
    "        #   - In a smoothed region, the maximum credit is 0.5.\n",
    "        #   - We define:\n",
    "        #       distance = 1 + ((max_credit - v) * (window / max_credit))\n",
    "        #     so that if v == 0.5, distance = 1, and if v == 0, distance = 1 + window.\n",
    "        distance = 1.0 + (self.max_credit - y_true_non) * (self.window / self.max_credit)\n",
    "        distance = tf.where(distance >= 101.0, tf.constant(125.0, dtype=distance.dtype), distance)\n",
    "\n",
    "        # Only include entries that are false positives.\n",
    "        false_positive_distance = distance * false_positive_mask\n",
    "\n",
    "        # Sum distances and count false positives.\n",
    "        sum_distance = tf.reduce_sum(false_positive_distance)\n",
    "        count = tf.reduce_sum(false_positive_mask)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.reshape(sample_weight, [-1, 1])\n",
    "            sum_distance = tf.reduce_sum(false_positive_distance * sample_weight)\n",
    "            count = tf.reduce_sum(false_positive_mask * sample_weight)\n",
    "\n",
    "        self.total_distance.assign_add(sum_distance)\n",
    "        self.false_positive_count.assign_add(count)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"Returns the average distance error over all false positives (or 0 if none).\"\"\"\n",
    "        return tf.math.divide_no_nan(self.total_distance, self.false_positive_count)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"Resets the accumulated total distance and count.\"\"\"\n",
    "        self.total_distance.assign(0.0)\n",
    "        self.false_positive_count.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomFalsePositiveDistance, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'threshold': self.threshold,\n",
    "            'window': self.window,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundAUC(metrics.Metric):\n",
    "    def __init__(self, curve='PR', name='no_background_auc', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom AUC metric computed only for columns 1-4.\n",
    "\n",
    "        Args:\n",
    "            curve (str): The type of AUC curve to use, e.g. 'ROC' (default) or 'PR'.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundAUC, self).__init__(name=name, **kwargs)\n",
    "        # Store the curve parameter as a string to aid serialization.\n",
    "        self.curve = curve  \n",
    "        # Create one AUC metric per target column (columns 1-4).\n",
    "        self.auc_metrics = [\n",
    "            metrics.AUC(curve=self.curve, name=f'auc_col_{i+1}')\n",
    "            for i in range(4)\n",
    "        ]\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Ensure inputs are 2D tensors with shape (batch_size, num_classes).\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        # Select target columns (1-4) and ignore background (column 0).\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        # For each target column, update the corresponding AUC metric.\n",
    "        for i, auc_metric in enumerate(self.auc_metrics):\n",
    "            # Ground truth: positive only if exactly equal to 1.\n",
    "            y_true_col = tf.cast(tf.equal(y_true_subset[:, i], 1.0), tf.float32)\n",
    "            y_pred_col = y_pred_subset[:, i]\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = tf.reshape(sample_weight, [-1])\n",
    "                auc_metric.update_state(y_true_col, y_pred_col, sample_weight=sample_weight)\n",
    "            else:\n",
    "                auc_metric.update_state(y_true_col, y_pred_col)\n",
    "\n",
    "    def result(self):\n",
    "        # Average AUC over all target columns.\n",
    "        auc_results = [auc_metric.result() for auc_metric in self.auc_metrics]\n",
    "        return tf.reduce_mean(auc_results)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for auc_metric in self.auc_metrics:\n",
    "            auc_metric.reset_states()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomNoBackgroundAUC, self).get_config()\n",
    "        # Return the curve as a string.\n",
    "        config.update({\n",
    "            'curve': self.curve,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundAccuracy(metrics.Metric):\n",
    "    def __init__(self, threshold=0.5, name='no_background_accuracy', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom accuracy metric computed only for columns 1-4.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Threshold for y_pred (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.total_correct = self.add_weight(name='total_correct', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_count = self.add_weight(name='total_count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Reshape inputs to 2D tensors.\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        # Extract columns 1-4.\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        # Binarize ground truth: positive if exactly 1.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_subset, 1.0), tf.int32)\n",
    "        # Binarize predictions using the threshold.\n",
    "        y_pred_bin = tf.cast(y_pred_subset >= self.threshold, tf.int32)\n",
    "        # Element-wise correctness.\n",
    "        correct = tf.cast(tf.equal(y_true_bin, y_pred_bin), tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            # Tile sample weights to match the shape of correct.\n",
    "            sample_weight = tf.tile(sample_weight, [1, tf.shape(correct)[1]])\n",
    "            correct = correct * sample_weight\n",
    "            count = tf.reduce_sum(sample_weight)\n",
    "        else:\n",
    "            count = tf.cast(tf.size(correct), tf.float32)\n",
    "        self.total_correct.assign_add(tf.reduce_sum(correct))\n",
    "        self.total_count.assign_add(count)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.total_correct, self.total_count)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total_correct.assign(0)\n",
    "        self.total_count.assign(0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomNoBackgroundAccuracy, self).get_config()\n",
    "        config.update({'threshold': self.threshold})\n",
    "        return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundPrecision(metrics.Metric):\n",
    "    def __init__(self, threshold=0.5, average='weighted', name='no_background_precision', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom precision metric computed only for columns 1-4.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Threshold for y_pred (default 0.5).\n",
    "            average (str): 'weighted' (default) or 'macro'.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundPrecision, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "        self.num_target_columns = 4\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        # For weighted averaging, we also need the support (true positives + false negatives).\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Reshape inputs.\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        # Extract target columns (1-4).\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        # Binarize ground truth and predictions.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_subset, 1.0), tf.int32)\n",
    "        y_pred_bin = tf.cast(y_pred_subset >= self.threshold, tf.int32)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.tile(sample_weight, [1, tf.shape(y_true_bin)[1]])\n",
    "            y_true_bin = y_true_bin * tf.cast(sample_weight, tf.int32)\n",
    "            y_pred_bin = y_pred_bin * tf.cast(sample_weight, tf.int32)\n",
    "        # Compute counts per column.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_bin) * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
    "        if self.average == 'weighted':\n",
    "            # Weight each column by its support (TP + FN).\n",
    "            support = self.true_positives + self.false_negatives\n",
    "            weighted_precision = tf.reduce_sum(precision * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_precision\n",
    "        else:  # macro\n",
    "            return tf.reduce_mean(precision)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "        self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
    "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomNoBackgroundPrecision, self).get_config()\n",
    "        config.update({\n",
    "            'threshold': self.threshold,\n",
    "            'average': self.average,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundRecall(metrics.Metric):\n",
    "    def __init__(self, threshold=0.5, average='weighted', name='no_background_recall', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom recall metric computed only for columns 1-4.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Threshold for y_pred (default 0.5).\n",
    "            average (str): 'weighted' (default) or 'macro'.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundRecall, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "        self.num_target_columns = 4\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Reshape inputs.\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        # Extract target columns (1-4).\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        # Binarize ground truth and predictions.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_subset, 1.0), tf.int32)\n",
    "        y_pred_bin = tf.cast(y_pred_subset >= self.threshold, tf.int32)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.tile(sample_weight, [1, tf.shape(y_true_bin)[1]])\n",
    "            y_true_bin = y_true_bin * tf.cast(sample_weight, tf.int32)\n",
    "            y_pred_bin = y_pred_bin * tf.cast(sample_weight, tf.int32)\n",
    "        # Compute per-column true positives and false negatives.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
    "        if self.average == 'weighted':\n",
    "            support = self.true_positives + self.false_negatives\n",
    "            weighted_recall = tf.reduce_sum(recall * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_recall\n",
    "        else:\n",
    "            return tf.reduce_mean(recall)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomNoBackgroundRecall, self).get_config()\n",
    "        config.update({\n",
    "            'threshold': self.threshold,\n",
    "            'average': self.average,\n",
    "        })\n",
    "        return config\n",
    "@utils.register_keras_serializable()\n",
    "class CustomBackgroundOnlyF1Score(metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', threshold=0.5, name='background_only_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers the dominant (background) class (index 0).\n",
    "\n",
    "        This metric is designed for multi-encoded labels where:\n",
    "          - The dominant class (index 0) aka background is represented as a hard label [1, 0, 0, ...].\n",
    "          - For the dominant class, a label is considered positive only if it is exactly 1.\n",
    "          - Predictions are thresholded (default threshold = 0.5) to decide 1 vs. 0.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            average (str): 'weighted' (default) or 'macro'. (Since only one class is considered, this\n",
    "                           choice won’t make much difference.)\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomBackgroundOnlyF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "\n",
    "        # We still create vectors of length num_classes, but will only update index 0.\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the metric state using only the dominant class (index 0).\n",
    "\n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, num_classes). For the dominant class,\n",
    "                    a label is considered positive only if it is exactly 1.\n",
    "            y_pred: Tensor of shape (batch_size, num_classes) (e.g. probabilities).\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        # Reshape to (-1, num_classes) in case additional dimensions exist.\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "\n",
    "        # Extract the dominant class (index 0)\n",
    "        y_true_dominant = y_true[:, 0]\n",
    "        y_pred_dominant = y_pred[:, 0]\n",
    "\n",
    "        # For ground truth, treat as positive only if exactly equal to 1.\n",
    "        one_value = tf.cast(1.0, dtype=y_true_dominant.dtype)\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_dominant, one_value), tf.float32)\n",
    "\n",
    "        # For predictions, apply thresholding.\n",
    "        y_pred_bin = tf.cast(y_pred_dominant >= self.threshold, tf.float32)\n",
    "\n",
    "        # Optionally apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.reshape(sample_weight, [-1])\n",
    "            y_true_bin = y_true_bin * sample_weight\n",
    "            y_pred_bin = y_pred_bin * sample_weight\n",
    "\n",
    "        # Compute true positives, false positives, and false negatives for the dominant class.\n",
    "        tp = tf.reduce_sum(y_true_bin * y_pred_bin)\n",
    "        fp = tf.reduce_sum((1 - y_true_bin) * y_pred_bin)\n",
    "        fn = tf.reduce_sum(y_true_bin * (1 - y_pred_bin))\n",
    "\n",
    "        # We create update vectors that place the computed scalar at index 0 and zeros elsewhere.\n",
    "        zeros = tf.zeros([self.num_classes - 1], dtype=tf.float32)\n",
    "        tp_update = tf.concat([[tp], zeros], axis=0)\n",
    "        fp_update = tf.concat([[fp], zeros], axis=0)\n",
    "        fn_update = tf.concat([[fn], zeros], axis=0)\n",
    "\n",
    "        self.true_positives.assign_add(tp_update)\n",
    "        self.false_positives.assign_add(fp_update)\n",
    "        self.false_negatives.assign_add(fn_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score for the dominant (background) class (index 0).\n",
    "        \"\"\"\n",
    "        tp = self.true_positives[0]\n",
    "        fp = self.false_positives[0]\n",
    "        fn = self.false_negatives[0]\n",
    "\n",
    "        precision = tf.math.divide_no_nan(tp, tp + fp)\n",
    "        recall = tf.math.divide_no_nan(tp, tp + fn)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "\n",
    "        # Although averaging is not critical with a single class, we mirror the interface.\n",
    "        if self.average == 'weighted':\n",
    "            support = tp + fn\n",
    "            weighted_f1 = tf.math.divide_no_nan(f1 * support, support + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro\n",
    "            return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n",
    "            \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the metric, so it can be recreated later.\n",
    "        \"\"\"\n",
    "        config = super(CustomBackgroundOnlyF1Score, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'average': self.average,\n",
    "            'threshold': self.threshold,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss function, made to get granular control over how the model learns.  In particular, it addresses the background column (dominant class) and \"smoothing\" data that the basic keras binary focal loss function would not handle well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.register_keras_serializable()\n",
    "class CustomBinaryFocalLoss(losses.Loss):\n",
    "    def __init__(self,\n",
    "                 dominant_class_index=0,\n",
    "                 # Dominant class multipliers\n",
    "                 dominant_correct_multiplier=0.99,    # Reward when dominant class is correct\n",
    "                 dominant_incorrect_multiplier=2.5,     # Penalty when dominant class is incorrect\n",
    "                 # Expanded non-dominant multipliers for hard labels\n",
    "                 other_class_true_positive_multiplier=0.05,   # Reward when y_true==1 and prediction is positive\n",
    "                 other_class_false_negative_multiplier=3.0,     # Punish when y_true==1 but prediction is negative\n",
    "                 other_class_false_positive_multiplier=1.0,     # Punish when y_true==0 but prediction is positive\n",
    "                 other_class_true_negative_multiplier=0.99,     # Reward when y_true==0 and prediction is negative\n",
    "                 # For smoothed labels (0 < y_true < 1)\n",
    "                 smoothing_multiplier=0.5,              # Scales the effect of a smoothed label\n",
    "                 smoothing_as_correct=True,             # If True, a high prediction on a smoothed label is rewarded; else, punished\n",
    "                 threshold=0.5,                         # Threshold to decide if a prediction is \"positive\"\n",
    "                 # Focal loss parameters\n",
    "                 focal_gamma=2.0,                       # Focusing parameter gamma\n",
    "                 focal_alpha=0.25,                      # Balance parameter alpha\n",
    "                 name=\"custom_binary_focal_loss\",\n",
    "                 reduction=\"sum_over_batch_size\"):\n",
    "        super().__init__(name=name)\n",
    "        self.dominant_class_index = dominant_class_index\n",
    "        self.dominant_correct_multiplier = dominant_correct_multiplier\n",
    "        self.dominant_incorrect_multiplier = dominant_incorrect_multiplier\n",
    "\n",
    "        self.other_class_true_positive_multiplier = other_class_true_positive_multiplier\n",
    "        self.other_class_false_negative_multiplier = other_class_false_negative_multiplier\n",
    "        self.other_class_false_positive_multiplier = other_class_false_positive_multiplier\n",
    "        self.other_class_true_negative_multiplier = other_class_true_negative_multiplier\n",
    "\n",
    "        self.smoothing_multiplier = smoothing_multiplier\n",
    "        self.smoothing_as_correct = smoothing_as_correct\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.focal_alpha = focal_alpha\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Prevent log(0) issues.\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Reshape to (batch_size, num_classes)\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "        # Compute the focal loss elementwise.\n",
    "        # For each element, p_t = y_pred if y_true==1, else 1 - y_pred.\n",
    "        p_t = tf.where(tf.equal(y_true, tf.constant(1.0, dtype=y_true.dtype)), y_pred, 1 - y_pred)\n",
    "        focal_loss = - self.focal_alpha * tf.pow(1 - p_t, self.focal_gamma) * tf.math.log(p_t)\n",
    "        \n",
    "        # Determine the number of classes.\n",
    "        num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Create masks for the dominant vs. non-dominant classes.\n",
    "        dominant_mask = tf.one_hot(self.dominant_class_index, depth=num_classes, dtype=tf.float32)\n",
    "        non_dominant_mask = tf.cast(1.0 - dominant_mask, dtype=tf.float32)\n",
    "        \n",
    "        # === Dominant Class Weighting ===\n",
    "        # For the dominant class, use one multiplier if y_true==1 and another if y_true==0.\n",
    "        dominant_true = y_true[:, self.dominant_class_index]  # shape: (batch_size,)\n",
    "        dominant_weight = tf.where(\n",
    "            tf.equal(dominant_true, tf.constant(1.0, dtype=y_true.dtype)),\n",
    "            tf.constant(self.dominant_correct_multiplier, dtype=y_true.dtype),\n",
    "            tf.constant(self.dominant_incorrect_multiplier, dtype=y_true.dtype)\n",
    "        )\n",
    "        dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # shape: (batch_size, 1)\n",
    "        \n",
    "        # === Non-Dominant Class Weighting ===\n",
    "        # Distinguish between hard labels (exactly 0 or 1) and smoothed labels (0 < y_true < 1).\n",
    "        is_hard_positive = tf.equal(y_true, tf.constant(1.0, dtype=y_true.dtype))\n",
    "        is_hard_negative = tf.equal(y_true, tf.constant(0.0, dtype=y_true.dtype))\n",
    "        is_hard = tf.logical_or(is_hard_positive, is_hard_negative)\n",
    "        \n",
    "        # Determine if the prediction is \"positive\" (i.e. y_pred >= threshold).\n",
    "        pred_positive = tf.greater_equal(y_pred, tf.constant(self.threshold, dtype=y_true.dtype))\n",
    "        \n",
    "        # For hard labels:\n",
    "        #   - If y_true==1:\n",
    "        #       * If prediction is positive: use true positive multiplier.\n",
    "        #       * Else: use false negative multiplier.\n",
    "        #   - If y_true==0:\n",
    "        #       * If prediction is positive: use false positive multiplier.\n",
    "        #       * Else: use true negative multiplier.\n",
    "        hard_weight = tf.where(\n",
    "            tf.equal(y_true, tf.constant(1.0, dtype=y_true.dtype)),\n",
    "            tf.where(\n",
    "                pred_positive,\n",
    "                tf.constant(self.other_class_true_positive_multiplier, dtype=y_true.dtype),\n",
    "                tf.constant(self.other_class_false_negative_multiplier, dtype=y_true.dtype)\n",
    "            ),\n",
    "            tf.where(\n",
    "                tf.equal(y_true, tf.constant(0.0, dtype=y_true.dtype)),\n",
    "                tf.where(\n",
    "                    pred_positive,\n",
    "                    tf.constant(self.other_class_false_positive_multiplier, dtype=y_true.dtype),\n",
    "                    tf.constant(self.other_class_true_negative_multiplier, dtype=y_true.dtype)\n",
    "                ),\n",
    "                tf.constant(1.0, dtype=y_true.dtype)  # fallback; should not occur for a hard label.\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # For smoothed labels: (values strictly between 0 and 1)\n",
    "        is_smoothed = tf.logical_and(\n",
    "            tf.greater(y_true, tf.constant(0.0, dtype=y_true.dtype)),\n",
    "            tf.less(y_true, tf.constant(1.0, dtype=y_true.dtype))\n",
    "        )\n",
    "        if self.smoothing_as_correct:\n",
    "            smoothed_weight = tf.where(\n",
    "                pred_positive,\n",
    "                (1.0 - y_true) * self.smoothing_multiplier,  # reward by reducing the loss, smaller reduction for further distance\n",
    "                1.0 * self.other_class_false_positive_multiplier   # punish for predicting a false positive\n",
    "            )\n",
    "        # elif self.smoothing_as_correct == None:\n",
    "            \n",
    "        else:\n",
    "            smoothed_weight = tf.where(\n",
    "                pred_positive,\n",
    "                1.0 + (1-y_true) * self.smoothing_multiplier,  # punish, punishment increases with distance\n",
    "                1.0 * self.other_class_true_negative_multiplier   # reward for predicting a true negative\n",
    "            )\n",
    "        \n",
    "        # Combine weights for non-dominant classes.\n",
    "        non_dominant_weight = tf.where(\n",
    "            is_hard,\n",
    "            hard_weight,\n",
    "            tf.where(\n",
    "                is_smoothed,\n",
    "                smoothed_weight,\n",
    "                tf.constant(1.0, dtype=y_true.dtype)  # fallback\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Reshape the masks so they broadcast properly.\n",
    "        dominant_mask = tf.reshape(dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "        non_dominant_mask = tf.reshape(non_dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "        \n",
    "        # Combine weights: for each sample and class,\n",
    "        # use dominant_weight for the dominant class and non_dominant_weight for others.\n",
    "        weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "        # Compute the final weighted loss.\n",
    "        weighted_loss = focal_loss * weights\n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'dominant_class_index': self.dominant_class_index,\n",
    "            'dominant_correct_multiplier': self.dominant_correct_multiplier,\n",
    "            'dominant_incorrect_multiplier': self.dominant_incorrect_multiplier,\n",
    "            'other_class_true_positive_multiplier': self.other_class_true_positive_multiplier,\n",
    "            'other_class_false_negative_multiplier': self.other_class_false_negative_multiplier,\n",
    "            'other_class_false_positive_multiplier': self.other_class_false_positive_multiplier,\n",
    "            'other_class_true_negative_multiplier': self.other_class_true_negative_multiplier,\n",
    "            'smoothing_multiplier': self.smoothing_multiplier,\n",
    "            'smoothing_as_correct': self.smoothing_as_correct,\n",
    "            'threshold': self.threshold,\n",
    "            'focal_gamma': self.focal_gamma,\n",
    "            'focal_alpha': self.focal_alpha\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.register_keras_serializable()\n",
    "def tile_to_batch(z):\n",
    "    pe, x = z\n",
    "    return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "def create_dcnn_model(\n",
    "    input_dim=5,\n",
    "    sequence_length=5000,\n",
    "    num_classes=5\n",
    "):\n",
    "    inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "    # Condensed positional encoding block.  See cnn for description\n",
    "    positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    # def tile_to_batch(z):\n",
    "    #     pe, x = z\n",
    "    #     return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "    pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "\n",
    "    concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "    \n",
    "    '''Initial training hyperparameters'''\n",
    "    early_dropout = 0\n",
    "    middle_dropout = 0.1\n",
    "    late_dropout = 0.2\n",
    "\n",
    "    cnn = layers.Conv1D(filters=64, kernel_size=9, activation='relu', padding='same')(concat_input)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Dropout(early_dropout)(cnn)\n",
    "    # Uses six layers with increasing dilation rates to capture a wider receptive field.\n",
    "    # Dilating convolutional blocks with dropout (pooling is bad because exact sequence matters)\n",
    "    skip = concat_input\n",
    "    skip = layers.Conv1D(filters=64, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=1, activation='relu', padding='same')(skip)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(early_dropout)(dcnn)\n",
    "    low_dcnn = dcnn\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(early_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=64, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=128, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "        \n",
    "    second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn, low_dcnn])\n",
    "\n",
    "    # Instead of flattening, use Conv1D with kernel_size=1 as dense layers:\n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(late_dropout)(dense)\n",
    "    \n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(late_dropout)(dense)\n",
    "\n",
    "    # Final classification layer applied at every time step:\n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# This part exists to view the model summary since it is inside a function definition\n",
    "metrics_lst=[\n",
    "        CustomNoBackgroundF1Score(num_classes=5, threshold=0.5, average='weighted'),  # Existing F1 metric\n",
    "        CustomConditionalF1Score(threshold=0.5, average='weighted', filter_mode='pred'),  # 'pred' 'true' or 'either'\n",
    "        CustomConditionalF1Score(threshold=0.5, average='weighted', filter_mode='true'),  # 'pred' 'true' or 'either'\n",
    "        CustomFalsePositiveDistance(num_classes=5, threshold=0.5, window=100),\n",
    "        CustomNoBackgroundAUC(curve='PR'),\n",
    "        CustomNoBackgroundAccuracy(threshold=0.5),\n",
    "        CustomNoBackgroundPrecision(threshold=0.5, average='weighted'),\n",
    "        CustomNoBackgroundRecall(threshold=0.5, average='weighted'),\n",
    "        CustomBackgroundOnlyF1Score(num_classes=5, threshold=0.5, average='weighted')\n",
    "    ]\n",
    "loss_fn = CustomBinaryFocalLoss(\n",
    "        dominant_class_index=0,\n",
    "        dominant_correct_multiplier=0.98,\n",
    "        dominant_incorrect_multiplier=2.5,\n",
    "        other_class_true_positive_multiplier=0.075,\n",
    "        other_class_false_negative_multiplier=5.5,\n",
    "        other_class_false_positive_multiplier=2.0,\n",
    "        other_class_true_negative_multiplier=0.98,\n",
    "        smoothing_multiplier=0.06,\n",
    "        smoothing_as_correct=True,\n",
    "        threshold=0.5,\n",
    "        focal_gamma=2.0,\n",
    "        focal_alpha=0.25\n",
    "    )\n",
    "\n",
    "decay_steps = 10000 # realistically, make this a function of total epochs\n",
    "initial_learning_rate = 0.001\n",
    "warmup_steps = 1000\n",
    "target_learning_rate = 0.01\n",
    "lr_warmup_decayed_fn = optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n",
    "    warmup_steps=warmup_steps\n",
    ")\n",
    "\n",
    "dcnn_model = create_dcnn_model(5, 5000, 5)\n",
    "dcnn_model.compile(\n",
    "                optimizer=optimizers.Adam(learning_rate=lr_warmup_decayed_fn),\n",
    "                loss=loss_fn,\n",
    "                metrics=metrics_lst\n",
    "                  )\n",
    "dcnn_model.summary()\n",
    "\n",
    "# Original training parameters after tuning\n",
    "# 'explore_filters_1': 64, 'explore_filters_2': 160, 'explore_filters_3': 192, 'explore_kernel_size': 9, 'explore_dropout': 0.4, 'learning_rate': 0.0005343042689938801, 'dominant_correct_multiplier': 0.95, 'dominant_incorrect_multiplier': 2.0, \n",
    "#  'other_class_true_positive_multiplier': 0.125, 'other_class_false_negative_multiplier': 5.0, 'other_class_false_positive_multiplier': 2.0, 'other_class_true_negative_multiplier': 1.0, 'smoothing_multiplier': 0.30000000000000004, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is for reading in tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_exact_records(dataset: tf.data.Dataset, total_records, num_to_drop, seed=None):\n",
    "    '''\n",
    "    Function to drop n records from data before constructing parsed dataset.  \n",
    "    Mostly for bug checking.\n",
    "    '''\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    drop_indices = set(np.random.choice(total_records, num_to_drop, replace=False))\n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.filter(lambda i, x: ~tf.reduce_any(tf.equal(i, list(drop_indices))))\n",
    "    dataset = dataset.map(lambda i, x: x)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def parse_chunk_example(serialized_example):\n",
    "    \"\"\"\n",
    "    Parses a single serialized tf.train.Example back into tensors.\n",
    "    Used in testing datasets and in piping tfrecords to DL Algorithms\n",
    "    \"\"\"\n",
    "    feature_spec = {\n",
    "        'X':          tf.io.VarLenFeature(tf.float32),\n",
    "        'y':          tf.io.VarLenFeature(tf.float32),\n",
    "        'record_id':  tf.io.FixedLenFeature([], tf.string),\n",
    "        'cstart':     tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'cend':       tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'strand':     tf.io.FixedLenFeature([], tf.string),\n",
    "        'chunk_size': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    }\n",
    "    \n",
    "    parsed = tf.io.parse_single_example(serialized_example, feature_spec)\n",
    "    \n",
    "    # chunk_size is shape [1]\n",
    "    chunk_size = parsed['chunk_size'][0]\n",
    "    \n",
    "    # Convert sparse to dense\n",
    "    X_flat = tf.sparse.to_dense(parsed['X'])\n",
    "    y_flat = tf.sparse.to_dense(parsed['y'])\n",
    "\n",
    "    # Reshape X to [chunk_size, 5]\n",
    "    X_reshaped = tf.reshape(X_flat, [chunk_size, 5])\n",
    "    # Reshape y to [chunk_size], probably redundant\n",
    "    y_reshaped = tf.reshape(y_flat, [chunk_size, 5])\n",
    "    \n",
    "    record_id = parsed['record_id']\n",
    "    cstart = parsed['cstart'][0]\n",
    "    cend = parsed['cend'][0]\n",
    "    strand = parsed['strand']\n",
    "    \n",
    "    return X_reshaped, y_reshaped, record_id, cstart, cend, strand\n",
    "\n",
    "\n",
    "def prepare_for_model(X, y, record_id, cstart, cend, strand):\n",
    "    '''\n",
    "    Helper function that extracts and reshapes parsed data for feeding to DL Models\n",
    "    '''\n",
    "    # Expand last dimension of y from (batch_size, 5000) to (batch_size, 5000, 1)\n",
    "    # y = tf.expand_dims(y, axis=-1) turns out this line is not needed\n",
    "    # Return only (X, y). Discard the extra columns for training knowing that \n",
    "    # they still exist in the TestValTrain originals if we need them\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prep_dataset_from_tfrecord(\n",
    "    tfrecord_path,\n",
    "    batch_size=28,\n",
    "    compression_type='GZIP',\n",
    "    shuffled = False,\n",
    "    shuffle_buffer=25000,\n",
    "    total_records=None,\n",
    "    num_to_drop=None,\n",
    "    seed=None\n",
    "):\n",
    "    '''\n",
    "    Imports tfrecord and shuffles it then parses it for use in fitting a model\n",
    "    '''\n",
    "    # Loads in records in a round robin fashion for slightly increased mixing\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path, compression_type=compression_type, num_parallel_reads = tf.data.AUTOTUNE)\n",
    "    \n",
    "    if num_to_drop:\n",
    "        dataset = drop_exact_records(dataset, total_records=total_records, num_to_drop=num_to_drop, seed=seed)\n",
    "    \n",
    "    if shuffled == True:\n",
    "        # Shuffle at the record level\n",
    "        dataset = dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
    "        \n",
    "    \n",
    "    dataset = dataset.map(parse_chunk_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(prepare_for_model, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # dataset = dataset.map(lambda x, y: (x, tf.cast(y, tf.int32))) # found out tensorflow wants int32 in y # Note: Not anymore due to change in label format\n",
    "\n",
    "    # Rebatch parsed and prefetch for efficient reading\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines callbacks for running the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimit(callbacks.Callback):\n",
    "    def __init__(self, max_time_seconds):\n",
    "        super().__init__()\n",
    "        self.max_time_seconds = max_time_seconds\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    # def on_batch_end(self, batch, logs=None):\n",
    "    #     if time.time() - self.start_time > self.max_time_seconds:\n",
    "    #         self.model.stop_training = True\n",
    "    \n",
    "    # def on_train_batch_end(self, batch, logs=None):  # ✅ Runs more frequently than `on_batch_end`\n",
    "    #     elapsed_time = time.time() - self.start_time\n",
    "    #     if elapsed_time > self.max_time_seconds:\n",
    "    #         print(f\"\\n⏳ Time limit of {self.max_time_seconds} sec reached. Stopping training!\")\n",
    "    #         self.model.stop_training = True  # 🔥 Stops training mid-batch\n",
    "    \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if elapsed_time > self.max_time_seconds:\n",
    "            print(f\"\\n⏳ Time limit of {self.max_time_seconds} sec reached. Stopping training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):  # New method added\n",
    "        if time.time() - self.start_time > self.max_time_seconds:\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class DebugCallback(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\n🚀 Starting Epoch {epoch+1}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if batch % 1000 == 0:\n",
    "            print(f\"🔄 Processing Batch {batch}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % 1000 == 0:\n",
    "            print(f\"✅ Finished Batch {batch}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\n🏁 Epoch {epoch+1} Completed!\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "class CleanupCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Example: force garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # If you need more extensive cleanup, you can add it here.\n",
    "        # e.g., close files, flush logs, free external resources, etc.\n",
    "        print(f\"Cleanup done at the end of epoch {epoch+1}\")\n",
    "        \n",
    "\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    filepath=models_path + 'checkpoints/epoch-{epoch:03d}-val_no_background_f1-{val_no_background_f1:.4f}.keras',\n",
    "    # monitor='val_loss',          # what metric to name file on\n",
    "    monitor='val_no_background_f1',\n",
    "    mode='max',                    # Required for monitoring f1, comment out if monitoring val loss\n",
    "    save_best_only=False,        # save model always \n",
    "    save_weights_only=False,     # save full model (architecture + weights)\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "early_stopping_cb = callbacks.EarlyStopping(\n",
    "    # monitor='val_loss',\n",
    "    monitor='val_no_background_f1',\n",
    "    mode='max',\n",
    "    patience=20,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "class CleanupCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Example: force garbage collection\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell sets up the tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.register_keras_serializable()\n",
    "def create_dcnn_model_tunable(\n",
    "    input_dim=5,\n",
    "    sequence_length=5000,\n",
    "    num_classes=5,\n",
    "    early_dropout_val=0.0,\n",
    "    middle_dropout_val=0.1,\n",
    "    late_dropout_val=0.2\n",
    "):\n",
    "    K.clear_session()\n",
    "    \n",
    "    inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "    # Positional encoding block.\n",
    "    positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "    \n",
    "    concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "    \n",
    "    # First convolution branch.\n",
    "    cnn = layers.Conv1D(filters=64, kernel_size=9, activation='relu', padding='same')(concat_input)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Dropout(early_dropout_val)(cnn)\n",
    "    \n",
    "    # Dilated convolution blocks.\n",
    "    skip = concat_input\n",
    "    skip = layers.Conv1D(filters=64, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=1, activation='relu', padding='same')(skip)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(early_dropout_val)(dcnn)\n",
    "    low_dcnn = dcnn\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(early_dropout_val)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout_val)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout_val)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout_val)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout_val)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=64, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout_val)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=128, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout_val)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn, low_dcnn])\n",
    "    \n",
    "    # \"Dense\" layers implemented as 1D convolutions.\n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(late_dropout_val)(dense)\n",
    "    \n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(late_dropout_val)(dense)\n",
    "    \n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def build_model(hp):\n",
    "    # Tune dropout rates.\n",
    "    early_dropout = hp.Float('early_dropout', min_value=0.0, max_value=0.3, step=0.1, default=0.1)\n",
    "    middle_dropout = hp.Float('middle_dropout', min_value=0.0, max_value=0.3, step=0.1, default=0.1)\n",
    "    late_dropout = hp.Float('late_dropout', min_value=0.1, max_value=0.4, step=0.1, default=0.2)\n",
    "    \n",
    "    # Tune the learning rate or rate scheduler.\n",
    "    # learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log', default=0.000534)\n",
    "    decay_steps = 15000\n",
    "    initial_learning_rate = hp.Float('initial_learning_rate', min_value=0.0001, max_value=0.01, sampling='log', default=0.001)\n",
    "    warmup_steps = hp.Int('warmup_steps', min_value=800, max_value=1500, step=100, default=1000)\n",
    "    target_learning_rate = hp.Float('target_learning_rate', min_value=0.001, max_value=0.1, sampling='log', default=0.01)\n",
    "    lr_warmup_decayed_fn = optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n",
    "        warmup_steps=warmup_steps\n",
    "    )\n",
    "    \n",
    "    # Tune the focal loss parameters.\n",
    "    dominant_correct_multiplier = hp.Float('dominant_correct_multiplier', min_value=0.95, max_value=1.0, step=0.01, default=0.98)\n",
    "    dominant_incorrect_multiplier = hp.Float('dominant_incorrect_multiplier', min_value=1.0, max_value=3.0, step=0.5, default=2.5)\n",
    "    other_class_true_positive_multiplier = hp.Float('other_class_true_positive_multiplier', min_value=0.05, max_value=0.1, step=0.01, default=0.075)\n",
    "    other_class_false_negative_multiplier = hp.Float('other_class_false_negative_multiplier', min_value=5.0, max_value=6.0, step=0.5, default=5.5)\n",
    "    other_class_false_positive_multiplier = hp.Float('other_class_false_positive_multiplier', min_value=1.0, max_value=5.0, step=1.0, default=2.0)\n",
    "    other_class_true_negative_multiplier = hp.Float('other_class_true_negative_multiplier', min_value=0.95, max_value=1.0, step=0.01, default=0.98)\n",
    "    smoothing_multiplier = hp.Float('smoothing_multiplier', min_value=1.0, max_value=3.0, step=0.5, default=0.0)\n",
    "    # Here we keep smoothing_as_correct fixed to False and threshold fixed to 0.5.\n",
    "    focal_gamma = hp.Float('focal_gamma', min_value=1.0, max_value=4.0, step=0.5, default=2.0)\n",
    "    focal_alpha = hp.Float('focal_alpha', min_value=0.05, max_value=0.5, step=0.05, default=0.25)\n",
    "    \n",
    "    # Build the model with the tunable dropout parameters.\n",
    "    model = create_dcnn_model_tunable(\n",
    "        input_dim=5,\n",
    "        sequence_length=5000,\n",
    "        num_classes=5,\n",
    "        early_dropout_val=early_dropout,\n",
    "        middle_dropout_val=middle_dropout,\n",
    "        late_dropout_val=late_dropout\n",
    "    )\n",
    "    \n",
    "    # Instantiate custom loss with the tuning parameters.\n",
    "    loss_fn = CustomBinaryFocalLoss(\n",
    "        dominant_class_index=0,\n",
    "        dominant_correct_multiplier=dominant_correct_multiplier,\n",
    "        dominant_incorrect_multiplier=dominant_incorrect_multiplier,\n",
    "        other_class_true_positive_multiplier=other_class_true_positive_multiplier,\n",
    "        other_class_false_negative_multiplier=other_class_false_negative_multiplier,\n",
    "        other_class_false_positive_multiplier=other_class_false_positive_multiplier,\n",
    "        other_class_true_negative_multiplier=other_class_true_negative_multiplier,\n",
    "        smoothing_multiplier=smoothing_multiplier,\n",
    "        smoothing_as_correct=False,\n",
    "        threshold=0.5,\n",
    "        focal_gamma=focal_gamma,\n",
    "        focal_alpha=focal_alpha\n",
    "    )\n",
    "    \n",
    "    metrics_lst=[\n",
    "        CustomNoBackgroundF1Score(num_classes=5, threshold=0.5, average='weighted'),  # Existing F1 metric\n",
    "        CustomConditionalF1Score(threshold=0.5, average='weighted', filter_mode='pred'),  # 'pred' 'true' or 'either'\n",
    "        CustomConditionalF1Score(threshold=0.5, average='weighted', filter_mode='true'),  # 'pred' 'true' or 'either'\n",
    "        CustomFalsePositiveDistance(num_classes=5, threshold=0.5, window=100),\n",
    "        CustomNoBackgroundAUC(curve='PR'),\n",
    "        CustomNoBackgroundAccuracy(threshold=0.5),\n",
    "        CustomNoBackgroundPrecision(threshold=0.5, average='weighted'),\n",
    "        CustomNoBackgroundRecall(threshold=0.5, average='weighted'),\n",
    "        CustomBackgroundOnlyF1Score(num_classes=5, threshold=0.5, average='weighted')\n",
    "    ]\n",
    "    \n",
    "    # Define optimizer using the tuned learning rate.\n",
    "    optimizer = optimizers.Adam(learning_rate=lr_warmup_decayed_fn)\n",
    "    \n",
    "    # To load in model weights for re-tuning\n",
    "    # model.load_weights(models_path + 'checkpoints/epoch-032-val_no_background_f1-0.6029.keras')\n",
    "    \n",
    "    # You can use the same metrics list defined earlier.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=metrics_lst\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Set up the tuner. For example, using Hyperband:\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"val_false_positive_distance\", direction=\"min\"),\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory=models_path + 'Tuning Data/DCNN_Tuner_11',\n",
    "    project_name='dcnn_tuning',\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell runs the tuning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time_seconds = 3600/2  # 1 hour is 3600 seconds\n",
    "batch_size = 28\n",
    "epochs = 10  # Set high enough to allow stopping by time\n",
    "steps_per_epoch = 1500\n",
    "\n",
    "print('Compiling train dataset')\n",
    "train_dataset = prep_dataset_from_tfrecord(datasets_path + \"TestValTrain/train.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=True,\n",
    "                                shuffle_buffer=5000,\n",
    "                                total_records=200985,\n",
    "                                num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "                                )\n",
    "train_dataset = train_dataset.repeat() # This is needed here because tuning a full epoch was not working and resulted in crashes.\n",
    "\n",
    "print('Compiling val dataset')\n",
    "val_dataset = prep_dataset_from_tfrecord(datasets_path + \"TestValTrain/val.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=False,\n",
    "                                shuffle_buffer=5000,\n",
    "                                total_records=23645,\n",
    "                                num_to_drop=13, # Batch size 28 leaves remainder of 13 records\n",
    "                                seed=42 # Seed for dropping the same 13 records every time\n",
    "                                )\n",
    "\n",
    "\n",
    "\n",
    "# stop_early = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "time_limit_callback = TimeLimit(max_time_seconds=max_time_seconds)\n",
    "\n",
    "tuner.search(train_dataset, epochs=10, steps_per_epoch = steps_per_epoch, validation_data=val_dataset, callbacks=[early_stopping_cb, time_limit_callback, CleanupCallback()])\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model and hyperparameters:\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This cell evaluates the validation data with each checkpoint'''\n",
    "\n",
    "max_time_seconds = 3600*12  # 1 hour is 3600 seconds\n",
    "batch_size = 28\n",
    "epochs = 400  # Set high enough to allow stopping by callback\n",
    "steps_per_epoch = 7178\n",
    "\n",
    "# print('Compiling train dataset')\n",
    "# train_dataset = prep_dataset_from_tfrecord(datasets_path + \"TestValTrain/train.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=True,\n",
    "#                                 shuffle_buffer=10000,\n",
    "#                                 total_records=200985,\n",
    "#                                 num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "#                                 )\n",
    "# train_dataset = train_dataset.repeat()\n",
    "\n",
    "print('Compiling val dataset')\n",
    "val_dataset = prep_dataset_from_tfrecord(datasets_path + \"TestValTrain/val.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=False,\n",
    "                                shuffle_buffer=5000,\n",
    "                                total_records=23645,\n",
    "                                num_to_drop=13, # Batch size 28 leaves remainder of 13 records\n",
    "                                seed=42 # Seed for dropping the same 13 records every time\n",
    "                                )\n",
    "\n",
    "\n",
    "loss_fn = loss_fn\n",
    "\n",
    "metrics_lst=[\n",
    "        CustomNoBackgroundF1Score(num_classes=5, threshold=0.5, average='weighted'),  # Existing F1 metric\n",
    "        CustomConditionalF1Score(threshold=0.5, average='weighted', filter_mode='pred'),  # 'pred' 'true' or 'either'\n",
    "        CustomConditionalF1Score(threshold=0.5, average='weighted', filter_mode='true'),  # 'pred' 'true' or 'either'\n",
    "        CustomFalsePositiveDistance(num_classes=5, threshold=0.5, window=100),\n",
    "        CustomNoBackgroundAUC(curve='PR'),\n",
    "        CustomNoBackgroundAccuracy(threshold=0.5),\n",
    "        CustomNoBackgroundPrecision(threshold=0.5, average='weighted'),\n",
    "        CustomNoBackgroundRecall(threshold=0.5, average='weighted'),\n",
    "        CustomBackgroundOnlyF1Score(num_classes=5, threshold=0.5, average='weighted')\n",
    "    ]\n",
    "\n",
    "# Define checkpoint directory\n",
    "checkpoint_dir = models_path + \"checkpoints\"\n",
    "\n",
    "# Store results in a list\n",
    "results_list = []\n",
    "\n",
    "# Loop through all saved model files\n",
    "for filename in sorted(os.listdir(checkpoint_dir)):  # Ensure sorted order\n",
    "    if filename.endswith(\".keras\"):  # Adjust if using TensorFlow checkpoints\n",
    "        model_path = os.path.join(checkpoint_dir, filename)\n",
    "        print(f\"Evaluating {filename}...\")\n",
    "\n",
    "        # Load the model (Include custom loss/metrics if necessary)\n",
    "        model = models.load_model(model_path) \n",
    "        model.compile(\n",
    "                    loss=loss_fn,\n",
    "                    metrics=metrics_lst\n",
    "                    )\n",
    "            \n",
    "\n",
    "        # Evaluate on validation dataset\n",
    "        results = model.evaluate(val_dataset, verbose=1)  # Suppress output\n",
    "\n",
    "        # Store results (Modify column names as needed)\n",
    "        results_list.append({\"Checkpoint\": filename, \"Results\" : results})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_results = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)\n",
    "\n",
    "# Save the DataFrame to a CSV file for later analysis after renaming\n",
    "# df_results.to_csv(models_path + \"Results/validation_results.csv\", index=False)\n",
    "\n",
    "# print(\"Validation results saved to 'Results/validation_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
