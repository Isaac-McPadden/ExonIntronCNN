{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook that converts fasta and gtf data into tfrecords.  Uses parallel processing to make the processing time more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First cell defines utility functions and a function that converts fasta and gtf data into encoded arrays with DNA location info as annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gtf_annotations(gtf_file):\n",
    "    \"\"\"\n",
    "    Loads GTF into a pandas DataFrame and converts cstart and cend to zero-based indexing.\n",
    "    \"\"\"\n",
    "    gtf_data = pd.read_csv(\n",
    "        gtf_file, sep='\\t', comment='#', header=None,\n",
    "        names=['seqname', 'source', 'feature', 'cstart', 'cend', \n",
    "               'score', 'strand', 'frame', 'attribute']\n",
    "    )\n",
    "    # Convert to zero-based indexing for cstart\n",
    "    gtf_data['cstart'] = gtf_data['cstart'] - 1\n",
    "    return gtf_data\n",
    "\n",
    "\n",
    "# def compute_chunk_indices(fasta_file, chunk_size):\n",
    "#     \"\"\"\n",
    "#     Creates a list of (record_id, cstart, cend) for each chunk in the FASTA.\n",
    "#     This version does not take window shifts\n",
    "#     \"\"\"\n",
    "#     print('Running compute_chunk_indices')\n",
    "    \n",
    "#     fa = Fasta(fasta_file)  # for indexed random access\n",
    "#     chunk_indices = []\n",
    "#     for record_id in fa.keys():\n",
    "#         seq_len = len(fa[record_id])\n",
    "#         for cstart in range(0, seq_len, chunk_size):\n",
    "#             cend = min(cstart + chunk_size, seq_len)\n",
    "#             chunk_indices.append((record_id, cstart, cend))\n",
    "#     return chunk_indices\n",
    "\n",
    "\n",
    "def compute_chunk_indices(fasta_file, chunk_size, shifts=[0]):\n",
    "    \"\"\"\n",
    "    Creates a list of (record_id, cstart, cend) for each chunk in the FASTA,\n",
    "    applying additional shifts to augment the dataset.  Shifts takes a list of window shift values.  \n",
    "    \"\"\"\n",
    "    print('Running compute_chunk_indices with data augmentation')\n",
    "    fa = Fasta(fasta_file)  # for indexed random access\n",
    "    chunk_indices = []\n",
    "    for record_id in fa.keys():\n",
    "        seq_len = len(fa[record_id])\n",
    "        for shift in shifts:\n",
    "            # Start at the given shift, then step by chunk_size\n",
    "            for cstart in range(shift, seq_len, chunk_size):\n",
    "                cend = min(cstart + chunk_size, seq_len)\n",
    "                chunk_indices.append((record_id, cstart, cend))\n",
    "    return chunk_indices\n",
    "\n",
    "\n",
    "def one_hot_encode_reference(sequence):\n",
    "    \"\"\"\n",
    "    Returns a list of 4-element lists for each base (A, C, G, T).\n",
    "    \"\"\"\n",
    "    n_base_encoder = {\n",
    "        'A': [1, 0, 0, 0],\n",
    "        'C': [0, 1, 0, 0],\n",
    "        'G': [0, 0, 1, 0],\n",
    "        'T': [0, 0, 0, 1],\n",
    "        'N': [0, 0, 0, 0],  \n",
    "        }\n",
    "    return [n_base_encoder.get(nuc, [0, 0, 0, 0]) for nuc in sequence]\n",
    "\n",
    "\n",
    "def label_sequence_local(sequence_length, annotations, window=50):\n",
    "    \"\"\"\n",
    "    Builds a label matrix with partial credit for boundary annotations and a background channel\n",
    "    that is set to 0 if any full annotation (value 1) is present in the other channels, or 1\n",
    "    if only partial credit is present.\n",
    "    \n",
    "    For each annotation (e.g. an exon start), the target values are assigned as follows:\n",
    "      - At the annotated base: 1.0.\n",
    "      - At positions 1 base away: 0.5.\n",
    "      - At positions at distance d (for d=2,...,window):\n",
    "            credit = 0.5 - 0.01*(d-1)\n",
    "        so that at distance 50 the credit is 0.5 - 0.01*(50-1) = 0.01.\n",
    "    \n",
    "    When two annotations of the same channel are nearby, each base’s final target is the maximum\n",
    "    credit received from any annotation.\n",
    "    \n",
    "    Channels are assigned as:\n",
    "      - Column 0: non‑coding (background).  \n",
    "          * It is set to 0 when any of the other channels equals 1.\n",
    "          * If none of the other channels have a full (1.0) annotation, background is left at 1.\n",
    "      - Column 1: intron cstart (annotation at cstart for feature \"intron\")\n",
    "      - Column 2: intron cend (annotation at cend-1 for feature \"intron\")\n",
    "      - Column 3: exon cstart (annotation at cstart for feature \"exon\")\n",
    "      - Column 4: exon cend (annotation at cend-1 for feature \"exon\")\n",
    "    \n",
    "    Parameters:\n",
    "      sequence_length (int): Length of the sequence.\n",
    "      annotations (DataFrame): Must have columns 'cstart', 'cend', and 'feature'.\n",
    "      window (int): How far (in bases) the partial credit is spread (default 50).\n",
    "    \n",
    "    Returns:\n",
    "      np.ndarray: A (sequence_length x 5) label matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create binary arrays for the four annotation channels.\n",
    "    exon_cstart_binary   = np.zeros(sequence_length)\n",
    "    exon_cend_binary     = np.zeros(sequence_length)\n",
    "    intron_cstart_binary = np.zeros(sequence_length)\n",
    "    intron_cend_binary   = np.zeros(sequence_length)\n",
    "    \n",
    "    # Mark exact positions from annotations.\n",
    "    for _, row in annotations.iterrows():\n",
    "        cs = int(row['cstart'])\n",
    "        ce = int(row['cend'])\n",
    "        feat = row['feature'].strip().lower()\n",
    "        if feat == 'exon':\n",
    "            if 0 <= cs < sequence_length:\n",
    "                exon_cstart_binary[cs] = 1\n",
    "            if 0 <= ce - 1 < sequence_length:\n",
    "                exon_cend_binary[ce - 1] = 1\n",
    "        elif feat == 'intron':\n",
    "            if 0 <= cs < sequence_length:\n",
    "                intron_cstart_binary[cs] = 1\n",
    "            if 0 <= ce - 1 < sequence_length:\n",
    "                intron_cend_binary[ce - 1] = 1\n",
    "\n",
    "    def smooth_binary(binary_arr, window):\n",
    "        \"\"\"\n",
    "        Given a binary array (with 1’s at annotated positions), create a custom \n",
    "        \"smoothed\" array where an annotation at position i contributes:\n",
    "          - 1.0 at position i,\n",
    "          - 0.5 at positions i ± 1,\n",
    "          - and for positions i ± d (with 2 <= d <= window):\n",
    "                0.5 - 0.01*(d-1)\n",
    "        Contributions from multiple annotations are combined via max().\n",
    "        \"\"\"\n",
    "        L = len(binary_arr)\n",
    "        smooth_arr = np.zeros(L)\n",
    "        # Find indices where an annotation is present.\n",
    "        annotation_indices = np.where(binary_arr == 1)[0]\n",
    "        for idx in annotation_indices:\n",
    "            # Annotated base gets full credit.\n",
    "            smooth_arr[idx] = 1.0\n",
    "            # Spread out to left and right.\n",
    "            for d in range(1, window + 1):\n",
    "                credit = 0.5 - (0.5 / window) * (d - 1)\n",
    "                # Ensure credit is not negative.\n",
    "                credit = max(credit, 0)\n",
    "                left = idx - d\n",
    "                right = idx + d\n",
    "                if left >= 0:\n",
    "                    smooth_arr[left] = max(smooth_arr[left], credit)\n",
    "                if right < L:\n",
    "                    smooth_arr[right] = max(smooth_arr[right], credit)\n",
    "        return smooth_arr\n",
    "\n",
    "    # Smooth each binary channel.\n",
    "    exon_cstart_smooth   = smooth_binary(exon_cstart_binary, window)\n",
    "    exon_cend_smooth     = smooth_binary(exon_cend_binary, window)\n",
    "    intron_cstart_smooth = smooth_binary(intron_cstart_binary, window)\n",
    "    intron_cend_smooth   = smooth_binary(intron_cend_binary, window)\n",
    "    \n",
    "    # Build the full label matrix.\n",
    "    # Columns: [non-coding, intron cstart, intron cend, exon cstart, exon cend]\n",
    "    labels = np.zeros((sequence_length, 5))\n",
    "    labels[:, 1] = intron_cstart_smooth\n",
    "    labels[:, 2] = intron_cend_smooth\n",
    "    labels[:, 3] = exon_cstart_smooth\n",
    "    labels[:, 4] = exon_cend_smooth\n",
    "    \n",
    "    # For each base:\n",
    "    # - If any annotation channel is exactly 1, set background to 0.\n",
    "    # - Otherwise (only partial credits present), leave background as 1.\n",
    "    max_annotation = np.max(labels[:, 1:], axis=1)\n",
    "    # Where max_annotation is 1, background = 0; otherwise background = 1.\n",
    "    labels[:, 0] = np.where(max_annotation == 1, 0, 1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def pad_labels(labels, target_length):\n",
    "    \"\"\"\n",
    "    Pads a NumPy label array (shape: [current_length, 5]) up to target_length\n",
    "    by adding rows of [1, 0, 0, 0, 0] (representing non-coding).\n",
    "    \"\"\"\n",
    "    current_length = len(labels)\n",
    "    if current_length < target_length:\n",
    "        pad_length = target_length - current_length\n",
    "        # Create an array with pad_length rows of the non-coding label.\n",
    "        pad_array = np.tile(np.array([[1, 0, 0, 0, 0]]), (pad_length, 1))\n",
    "        labels = np.concatenate([labels, pad_array], axis=0)\n",
    "        labels = labels.tolist()\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def pad_encoded_seq(encoded_seq, target_length):\n",
    "    \"\"\"\n",
    "    Pads sequence of shape (seq_len, 5) up to (target_length, 5) with zeros.\n",
    "    \"\"\"\n",
    "    seq_len = len(encoded_seq)\n",
    "    pad_size = target_length - seq_len\n",
    "    if pad_size > 0:\n",
    "        encoded_seq += [[0, 0, 0, 0, 0]] * pad_size\n",
    "    return encoded_seq\n",
    "\n",
    "\n",
    "def build_chunk_data_for_indices(fasta_file, gtf_df, subset_indices, skip_empty=True, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    For each chunk (record_id, cstart, cend) in the FASTA:\n",
    "      - Extract the reference sequence\n",
    "      - For each strand (+ and -):\n",
    "          1) Create a [chunk_size x 5] input: 4 channels for bases, 1 for strand\n",
    "          2) Label the chunk (0..chunk_size-1) using the GTF annotations that\n",
    "             fall on this chunk AND on this strand\n",
    "          3) If skip_empty=True and all labels are background, skip\n",
    "          4) Yield (X, y) or store it somewhere\n",
    "\n",
    "    Returns: generator of (X, y, record_id, chunk_cstart, chunk_cend, strand, chunk_size)\n",
    "    \"\"\"\n",
    "    print('running build_chunk_data_for_indices')\n",
    "    \n",
    "    fa = Fasta(fasta_file)\n",
    "\n",
    "    # Pre-group GTF by (seqname, strand) to speed up filtering\n",
    "    grouped_gtf = {}\n",
    "    for (seqname, strand), sub_df in gtf_df.groupby(['seqname', 'strand']):\n",
    "        grouped_gtf[(seqname, strand)] = sub_df\n",
    "\n",
    "        \n",
    "    for (record_id, cstart, cend) in subset_indices:\n",
    "        # Read the reference chunk\n",
    "        seq = str(fa[record_id][cstart:cend])  # raw bases from reference\n",
    "        base_encoded_4 = one_hot_encode_reference(seq)  # shape => (chunk_len, 4)\n",
    "\n",
    "        chunk_len = len(base_encoded_4)  # could be < chunk_size if at the cend of the chromosome\n",
    "\n",
    "        for strand_symbol in ['+', '-']:\n",
    "            # append the 5th channel for strand\n",
    "            # 1 for +, 0 for -:\n",
    "            strand_flag = 1 if strand_symbol == '+' else 0\n",
    "            encoded_seq_5 = [row + [strand_flag] for row in base_encoded_4]\n",
    "\n",
    "            # Filter GTF for (record_id, strand_symbol)\n",
    "            if (record_id, strand_symbol) not in grouped_gtf:\n",
    "                # No annotations for that contig+strand => all labels=[[1, 0, 0, 0, 0]]\n",
    "                labels = [[1, 0, 0, 0, 0]]*chunk_len\n",
    "            else:\n",
    "                sub_df = grouped_gtf[(record_id, strand_symbol)]\n",
    "                # Keep only rows that overlap [cstart, cend)\n",
    "                # Then shift 'cstart' and 'cend' to local coordinates\n",
    "                overlap = sub_df[\n",
    "                    (sub_df['cstart'] < cend) & \n",
    "                    (sub_df['cend'] > cstart)\n",
    "                ].copy()\n",
    "\n",
    "                if len(overlap) == 0:\n",
    "                    labels = [[1, 0, 0, 0, 0]]*chunk_len\n",
    "                else:\n",
    "                    # Shift coords so that cstart => 0\n",
    "                    overlap['cstart'] = overlap['cstart'] - cstart\n",
    "                    overlap['cend']   = overlap['cend']   - cstart\n",
    "\n",
    "                    # Now label them in local chunk coords\n",
    "                    labels = label_sequence_local(chunk_len, overlap, window=100)\n",
    "                    labels = labels.tolist()\n",
    "\n",
    "            # Optionally skip if all labels=0 and skip_empty=True\n",
    "            if skip_empty and all(lbl == [1, 0, 0, 0, 0] for lbl in labels):\n",
    "                continue\n",
    "\n",
    "            # Pad up to chunk_size if needed\n",
    "            encoded_seq_5 = pad_encoded_seq(encoded_seq_5, chunk_size)\n",
    "            labels = pad_labels(labels, chunk_size)\n",
    "\n",
    "            # Convert to np.array for deep learning frameworks\n",
    "            X = np.array(encoded_seq_5, dtype=np.float32)   # shape [chunk_size, 5]\n",
    "            y = np.array(labels, dtype=np.float32)            # shape [chunk_size]\n",
    "            \n",
    "            # Passing chunk_size through for unzipping the tfrecord later\n",
    "            chunk_size = chunk_size\n",
    "\n",
    "            # Yield or store\n",
    "            yield (X, y, record_id, cstart, cend, strand_symbol, chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second cell defines functions to optimize output of first cell's last function for use in a tfrecord file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_feature_list(value_list):\n",
    "    \"\"\"\n",
    "    Utility to convert a list of floats into a FloatList. Floats are needed for backpropagation, \n",
    "    activation functions, and loss calculations and are thus the default type in TF and PyTorch.\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value_list))\n",
    "\n",
    "\n",
    "def int_feature_list(value_list):\n",
    "    \"\"\"\n",
    "    Utility to convert a list of ints into an Int64List.  Ints fine because this is classification\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value_list))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"\n",
    "    Utility for a single string/bytes feature. For efficient string storage\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.encode('utf-8')\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def serialize_chunk_example(X, y, record_id, cstart, cend, strand_symbol, chunk_size = 5000):\n",
    "    \"\"\"\n",
    "    Converts a single chunk's data into a tf.train.Example protobuf.\n",
    "\n",
    "    :param X: np.array(float32) of shape [chunk_size, 5]\n",
    "    :param y: np.array(int32) of shape [chunk_size]\n",
    "    :param record_id: str (e.g. chromosome name)\n",
    "    :param cstart, cend: int\n",
    "    :param strand_symbol: '+' or '-'\n",
    "\n",
    "    Flattens X and y for storage. Requires parse/reshape at read time.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunk_size = chunk_size\n",
    "    \n",
    "    # Flattens X to 1D and stores it in row-major order.\n",
    "    X_flat = X.flatten().tolist()\n",
    "    \n",
    "    # y is already 1D, but ensure it's a list of int\n",
    "    y_list = y.flatten().tolist()\n",
    "    \n",
    "    # Builds a dictionary of features using utility functions above \n",
    "    # to cast into types preferred by tensorflow\n",
    "    feature_dict = {        \n",
    "        'X':           float_feature_list(X_flat),\n",
    "        'y':           float_feature_list(y_list),\n",
    "        'record_id':   bytes_feature(record_id),\n",
    "        'cstart':      int_feature_list([cstart]),\n",
    "        'cend':        int_feature_list([cend]),\n",
    "        'strand':      bytes_feature(strand_symbol),\n",
    "        'chunk_size':  int_feature_list([chunk_size]),\n",
    "    }\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third cell defines a function that performs row/example generation and breaks writing process into threads.  \n",
    "\n",
    "Uses functions defined in first and second cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_shard_with_threads(\n",
    "    shard_id,\n",
    "    shard_path,\n",
    "    num_shards,\n",
    "    all_indices,\n",
    "    fasta_file,\n",
    "    gtf_df,\n",
    "    compression_type=\"GZIP\",\n",
    "    skip_empty=True,\n",
    "    max_threads_per_process=4,\n",
    "    chunk_size_input = 5000\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes data for a specific shard using threads for concurrent writes.\n",
    "    Data is processed incrementally to avoid loading everything into memory.\n",
    "    \"\"\"\n",
    "    print('Running write_to_shard_with_threads')\n",
    "    \n",
    "    # Filter indices for this shard\n",
    "    subset_indices = [\n",
    "        idx for i, idx in enumerate(all_indices)\n",
    "        if i % num_shards == shard_id\n",
    "    ]\n",
    "    print(f'Shard subset indices gathered for shard {shard_id}')\n",
    "    \n",
    "    # Passing chunk_size through for the unzipping step later\n",
    "    chunk_size_in = chunk_size_input\n",
    "    \n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    writer = tf.io.TFRecordWriter(shard_path, options=options)\n",
    "    lock = threading.Lock()  # Ensure thread-safe writes\n",
    "\n",
    "    def thread_worker(subset_indices_split):\n",
    "        \"\"\"Thread worker to process and write chunks.\"\"\"\n",
    "        for X, y, record_id, cstart, cend, strand_symbol, chunk_size in build_chunk_data_for_indices(\n",
    "            fasta_file, gtf_df, subset_indices_split, skip_empty=skip_empty, chunk_size=chunk_size_in\n",
    "            ):\n",
    "            try:\n",
    "                # Serialize the chunk\n",
    "                example_str = serialize_chunk_example(X, y, record_id, cstart, cend, strand_symbol, chunk_size)\n",
    "                with lock:  # Ensure thread-safe writes\n",
    "                    writer.write(example_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing chunk: {e}\")\n",
    "\n",
    "    # Divide the subset_indices into splits for each thread\n",
    "    subset_splits = [\n",
    "        subset_indices[i::max_threads_per_process] for i in range(max_threads_per_process)\n",
    "    ]\n",
    "\n",
    "    # Start threads\n",
    "    with cf.ThreadPoolExecutor(max_threads_per_process) as thread_executor:\n",
    "        thread_futures = [\n",
    "            thread_executor.submit(thread_worker, subset_split)\n",
    "            for subset_split in subset_splits\n",
    "        ]\n",
    "        \n",
    "        # After executor finishes\n",
    "        for future in thread_futures:\n",
    "            future.result() # Returns None but terminates thread if using 'with' didn't\n",
    "            worker_number = thread_futures.index(future)\n",
    "            print(f'Thread executor {worker_number} completed for process executor {shard_id}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth cell splits data generation into multiple processes and feeds options to lower functions.\n",
    "\n",
    "Uses functions defined in first and third cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord_in_shards_hybrid(\n",
    "    shard_prefix,\n",
    "    fasta_file,\n",
    "    gtf_df,\n",
    "    num_shards=4,\n",
    "    compression_type=\"GZIP\",\n",
    "    max_processes=4,\n",
    "    max_threads_per_process=4,\n",
    "    chunk_size=5000,\n",
    "    skip_empty=True,\n",
    "    shifts = [0]\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes data in multiple TFRecord shards using multiprocessing for shards\n",
    "    and threading within each shard (hybrid).\n",
    "    \n",
    "    Var_name reminders:\n",
    "    :param shard_prefix: Base path for shards, e.g., \"my_chunks\"\n",
    "    :param build_chunk_generator: Generator yielding (X, y, record_id, cstart, cend, strand)\n",
    "    :param num_shards: Number of TFRecord shards to create\n",
    "    :param compression_type: 'GZIP', 'ZLIB', or None\n",
    "    :param max_processes: Number of processes to use for parallel writing\n",
    "    :param max_threads_per_process: Number of threads to use within each process\n",
    "    \"\"\"\n",
    "    print('Running write_tfrecord_in_shards_hybrid')\n",
    "    \n",
    "    # Compute all chunk indices first\n",
    "    all_indices = compute_chunk_indices(fasta_file, chunk_size, shifts=shifts)\n",
    "    print('all_indices calculated')\n",
    "    \n",
    "    # Create shard paths\n",
    "    shard_paths = []\n",
    "    for shard_id in range(num_shards):\n",
    "        shard_path = f\"{shard_prefix}-{shard_id:04d}.tfrecord\"\n",
    "        if compression_type == \"GZIP\":\n",
    "            shard_path += \".gz\"  # Naming convention\n",
    "        shard_paths.append(shard_path)\n",
    "    print(shard_paths)\n",
    "\n",
    "    # Spawn multiple processes (one per shard, or up to max_processes)\n",
    "    with cf.ProcessPoolExecutor(max_workers=max_processes) as process_executor:\n",
    "        futures = []\n",
    "        for shard_id in range(num_shards):\n",
    "            proc = process_executor.submit(\n",
    "                write_to_shard_with_threads,\n",
    "                shard_id,\n",
    "                shard_paths[shard_id],\n",
    "                num_shards,\n",
    "                all_indices,\n",
    "                fasta_file,\n",
    "                gtf_df,\n",
    "                compression_type,\n",
    "                skip_empty,\n",
    "                max_threads_per_process,\n",
    "                chunk_size\n",
    "            )\n",
    "            futures.append(proc)\n",
    "            print(f'Process executor {shard_id} has started')\n",
    "\n",
    "        # After all shards finish\n",
    "        for future in futures:\n",
    "            future.result() # Returns None but terminates thread if using 'with' didn't\n",
    "            shard_number = futures.index(future)\n",
    "            print(f\"Process executor {shard_number} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74 mins 37.5 seconds with compression\n",
    "\n",
    "73 mins 28.6 seconds without compression\n",
    "\n",
    "I/O for compressed is much better than time savings on compression and unzipping.\n",
    "\n",
    "Compressed: 200 MB\n",
    "\n",
    "Not compressed: 5.4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running write_tfrecord_in_shards_hybrid\n",
      "Running compute_chunk_indices with data augmentation\n",
      "all_indices calculated\n",
      "['Final_Optimized_TFRecord_Shards/3334_inex_shard-0000.tfrecord.gz', 'Final_Optimized_TFRecord_Shards/3334_inex_shard-0001.tfrecord.gz', 'Final_Optimized_TFRecord_Shards/3334_inex_shard-0002.tfrecord.gz', 'Final_Optimized_TFRecord_Shards/3334_inex_shard-0003.tfrecord.gz']\n",
      "Process executor 0 has started\n",
      "Process executor 1 has started\n",
      "Process executor 2 has started\n",
      "Process executor 3 has started\n",
      "Running write_to_shard_with_threads\n",
      "Shard subset indices gathered for shard 0\n",
      "running build_chunk_data_for_indicesrunning build_chunk_data_for_indices\n",
      "\n",
      "Running write_to_shard_with_threads\n",
      "Shard subset indices gathered for shard 1\n",
      "running build_chunk_data_for_indicesrunning build_chunk_data_for_indices\n",
      "\n",
      "Running write_to_shard_with_threads\n",
      "Shard subset indices gathered for shard 2\n",
      "running build_chunk_data_for_indicesrunning build_chunk_data_for_indices\n",
      "\n",
      "Running write_to_shard_with_threads\n",
      "Shard subset indices gathered for shard 3\n",
      "running build_chunk_data_for_indicesrunning build_chunk_data_for_indices\n",
      "\n",
      "Thread executor 0 completed for process executor 2.\n",
      "Thread executor 1 completed for process executor 2.\n",
      "Thread executor 0 completed for process executor 0.\n",
      "Thread executor 1 completed for process executor 0.\n",
      "Process executor 0 completed.\n",
      "Thread executor 0 completed for process executor 1.\n",
      "Thread executor 1 completed for process executor 1.\n",
      "Process executor 1 completed.\n",
      "Process executor 2 completed.\n",
      "Thread executor 0 completed for process executor 3.\n",
      "Thread executor 1 completed for process executor 3.\n",
      "Process executor 3 completed.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    my_fasta = 'trim_chr_genome.fa'\n",
    "    my_gtf_df = pd.read_csv(\"FinalIntronExonDF.csv\")\n",
    "    output_directory = \"Final_Optimized_TFRecord_Shards\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # change these every time\n",
    "    shifts = [3334]\n",
    "    my_prefix = output_directory + '/3334_inex_shard'\n",
    "    \n",
    "    write_tfrecord_in_shards_hybrid(\n",
    "        shard_prefix=my_prefix, \n",
    "        fasta_file=my_fasta, \n",
    "        gtf_df=my_gtf_df, \n",
    "        num_shards=4, \n",
    "        compression_type=\"GZIP\", \n",
    "        max_processes=4, \n",
    "        max_threads_per_process=2, \n",
    "        chunk_size=5000, \n",
    "        skip_empty=True,\n",
    "        shifts=shifts\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset size went from 0.79 GB to 1.14 GB after making a few fixes. 89 mins 25 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
