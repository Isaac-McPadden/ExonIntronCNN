{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be2eef24-0682-4300-9a08-ff5c8d4cb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sagemaker ipywidgets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c16590ae-1f18-4177-8c03-cf3541a766f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker, boto3, json\n",
    "# from sagemaker.session import Session\n",
    "\n",
    "# sagemaker_session = Session()\n",
    "# aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "# aws_region = boto3.Session().region_name\n",
    "# sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa955e9c-9915-418c-aee4-6a1dc22d14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = \"sagemaker-studio-084828585863-ktnukrxitg\"\n",
    "# s3 = boto3.client(\"s3\")\n",
    "# s3.download_file(s3_bucket, \"TestValTrain/val.tfrecord.gz\", \"val.tfrecord.gz\")\n",
    "# s3.download_file(s3_bucket, \"TestValTrain/test.tfrecord.gz\", \"test.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831313b-84c4-4c12-99ca-082e6b3e94aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 01:15:45.967754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-17 01:15:45.985560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-17 01:15:45.991629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-17 01:15:46.009082: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 01:15:46.835793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d817c686-ae29-476c-95d6-4b64fe13d51b",
   "metadata": {},
   "source": [
    "s3://sagemaker-studio-084828585863-ktnukrxitg\n",
    "\n",
    "s3://sagemaker-studio-084828585863-ktnukrxitg/TestValTrain/test.tfrecord.gz\n",
    "s3://sagemaker-studio-084828585863-ktnukrxitg/TestValTrain/train.tfrecord.gz\n",
    "s3://sagemaker-studio-084828585863-ktnukrxitg/TestValTrain/val.tfrecord.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3e7aaf-9a42-4491-9504-389ecce681f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundF1Score(metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', threshold=0.5, name='no_background_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers non-dominant classes (ignoring index 0).\n",
    "        \n",
    "        This version is designed for multi-encoded labels where:\n",
    "          - The dominant class (index 0) is represented as a hard label [1, 0, 0, ...]\n",
    "          - For non-dominant classes (indices 1 to num_classes-1), only an exact label of 1 is considered positive.\n",
    "            (Any partial credit/smoothed values below 1 are treated as 0.)\n",
    "          - Predictions are thresholded (default threshold = 0.5) to decide 1 vs. 0.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "\n",
    "        # Create state variables to accumulate counts for each class.\n",
    "        # We use a vector of length num_classes but we will update only indices 1...num_classes-1.\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the metric state.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, num_classes). These are multi-encoded labels.\n",
    "                    For non-dominant classes, a label is considered positive only if it is exactly 1.\n",
    "            y_pred: Tensor of shape (batch_size, num_classes) with predictions (e.g. probabilities).\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Flatten all dimensions except the last one (which should be num_classes).\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "        \n",
    "        # We want to ignore the dominant class (index 0) and work on classes 1...num_classes-1.\n",
    "        # Assume y_true and y_pred are both of shape (batch_size, num_classes).\n",
    "        y_true_non_dominant = y_true[:, 1:]\n",
    "        y_pred_non_dominant = y_pred[:, 1:]\n",
    "        \n",
    "        # For ground truth: treat a class as positive only if its value is exactly 1.\n",
    "        one_value = tf.cast(1.0, dtype=y_true_non_dominant.dtype)\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_non_dominant, one_value), tf.int32)\n",
    "        # For predictions: apply thresholding.\n",
    "        y_pred_bin = tf.cast(y_pred_non_dominant >= self.threshold, tf.int32)\n",
    "        \n",
    "        # (Optionally) apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.int32)\n",
    "            sample_weight = tf.reshape(sample_weight, (-1, 1))\n",
    "            y_true_bin = y_true_bin * sample_weight\n",
    "            y_pred_bin = y_pred_bin * sample_weight\n",
    "        \n",
    "        # Compute per-class true positives, false positives, and false negatives for non-dominant classes.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_bin) * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        \n",
    "        # Our state variables have length num_classes. We want to update only indices 1... with our computed values.\n",
    "        zeros = tf.zeros([1], dtype=tf.float32)\n",
    "        tp_update = tf.concat([zeros, tp], axis=0)\n",
    "        fp_update = tf.concat([zeros, fp], axis=0)\n",
    "        fn_update = tf.concat([zeros, fn], axis=0)\n",
    "        \n",
    "        self.true_positives.assign_add(tp_update)\n",
    "        self.false_positives.assign_add(fp_update)\n",
    "        self.false_negatives.assign_add(fn_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score over the non-dominant classes (indices 1...num_classes-1).\n",
    "        \"\"\"\n",
    "        # Select non-dominant classes only.\n",
    "        tp = self.true_positives[1:]\n",
    "        fp = self.false_positives[1:]\n",
    "        fn = self.false_negatives[1:]\n",
    "        \n",
    "        precision = tf.math.divide_no_nan(tp, tp + fp)\n",
    "        recall = tf.math.divide_no_nan(tp, tp + fn)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            support = tp + fn\n",
    "            weighted_f1 = tf.reduce_sum(f1 * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro\n",
    "            return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n",
    "            \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the metric, so it can be recreated later.\n",
    "        \"\"\"\n",
    "        config = super(CustomNoBackgroundF1Score, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'average': self.average,\n",
    "            'threshold': self.threshold,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fe4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.register_keras_serializable()\n",
    "class CustomFalsePositiveDistance(metrics.Metric):\n",
    "    def __init__(self, num_classes, threshold=0.5, window=100, name='false_positive_distance', **kwargs):\n",
    "        \"\"\"\n",
    "        Metric that accumulates a running average “distance” error for false positive predictions,\n",
    "        ignoring the dominant (background) class (index 0).\n",
    "\n",
    "        For each false positive (i.e. a prediction >= threshold when the strict label is not 1),\n",
    "        the distance is computed from the raw label value (which encodes proximity to an actual annotation)\n",
    "        as follows:\n",
    "\n",
    "            distance = 1 + ((max_credit - v) * (window / max_credit))\n",
    "\n",
    "        where:\n",
    "            - v is the raw label value at that position,\n",
    "            - max_credit is the maximum smoothing credit (0.5 in our scheme), so that if v == 0.5 the distance is 1,\n",
    "              and if v == 0 the distance is 1 + window (i.e. 101 for window=100).\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            threshold (float): Threshold on y_pred to decide a positive.\n",
    "            window (int): Window size used in the smoothing scheme.\n",
    "            name (str): Name of the metric.\n",
    "        \"\"\"\n",
    "        super(CustomFalsePositiveDistance, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        self.window = float(window)\n",
    "        self.max_credit = 0.5  # Based on your smoothing scheme.\n",
    "\n",
    "        # State variables to accumulate total distance and count of false positives.\n",
    "        self.total_distance = self.add_weight(\n",
    "            name='total_distance', initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positive_count = self.add_weight(\n",
    "            name='false_positive_count', initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        For non-dominant classes (indices 1:), this method:\n",
    "          - thresholds predictions,\n",
    "          - identifies false positives (prediction is positive while strict label != 1),\n",
    "          - computes the distance error from the raw (smoothed) label value, and\n",
    "          - accumulates the sum of distances and count of false positives.\n",
    "        \"\"\"\n",
    "        # Ensure shape (batch_size, num_classes)\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "\n",
    "        # Ignore the dominant/background class (index 0)\n",
    "        y_true_non = y_true[:, 1:]\n",
    "        y_pred_non = y_pred[:, 1:]\n",
    "\n",
    "        # Threshold predictions\n",
    "        y_pred_bin = tf.cast(y_pred_non >= self.threshold, tf.float32)\n",
    "\n",
    "        # For strict classification, a label is positive only if it is exactly 1.\n",
    "        # So a false positive is when y_pred_bin==1 but y_true (strict) is not 1.\n",
    "        # (This is similar to your F1 metric, i.e. smoothing values are treated as negatives.)\n",
    "        false_positive_mask = tf.logical_and(\n",
    "            tf.equal(y_pred_bin, 1.0),\n",
    "            tf.not_equal(y_true_non, 1.0)\n",
    "        )\n",
    "        false_positive_mask = tf.cast(false_positive_mask, tf.float32)\n",
    "\n",
    "        # Compute distance per element.\n",
    "        # In our smoothing scheme:\n",
    "        #   - At a true annotation (v = 1), we wouldn’t count a false positive.\n",
    "        #   - In a smoothed region, the maximum credit is 0.5.\n",
    "        #   - We define:\n",
    "        #       distance = 1 + ((max_credit - v) * (window / max_credit))\n",
    "        #     so that if v == 0.5, distance = 1, and if v == 0, distance = 1 + window.\n",
    "        distance = 1.0 + (self.max_credit - y_true_non) * (self.window / self.max_credit)\n",
    "\n",
    "        # Only include entries that are false positives.\n",
    "        false_positive_distance = distance * false_positive_mask\n",
    "\n",
    "        # Sum distances and count false positives.\n",
    "        sum_distance = tf.reduce_sum(false_positive_distance)\n",
    "        count = tf.reduce_sum(false_positive_mask)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.reshape(sample_weight, [-1, 1])\n",
    "            sum_distance = tf.reduce_sum(false_positive_distance * sample_weight)\n",
    "            count = tf.reduce_sum(false_positive_mask * sample_weight)\n",
    "\n",
    "        self.total_distance.assign_add(sum_distance)\n",
    "        self.false_positive_count.assign_add(count)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"Returns the average distance error over all false positives (or 0 if none).\"\"\"\n",
    "        return tf.math.divide_no_nan(self.total_distance, self.false_positive_count)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"Resets the accumulated total distance and count.\"\"\"\n",
    "        self.total_distance.assign(0.0)\n",
    "        self.false_positive_count.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomFalsePositiveDistance, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'threshold': self.threshold,\n",
    "            'window': self.window,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbbea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.register_keras_serializable()\n",
    "class CustomBackgroundOnlyF1Score(metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', threshold=0.5, name='background_only_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers the dominant (background) class (index 0).\n",
    "\n",
    "        This metric is designed for multi-encoded labels where:\n",
    "          - The dominant class (index 0) aka background is represented as a hard label [1, 0, 0, ...].\n",
    "          - For the dominant class, a label is considered positive only if it is exactly 1.\n",
    "          - Predictions are thresholded (default threshold = 0.5) to decide 1 vs. 0.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            average (str): 'weighted' (default) or 'macro'. (Since only one class is considered, this\n",
    "                           choice won’t make much difference.)\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomBackgroundOnlyF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "\n",
    "        # We still create vectors of length num_classes, but will only update index 0.\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the metric state using only the dominant class (index 0).\n",
    "\n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, num_classes). For the dominant class,\n",
    "                    a label is considered positive only if it is exactly 1.\n",
    "            y_pred: Tensor of shape (batch_size, num_classes) (e.g. probabilities).\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        # Reshape to (-1, num_classes) in case additional dimensions exist.\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "\n",
    "        # Extract the dominant class (index 0)\n",
    "        y_true_dominant = y_true[:, 0]\n",
    "        y_pred_dominant = y_pred[:, 0]\n",
    "\n",
    "        # For ground truth, treat as positive only if exactly equal to 1.\n",
    "        one_value = tf.cast(1.0, dtype=y_true_dominant.dtype)\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_dominant, one_value), tf.float32)\n",
    "\n",
    "        # For predictions, apply thresholding.\n",
    "        y_pred_bin = tf.cast(y_pred_dominant >= self.threshold, tf.float32)\n",
    "\n",
    "        # Optionally apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.reshape(sample_weight, [-1])\n",
    "            y_true_bin = y_true_bin * sample_weight\n",
    "            y_pred_bin = y_pred_bin * sample_weight\n",
    "\n",
    "        # Compute true positives, false positives, and false negatives for the dominant class.\n",
    "        tp = tf.reduce_sum(y_true_bin * y_pred_bin)\n",
    "        fp = tf.reduce_sum((1 - y_true_bin) * y_pred_bin)\n",
    "        fn = tf.reduce_sum(y_true_bin * (1 - y_pred_bin))\n",
    "\n",
    "        # We create update vectors that place the computed scalar at index 0 and zeros elsewhere.\n",
    "        zeros = tf.zeros([self.num_classes - 1], dtype=tf.float32)\n",
    "        tp_update = tf.concat([[tp], zeros], axis=0)\n",
    "        fp_update = tf.concat([[fp], zeros], axis=0)\n",
    "        fn_update = tf.concat([[fn], zeros], axis=0)\n",
    "\n",
    "        self.true_positives.assign_add(tp_update)\n",
    "        self.false_positives.assign_add(fp_update)\n",
    "        self.false_negatives.assign_add(fn_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score for the dominant (background) class (index 0).\n",
    "        \"\"\"\n",
    "        tp = self.true_positives[0]\n",
    "        fp = self.false_positives[0]\n",
    "        fn = self.false_negatives[0]\n",
    "\n",
    "        precision = tf.math.divide_no_nan(tp, tp + fp)\n",
    "        recall = tf.math.divide_no_nan(tp, tp + fn)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "\n",
    "        # Although averaging is not critical with a single class, we mirror the interface.\n",
    "        if self.average == 'weighted':\n",
    "            support = tp + fn\n",
    "            weighted_f1 = tf.math.divide_no_nan(f1 * support, support + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro\n",
    "            return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n",
    "            \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the metric, so it can be recreated later.\n",
    "        \"\"\"\n",
    "        config = super(CustomBackgroundOnlyF1Score, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'average': self.average,\n",
    "            'threshold': self.threshold,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2373b0-f99f-4019-a35a-94d810e9bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss(y_true, y_pred):\n",
    "#         # Prevent log(0) issues.\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "#         # Flatten all dimensions except the last one (which should represent the classes).\n",
    "#         y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "#         y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "#         # Compute the standard element-wise binary crossentropy.\n",
    "#         base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "#                        (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "#         # Determine the number of classes.\n",
    "#         num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "#         # Create a one-hot mask for the dominant class.\n",
    "#         # This yields a tensor of shape (num_classes,).\n",
    "#         dominant_mask = tf.one_hot(dominant_class_index, depth=num_classes, dtype=y_true.dtype)\n",
    "#         non_dominant_mask = 1 - dominant_mask\n",
    "        \n",
    "#         # --- Dominant Class Weighting ---\n",
    "#         # For the dominant class: if y_true == 1, use dominant_correct_multiplier; otherwise, use dominant_incorrect_multiplier.\n",
    "#         dominant_true = y_true[:, dominant_class_index]  # Shape: (N,)\n",
    "#         dominant_weight = tf.where(tf.equal(dominant_true, 1.0),\n",
    "#                                    dominant_correct_multiplier,\n",
    "#                                    dominant_incorrect_multiplier)  # Shape: (N,)\n",
    "#         dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # Shape: (N, 1)\n",
    "        \n",
    "#         # --- Non-Dominant Class Weighting ---\n",
    "#         # For non-dominant classes:\n",
    "#         #   - If y_true == 1, use other_class_multiplier.\n",
    "#         #   - If 0 < y_true < 1 (i.e. a smoothed value), use smoothing_multiplier.\n",
    "#         #   - Otherwise (y_true == 0) use 1.\n",
    "#         non_dominant_weight = tf.where(\n",
    "#             tf.equal(y_true, 1.0),\n",
    "#             other_class_multiplier,\n",
    "#             tf.where(tf.greater(y_true, 0.0),\n",
    "#                      smoothing_multiplier,\n",
    "#                      1.0)\n",
    "#         )\n",
    "        \n",
    "#         # Reshape the masks so they broadcast properly with the batch.\n",
    "#         dominant_mask = tf.reshape(dominant_mask, [1, num_classes])\n",
    "#         non_dominant_mask = tf.reshape(non_dominant_mask, [1, num_classes])\n",
    "        \n",
    "#         # Combine the weights: for each sample and each class,\n",
    "#         # the weight is dominant_weight for the dominant class and non_dominant_weight for the others.\n",
    "#         weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "#         # Compute the weighted loss.\n",
    "#         weighted_loss = base_loss * weights\n",
    "#         return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# def custom_binary_crossentropy_loss(\n",
    "#     dominant_class_index=0,\n",
    "#     dominant_correct_multiplier=0.07,    # Reward factor when the dominant class is correct\n",
    "#     dominant_incorrect_multiplier=2.5,    # Penalty factor when the dominant class is predicted incorrectly\n",
    "#     other_class_multiplier=2.0,           # Multiplier for non-dominant classes when y_true == 1\n",
    "#     smoothing_multiplier=0.5              # Multiplier for non-dominant classes when y_true is a smoothed value (0 < y_true < 1)\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Returns a custom binary crossentropy loss function that treats the dominant class specially,\n",
    "#     and applies different multipliers for non-dominant classes based on their true label values.\n",
    "\n",
    "#     For the dominant class (specified by dominant_class_index):\n",
    "#       - If y_true == 1, the loss is scaled by dominant_correct_multiplier.\n",
    "#       - Otherwise, it is scaled by dominant_incorrect_multiplier.\n",
    "\n",
    "#     For non-dominant classes:\n",
    "#       - If y_true == 1, the loss is scaled by other_class_multiplier.\n",
    "#       - If 0 < y_true < 1 (e.g. label-smoothed values), the loss is scaled by smoothing_multiplier.\n",
    "#       - If y_true == 0, no additional scaling is applied.\n",
    "\n",
    "#     This version also reshapes the inputs so that it can handle batches with extra dimensions.\n",
    "\n",
    "#     Parameters:\n",
    "#       dominant_class_index (int): Index of the dominant class in the output vector.\n",
    "#       dominant_correct_multiplier (float): Multiplier for the loss when the dominant class is correctly predicted.\n",
    "#       dominant_incorrect_multiplier (float): Multiplier for the loss when the dominant class is incorrectly predicted.\n",
    "#       other_class_multiplier (float): Multiplier for non-dominant classes when the true label is 1.\n",
    "#       smoothing_multiplier (float): Multiplier for non-dominant classes when the true label is a smoothed value (0 < y_true < 1).\n",
    "\n",
    "#     Returns:\n",
    "#       A callable loss function usable with model.compile(loss=...).\n",
    "#     \"\"\"\n",
    "\n",
    "#     def loss(y_true, y_pred):\n",
    "#         # Prevent log(0) issues.\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "#         # Flatten all dimensions except the last one (which should represent the classes).\n",
    "#         y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "#         y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "#         # Compute the standard element-wise binary crossentropy.\n",
    "#         base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "#                        (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "#         # Determine the number of classes.\n",
    "#         num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "#         # Create a one-hot mask for the dominant class.\n",
    "#         # This yields a tensor of shape (num_classes,).\n",
    "#         dominant_mask = tf.one_hot(dominant_class_index, depth=num_classes, dtype=y_true.dtype)\n",
    "#         non_dominant_mask = 1 - dominant_mask\n",
    "        \n",
    "#         # --- Dominant Class Weighting ---\n",
    "#         # For the dominant class: if y_true == 1, use dominant_correct_multiplier; otherwise, use dominant_incorrect_multiplier.\n",
    "#         dominant_true = y_true[:, dominant_class_index]  # Shape: (N,)\n",
    "#         dominant_weight = tf.where(tf.equal(dominant_true, 1.0),\n",
    "#                                    dominant_correct_multiplier,\n",
    "#                                    dominant_incorrect_multiplier)  # Shape: (N,)\n",
    "#         dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # Shape: (N, 1)\n",
    "        \n",
    "#         # --- Non-Dominant Class Weighting ---\n",
    "#         # For non-dominant classes:\n",
    "#         #   - If y_true == 1, use other_class_multiplier.\n",
    "#         #   - If 0 < y_true < 1 (i.e. a smoothed value), use smoothing_multiplier.\n",
    "#         #   - Otherwise (y_true == 0) use 1.\n",
    "#         non_dominant_weight = tf.where(\n",
    "#             tf.equal(y_true, 1.0),\n",
    "#             other_class_multiplier,\n",
    "#             tf.where(tf.greater(y_true, 0.0),\n",
    "#                      smoothing_multiplier,\n",
    "#                      1.0)\n",
    "#         )\n",
    "        \n",
    "#         # Reshape the masks so they broadcast properly with the batch.\n",
    "#         dominant_mask = tf.reshape(dominant_mask, [1, num_classes])\n",
    "#         non_dominant_mask = tf.reshape(non_dominant_mask, [1, num_classes])\n",
    "        \n",
    "#         # Combine the weights: for each sample and each class,\n",
    "#         # the weight is dominant_weight for the dominant class and non_dominant_weight for the others.\n",
    "#         weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "#         # Compute the weighted loss.\n",
    "#         weighted_loss = base_loss * weights\n",
    "#         return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b355e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @utils.register_keras_serializable()\n",
    "# class CustomBinaryCrossentropyLoss(losses.Loss):\n",
    "#     def __init__(self,\n",
    "#                  dominant_class_index=0,\n",
    "#                  dominant_correct_multiplier=0.07,    # Reward factor when the dominant class is correct\n",
    "#                  dominant_incorrect_multiplier=2.5,     # Penalty factor when the dominant class is predicted incorrectly\n",
    "#                  other_class_multiplier=2.0,            # Multiplier for non-dominant classes when y_true == 1\n",
    "#                  smoothing_multiplier=0.5,              # Multiplier for non-dominant classes when y_true is a smoothed value (0 < y_true < 1)\n",
    "#                  name=\"custom_binary_crossentropy_loss\",\n",
    "#                  reduction=\"sum_over_batch_size\"):\n",
    "#         super().__init__(name=name)\n",
    "#         self.dominant_class_index = dominant_class_index\n",
    "#         self.dominant_correct_multiplier = dominant_correct_multiplier\n",
    "#         self.dominant_incorrect_multiplier = dominant_incorrect_multiplier\n",
    "#         self.other_class_multiplier = other_class_multiplier\n",
    "#         self.smoothing_multiplier = smoothing_multiplier\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         # Prevent log(0) issues.\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "#         # Flatten all dimensions except the last one (which should represent the classes).\n",
    "#         y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "#         y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "#         # Compute the standard element-wise binary crossentropy.\n",
    "#         base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "#                        (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "#         # Determine the number of classes.\n",
    "#         num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "#         # Create a one-hot mask for the dominant class.\n",
    "#         dominant_mask = tf.one_hot(self.dominant_class_index, depth=num_classes, dtype=tf.int32)\n",
    "#         non_dominant_mask = tf.cast(1 - dominant_mask, dtype=tf.float32)\n",
    "        \n",
    "#         # --- Dominant Class Weighting ---\n",
    "#         # For the dominant class: if y_true == 1, use dominant_correct_multiplier; otherwise, use dominant_incorrect_multiplier.\n",
    "#         dominant_true = y_true[:, self.dominant_class_index]  # Shape: (N,)\n",
    "#         dominant_weight = tf.where(tf.equal(dominant_true, 1.0),\n",
    "#                                    self.dominant_correct_multiplier,\n",
    "#                                    self.dominant_incorrect_multiplier)  # Shape: (N,)\n",
    "#         dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # Shape: (N, 1)\n",
    "        \n",
    "#         # --- Non-Dominant Class Weighting ---\n",
    "#         # For non-dominant classes:\n",
    "#         #   - If y_true == 1, use other_class_multiplier.\n",
    "#         #   - If 0 < y_true < 1 (i.e. a smoothed value), use smoothing_multiplier.\n",
    "#         #   - Otherwise (y_true == 0) use 1.\n",
    "#         non_dominant_weight = tf.where(\n",
    "#             tf.equal(y_true, 1.0),\n",
    "#             self.other_class_multiplier,\n",
    "#             tf.where(tf.greater(y_true, 0.0),\n",
    "#                      self.smoothing_multiplier,\n",
    "#                      1.0)\n",
    "#         )\n",
    "        \n",
    "#         # Reshape the masks so they broadcast properly.\n",
    "#         dominant_mask = tf.reshape(dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "#         non_dominant_mask = tf.reshape(non_dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "\n",
    "        \n",
    "#         # Combine the weights: for each sample and each class,\n",
    "#         # the weight is dominant_weight for the dominant class and non_dominant_weight for the others.\n",
    "#         weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "#         # Compute the weighted loss.\n",
    "#         weighted_loss = base_loss * weights\n",
    "#         return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "#     def get_config(self):\n",
    "#             config = super().get_config()\n",
    "#             config.update({\n",
    "#                 'dominant_class_index': self.dominant_class_index,\n",
    "#                 'dominant_correct_multiplier': self.dominant_correct_multiplier,\n",
    "#                 'dominant_incorrect_multiplier': self.dominant_incorrect_multiplier,\n",
    "#                 'other_class_multiplier': self.other_class_multiplier,\n",
    "#                 'smoothing_multiplier': self.smoothing_multiplier\n",
    "#             })\n",
    "#             return config\n",
    "\n",
    "# @utils.register_keras_serializable()\n",
    "# class CustomBinaryCrossentropyLoss(losses.Loss):\n",
    "#     def __init__(self,\n",
    "#                  dominant_class_index=0,\n",
    "#                  dominant_correct_multiplier=0.07,    # Reward factor when the dominant class is correct\n",
    "#                  dominant_incorrect_multiplier=2.5,     # Penalty factor when the dominant class is predicted incorrectly\n",
    "#                  other_class_multiplier=2.0,            # Multiplier for non-dominant classes when y_true == 1\n",
    "#                  smoothing_multiplier=0.5,              # Multiplier for non-dominant classes when y_true is a smoothed value (0 < y_true < 1)\n",
    "#                  name=\"custom_binary_crossentropy_loss\",\n",
    "#                  reduction=\"sum_over_batch_size\"):\n",
    "#         super().__init__(name=name)\n",
    "#         self.dominant_class_index = dominant_class_index\n",
    "#         self.dominant_correct_multiplier = dominant_correct_multiplier\n",
    "#         self.dominant_incorrect_multiplier = dominant_incorrect_multiplier\n",
    "#         self.other_class_multiplier = other_class_multiplier\n",
    "#         self.smoothing_multiplier = smoothing_multiplier\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         # Prevent log(0) issues.\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "#         # Flatten all dimensions except the last one (which should represent the classes).\n",
    "#         y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "#         y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "#         # Compute the standard element-wise binary crossentropy.\n",
    "#         base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "#                        (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "#         # Determine the number of classes.\n",
    "#         num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "#         # Create a one-hot mask for the dominant class.\n",
    "#         dominant_mask = tf.one_hot(self.dominant_class_index, depth=num_classes, dtype=tf.float32)\n",
    "#         non_dominant_mask = tf.cast(tf.constant(1, tf.float32) - dominant_mask, dtype=tf.float32)\n",
    "        \n",
    "#         # --- Dominant Class Weighting ---\n",
    "#         dominant_true = y_true[:, self.dominant_class_index]  # Shape: (N,)\n",
    "#         dominant_weight = tf.where(\n",
    "#                                 tf.equal(dominant_true, tf.constant(1.0, dtype=y_true.dtype)),\n",
    "#                                 tf.constant(self.dominant_correct_multiplier, dtype=y_true.dtype),\n",
    "#                                 tf.constant(self.dominant_incorrect_multiplier, dtype=y_true.dtype)\n",
    "#                             )\n",
    "#                             # Shape: (N,)\n",
    "#         dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # Shape: (N, 1)\n",
    "        \n",
    "#         # --- Non-Dominant Class Weighting ---\n",
    "#         non_dominant_weight = tf.where(\n",
    "#             tf.equal(y_true, tf.constant(1.0, dtype=y_true.dtype)),\n",
    "#             tf.constant(self.other_class_multiplier, dtype=y_true.dtype),\n",
    "#             tf.where(\n",
    "#                 tf.greater(y_true, tf.constant(0.0, dtype=y_true.dtype)),\n",
    "#                 tf.constant(self.smoothing_multiplier, dtype=y_true.dtype),\n",
    "#                 tf.constant(1.0, dtype=y_true.dtype)\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "        \n",
    "#         # Reshape the masks so they broadcast properly.\n",
    "#         dominant_mask = tf.reshape(dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "#         non_dominant_mask = tf.reshape(non_dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "        \n",
    "#         # Combine the weights.\n",
    "#         weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "#         # Compute the weighted loss.\n",
    "#         weighted_loss = base_loss * weights\n",
    "#         return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#             'dominant_class_index': self.dominant_class_index,\n",
    "#             'dominant_correct_multiplier': self.dominant_correct_multiplier,\n",
    "#             'dominant_incorrect_multiplier': self.dominant_incorrect_multiplier,\n",
    "#             'other_class_multiplier': self.other_class_multiplier,\n",
    "#             'smoothing_multiplier': self.smoothing_multiplier\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomBinaryCrossentropyLoss(losses.Loss):\n",
    "    def __init__(self,\n",
    "                 dominant_class_index=0,\n",
    "                 # Multiplier n values (0 < n < 1) reward the loss function, n = 1 makes no effect, n > 1 scales up the punishment. n > 5 is likely to cause instability\n",
    "                 # Shouldn't set to 0 or lower because 0 kills any signal for the gradient to use and negative numbers get weird with logs \n",
    "                 dominant_correct_multiplier=0.99,    # \"Reward\" factor when the dominant class is correct\n",
    "                 dominant_incorrect_multiplier=2.5,     # Penalty factor when the dominant class is predicted incorrectly\n",
    "                 # Expanded non-dominant multipliers for hard labels:\n",
    "                 other_class_true_positive_multiplier=0.05,   # \"Reward\" when y_true==1 and prediction is positive (Strong reward for a bulls-eye)\n",
    "                 other_class_false_negative_multiplier=3.0,    # Punish when y_true==1 but prediction is negative (Punish a miss on a rare opportunity to find a target)\n",
    "                 other_class_false_positive_multiplier=1.0,    # Punish when y_true==0 but prediction is positive (Keeping neutral for now, no shame in guessing)\n",
    "                 other_class_true_negative_multiplier=0.99,     # Neutral when y_true==0 and prediction is negative (Small reward for an easy correct guess)\n",
    "                 # For smoothed labels (0 < y_true < 1)\n",
    "                 smoothing_multiplier=0.5,              # Scales the effect of the smoothed label. Pair decimals with smoothing_as_correct = True, 2> n > 1 for False (Current setting rewards getting close to the target)\n",
    "                 smoothing_as_correct=True,             # If True, treat a high prediction as rewarded; if False, as punished\n",
    "                 threshold=0.5,                         # Threshold to decide if a prediction is “positive”\n",
    "                 name=\"custom_binary_crossentropy_loss\",\n",
    "                 reduction=\"sum_over_batch_size\"):\n",
    "        super().__init__(name=name)\n",
    "        self.dominant_class_index = dominant_class_index\n",
    "        self.dominant_correct_multiplier = dominant_correct_multiplier\n",
    "        self.dominant_incorrect_multiplier = dominant_incorrect_multiplier\n",
    "\n",
    "        self.other_class_true_positive_multiplier = other_class_true_positive_multiplier\n",
    "        self.other_class_false_negative_multiplier = other_class_false_negative_multiplier\n",
    "        self.other_class_false_positive_multiplier = other_class_false_positive_multiplier\n",
    "        self.other_class_true_negative_multiplier = other_class_true_negative_multiplier\n",
    "\n",
    "        self.smoothing_multiplier = smoothing_multiplier\n",
    "        self.smoothing_as_correct = smoothing_as_correct\n",
    "        self.threshold = threshold\n",
    "        if self.smoothing_multiplier > 2:\n",
    "            raise ValueError('Smoothing Multiplier must be less than or equal to 2')\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Prevent log(0) issues.\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Reshape to (batch_size, num_classes)\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "        # Compute standard binary crossentropy (elementwise).\n",
    "        base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "                       (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "        # Get number of classes.\n",
    "        num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Create masks for the dominant class vs. non-dominant.\n",
    "        dominant_mask = tf.one_hot(self.dominant_class_index, depth=num_classes, dtype=tf.float32)\n",
    "        non_dominant_mask = tf.cast(tf.constant(1.0, tf.float32) - dominant_mask, tf.float32)\n",
    "        \n",
    "        # --- Dominant Class Weighting ---\n",
    "        # For the dominant class, use one multiplier if the true label is 1,\n",
    "        # and another if it is 0.\n",
    "        dominant_true = y_true[:, self.dominant_class_index]  # shape: (batch_size,)\n",
    "        dominant_weight = tf.where(\n",
    "            tf.equal(dominant_true, tf.constant(1.0, dtype=y_true.dtype)),\n",
    "            tf.constant(self.dominant_correct_multiplier, dtype=y_true.dtype),\n",
    "            tf.constant(self.dominant_incorrect_multiplier, dtype=y_true.dtype)\n",
    "        )\n",
    "        dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # shape: (batch_size, 1)\n",
    "        \n",
    "        # --- Non-Dominant Class Weighting ---\n",
    "        # For non-dominant classes, we now treat hard labels (exactly 0 or 1) separately\n",
    "        # from smoothed labels (0 < y_true < 1).\n",
    "        \n",
    "        # Determine conditions for hard labels.\n",
    "        is_hard_positive = tf.equal(y_true, tf.constant(1.0, dtype=y_true.dtype))\n",
    "        is_hard_negative = tf.equal(y_true, tf.constant(0.0, dtype=y_true.dtype))\n",
    "        is_hard = tf.logical_or(is_hard_positive, is_hard_negative)\n",
    "        \n",
    "        # Determine prediction condition: is the prediction \"positive\"?\n",
    "        pred_positive = tf.greater_equal(y_pred, tf.constant(self.threshold, dtype=y_true.dtype))\n",
    "        \n",
    "        # For hard labels:\n",
    "        # If y_true is 1:\n",
    "        #    - If prediction is positive: true positive multiplier.\n",
    "        #    - Else: false negative multiplier.\n",
    "        # If y_true is 0:\n",
    "        #    - If prediction is positive: false positive multiplier.\n",
    "        #    - Else: true negative multiplier.\n",
    "        hard_weight = tf.where(\n",
    "            tf.equal(y_true, tf.constant(1.0, dtype=y_true.dtype)),\n",
    "            tf.where(\n",
    "                pred_positive,\n",
    "                tf.constant(self.other_class_true_positive_multiplier, dtype=y_true.dtype),\n",
    "                tf.constant(self.other_class_false_negative_multiplier, dtype=y_true.dtype)\n",
    "            ),\n",
    "            tf.where(\n",
    "                tf.equal(y_true, tf.constant(0.0, dtype=y_true.dtype)),\n",
    "                tf.where(\n",
    "                    pred_positive,\n",
    "                    tf.constant(self.other_class_false_positive_multiplier, dtype=y_true.dtype),\n",
    "                    tf.constant(self.other_class_true_negative_multiplier, dtype=y_true.dtype)\n",
    "                ),\n",
    "                tf.constant(1.0, dtype=y_true.dtype)  # fallback, should not occur for a hard label.\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # For smoothed labels: (values strictly between 0 and 1)\n",
    "        is_smoothed = tf.logical_and(\n",
    "            tf.greater(y_true, tf.constant(0.0, dtype=y_true.dtype)),\n",
    "            tf.less(y_true, tf.constant(1.0, dtype=y_true.dtype))\n",
    "        )\n",
    "        # Here, we modulate the weight using the label’s value and a multiplier.\n",
    "        # We let the toggle smoothing_as_correct decide which way to go.\n",
    "        if self.smoothing_as_correct:\n",
    "            smoothed_weight = tf.where(\n",
    "                pred_positive,\n",
    "                1.0 - y_true * self.smoothing_multiplier,  # reward by lowering the loss\n",
    "                1.0 + y_true * self.smoothing_multiplier   # punish by increasing the loss\n",
    "            )\n",
    "        else:\n",
    "            smoothed_weight = tf.where(\n",
    "                pred_positive,\n",
    "                1.0 + y_true * self.smoothing_multiplier,  # punish\n",
    "                1.0 - y_true * self.smoothing_multiplier   # reward\n",
    "            )\n",
    "        # Combine the hard and smoothed weights for non-dominant classes.\n",
    "        non_dominant_weight = tf.where(\n",
    "            is_hard,\n",
    "            hard_weight,\n",
    "            tf.where(\n",
    "                is_smoothed,\n",
    "                smoothed_weight,\n",
    "                tf.constant(1.0, dtype=y_true.dtype)  # fallback\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Reshape the masks so they broadcast properly.\n",
    "        dominant_mask = tf.reshape(dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "        non_dominant_mask = tf.reshape(non_dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "        \n",
    "        # Combine weights: dominant classes get their weight; non-dominant get their own.\n",
    "        # dominant_weight has shape (batch_size, 1) and will broadcast.\n",
    "        weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "        # Compute weighted loss.\n",
    "        weighted_loss = base_loss * weights\n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'dominant_class_index': self.dominant_class_index,\n",
    "            'dominant_correct_multiplier': self.dominant_correct_multiplier,\n",
    "            'dominant_incorrect_multiplier': self.dominant_incorrect_multiplier,\n",
    "            'other_class_true_positive_multiplier': self.other_class_true_positive_multiplier,\n",
    "            'other_class_false_negative_multiplier': self.other_class_false_negative_multiplier,\n",
    "            'other_class_false_positive_multiplier': self.other_class_false_positive_multiplier,\n",
    "            'other_class_true_negative_multiplier': self.other_class_true_negative_multiplier,\n",
    "            'smoothing_multiplier': self.smoothing_multiplier,\n",
    "            'smoothing_as_correct': self.smoothing_as_correct,\n",
    "            'threshold': self.threshold\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc90cc6-f645-4a10-bda8-f862fa69ab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740043030.260274     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740043030.441327     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740043030.441455     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740043030.446037     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740043030.446137     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740043030.446176     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740043030.673900     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740043030.673982     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-20 02:17:10.673992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1740043030.674054     774 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-20 02:17:10.676077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,824</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">92,320</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">230,560</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">276,672</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">331,968</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,824</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,912</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">266</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">34,176</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ batch_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m10\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m5,824\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m704\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │     \u001b[38;5;34m92,320\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │        \u001b[38;5;34m640\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │    \u001b[38;5;34m230,560\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │        \u001b[38;5;34m640\u001b[0m │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │     \u001b[38;5;34m10,400\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m276,672\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m331,968\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m5,824\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │     \u001b[38;5;34m30,912\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m266\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m34,176\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m16,512\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │        \u001b[38;5;34m645\u001b[0m │ batch_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,078,053</span> (4.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,078,053\u001b[0m (4.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,075,749</span> (4.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,075,749\u001b[0m (4.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> (9.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,304\u001b[0m (9.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@utils.register_keras_serializable()\n",
    "def tile_to_batch(z):\n",
    "    pe, x = z\n",
    "    return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "def create_dcnn_model(\n",
    "    input_dim=5,\n",
    "    sequence_length=5000,\n",
    "    num_classes=5\n",
    "):\n",
    "    inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "    # Condensed positional encoding block.  See cnn for description\n",
    "    positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    # def tile_to_batch(z):\n",
    "    #     pe, x = z\n",
    "    #     return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "    pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "\n",
    "    concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "    \n",
    "\n",
    "    cnn = layers.Conv1D(filters=64, kernel_size=9, activation='relu', padding='same')(concat_input)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Dropout(0.2)(cnn)\n",
    "    # We use six layers with increasing dilation rates to capture a wider receptive field.\n",
    "    # Dilating convolutional blocks with dropout (pooling is bad because exact sequence matters)\n",
    "    skip = concat_input\n",
    "    skip = layers.Conv1D(filters=64, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=1, activation='relu', padding='same')(concat_input)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn])\n",
    "\n",
    "    # Instead of flattening, use Conv1D with kernel_size=1 as dense layers:\n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(0.2)(dense)\n",
    "    \n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "\n",
    "    # Final classification layer applied at every time step:\n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "loss_fn = CustomBinaryCrossentropyLoss(\n",
    "        dominant_class_index=0,\n",
    "        dominant_correct_multiplier=0.95,\n",
    "        dominant_incorrect_multiplier=2.0,\n",
    "        other_class_true_positive_multiplier=0.125,\n",
    "        other_class_false_negative_multiplier=5.0,\n",
    "        other_class_false_positive_multiplier=2.0,\n",
    "        other_class_true_negative_multiplier=1.0,\n",
    "        smoothing_multiplier=0.3,\n",
    "        smoothing_as_correct=True\n",
    "    )\n",
    "dcnn_model = create_dcnn_model(5, 5000, 5)\n",
    "dcnn_model.compile(\n",
    "                optimizer=optimizers.Adam(learning_rate=0.000534),\n",
    "                loss=loss_fn,\n",
    "                metrics=[\n",
    "                    CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5), \n",
    "                    CustomBackgroundOnlyF1Score(num_classes=5, average='weighted', threshold=0.5)\n",
    "                    ]\n",
    "                  )\n",
    "dcnn_model.summary()\n",
    "\n",
    "# 'explore_filters_1': 64, 'explore_filters_2': 160, 'explore_filters_3': 192, 'explore_kernel_size': 9, 'explore_dropout': 0.4, 'learning_rate': 0.0005343042689938801, 'dominant_correct_multiplier': 0.95, 'dominant_incorrect_multiplier': 2.0, \n",
    "#  'other_class_true_positive_multiplier': 0.125, 'other_class_false_negative_multiplier': 5.0, 'other_class_false_positive_multiplier': 2.0, 'other_class_true_negative_multiplier': 1.0, 'smoothing_multiplier': 0.30000000000000004, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d453b3ee-0c55-4f2e-8b8a-04b46517e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_exact_records(dataset: tf.data.Dataset, total_records, num_to_drop, seed=None):\n",
    "    '''\n",
    "    Function to drop n records from data before constructing parsed dataset.  \n",
    "    Mostly for bug checking.\n",
    "    '''\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    drop_indices = set(np.random.choice(total_records, num_to_drop, replace=False))\n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.filter(lambda i, x: ~tf.reduce_any(tf.equal(i, list(drop_indices))))\n",
    "    dataset = dataset.map(lambda i, x: x)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def parse_chunk_example(serialized_example):\n",
    "    \"\"\"\n",
    "    Parses a single serialized tf.train.Example back into tensors.\n",
    "    Used in testing datasets and in piping tfrecords to DL Algorithms\n",
    "    \"\"\"\n",
    "    feature_spec = {\n",
    "        'X':          tf.io.VarLenFeature(tf.float32),\n",
    "        'y':          tf.io.VarLenFeature(tf.float32),\n",
    "        'record_id':  tf.io.FixedLenFeature([], tf.string),\n",
    "        'cstart':     tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'cend':       tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'strand':     tf.io.FixedLenFeature([], tf.string),\n",
    "        'chunk_size': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    }\n",
    "    \n",
    "    parsed = tf.io.parse_single_example(serialized_example, feature_spec)\n",
    "    \n",
    "    # chunk_size is shape [1]\n",
    "    chunk_size = parsed['chunk_size'][0]\n",
    "    \n",
    "    # Convert sparse to dense\n",
    "    X_flat = tf.sparse.to_dense(parsed['X'])\n",
    "    y_flat = tf.sparse.to_dense(parsed['y'])\n",
    "\n",
    "    # Reshape X to [chunk_size, 5]\n",
    "    X_reshaped = tf.reshape(X_flat, [chunk_size, 5])\n",
    "    # Reshape y to [chunk_size], probably redundant\n",
    "    y_reshaped = tf.reshape(y_flat, [chunk_size, 5])\n",
    "    \n",
    "    record_id = parsed['record_id']\n",
    "    cstart = parsed['cstart'][0]\n",
    "    cend = parsed['cend'][0]\n",
    "    strand = parsed['strand']\n",
    "    \n",
    "    return X_reshaped, y_reshaped, record_id, cstart, cend, strand\n",
    "\n",
    "\n",
    "def prepare_for_model(X, y, record_id, cstart, cend, strand):\n",
    "    '''\n",
    "    Helper function that extracts and reshapes parsed data for feeding to DL Models\n",
    "    '''\n",
    "    # Expand last dimension of y from (batch_size, 5000) to (batch_size, 5000, 1)\n",
    "    # y = tf.expand_dims(y, axis=-1) turns out this line is not needed\n",
    "    # Return only (X, y). Discard the extra columns for training knowing that \n",
    "    # they still exist in the TestValTrain originals if we need them\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prep_dataset_from_tfrecord(\n",
    "    tfrecord_path,\n",
    "    batch_size=28,\n",
    "    compression_type='GZIP',\n",
    "    shuffled = False,\n",
    "    shuffle_buffer=25000,\n",
    "    total_records=None,\n",
    "    num_to_drop=None,\n",
    "    seed=None\n",
    "):\n",
    "    '''\n",
    "    Imports tfrecord and shuffles it then parses it for use in fitting a model\n",
    "    '''\n",
    "    # Loads in records in a round robin fashion for slightly increased mixing\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path, compression_type=compression_type, num_parallel_reads = tf.data.AUTOTUNE)\n",
    "    \n",
    "    if num_to_drop:\n",
    "        dataset = drop_exact_records(dataset, total_records=total_records, num_to_drop=num_to_drop, seed=seed)\n",
    "    \n",
    "    if shuffled == True:\n",
    "        # Shuffle at the record level\n",
    "        dataset = dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
    "        \n",
    "    \n",
    "    dataset = dataset.map(parse_chunk_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(prepare_for_model, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # dataset = dataset.map(lambda x, y: (x, tf.cast(y, tf.int32))) # found out tensorflow wants int32 in y # Note: Not anymore due to change in label format\n",
    "\n",
    "    # Rebatch parsed and prefetch for efficient reading\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1864028e-06c4-4910-b05b-b8ef07e0db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "# def upload_file(file_name, bucket, object_name=None):\n",
    "#     \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "#     :param file_name: File to upload\n",
    "#     :param bucket: Bucket to upload to\n",
    "#     :param object_name: S3 object name. If not specified then file_name is used\n",
    "#     :return: True if file was uploaded, else False\n",
    "#     \"\"\"\n",
    "\n",
    "#     # If S3 object_name was not specified, use file_name\n",
    "#     if object_name is None:\n",
    "#         object_name = os.path.basename(file_name)\n",
    "\n",
    "#     # Upload the file\n",
    "#     s3_client = boto3.client('s3')\n",
    "#     try:\n",
    "#         response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "#     except ClientError as e:\n",
    "#         logging.error(e)\n",
    "#         return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df447960-5110-4487-ba51-329e2dd6271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimit(callbacks.Callback):\n",
    "    def __init__(self, max_time_seconds):\n",
    "        super().__init__()\n",
    "        self.max_time_seconds = max_time_seconds\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    # def on_batch_end(self, batch, logs=None):\n",
    "    #     if time.time() - self.start_time > self.max_time_seconds:\n",
    "    #         self.model.stop_training = True\n",
    "    \n",
    "    # def on_train_batch_end(self, batch, logs=None):  # ✅ Runs more frequently than `on_batch_end`\n",
    "    #     elapsed_time = time.time() - self.start_time\n",
    "    #     if elapsed_time > self.max_time_seconds:\n",
    "    #         print(f\"\\n⏳ Time limit of {self.max_time_seconds} sec reached. Stopping training!\")\n",
    "    #         self.model.stop_training = True  # 🔥 Stops training mid-batch\n",
    "    \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if elapsed_time > self.max_time_seconds:\n",
    "            print(f\"\\n⏳ Time limit of {self.max_time_seconds} sec reached. Stopping training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):  # New method added\n",
    "        if time.time() - self.start_time > self.max_time_seconds:\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class DebugCallback(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\n🚀 Starting Epoch {epoch+1}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if batch % 1000 == 0:\n",
    "            print(f\"🔄 Processing Batch {batch}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % 1000 == 0:\n",
    "            print(f\"✅ Finished Batch {batch}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\n🏁 Epoch {epoch+1} Completed!\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "class CleanupCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Example: force garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # If you need more extensive cleanup, you can add it here.\n",
    "        # e.g., close files, flush logs, free external resources, etc.\n",
    "        print(f\"Cleanup done at the end of epoch {epoch+1}\")\n",
    "        \n",
    "\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoints/epoch-{epoch:03d}-val_no_background_f1-{val_no_background_f1:.4f}.keras',\n",
    "    # monitor='val_loss',          # what metric to name file on\n",
    "    monitor='val_no_background_f1',\n",
    "    mode='max',                    # Required for monitoring f1, comment out if monitoring val loss\n",
    "    save_best_only=False,        # save model always \n",
    "    save_weights_only=False,     # save full model (architecture + weights)\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "# class UploadModelCheckpoint(callbacks.ModelCheckpoint):\n",
    "#     \"\"\"\n",
    "#     A custom ModelCheckpoint callback that, when a new best model is saved,\n",
    "#     calls the upload_file function to upload the saved model.\n",
    "    \n",
    "#     Inherits from tf.keras.callbacks.ModelCheckpoint.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, bucket, object_name=None, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             bucket (str): The bucket where the file should be uploaded.\n",
    "#             object_name (str, optional): The object name to use when uploading.\n",
    "#             **kwargs: All other keyword arguments for ModelCheckpoint.\n",
    "#         \"\"\"\n",
    "#         super(UploadModelCheckpoint, self).__init__(**kwargs)\n",
    "#         self.bucket = bucket\n",
    "#         self.object_name = object_name\n",
    "\n",
    "#     def _save_model(self, epoch, logs):\n",
    "#         \"\"\"\n",
    "#         Overrides the internal _save_model method. First saves the model as usual,\n",
    "#         then calls upload_file with the saved file's path.\n",
    "#         \"\"\"\n",
    "#         # Get the file path using the parent's filepath formatting\n",
    "#         filepath = self._get_file_path(epoch, logs)\n",
    "#         # Save the model as usual\n",
    "#         super(UploadModelCheckpoint, self)._save_model(epoch, logs)\n",
    "#         # Now call the upload function with the saved file\n",
    "#         upload_file(filepath, self.bucket, self.object_name)\n",
    "\n",
    "\n",
    "# checkpoint_cb = UploadModelCheckpoint(\n",
    "#     bucket=s3_bucket,  # The bucket to which files are uploaded\n",
    "#     object_name='checkpoints/recent_best_model.keras',         # Optional: if not provided, the local file name will be used as the object name\n",
    "#     filepath='checkpoints/recent_best_model.keras',\n",
    "#     monitor='val_loss',       # The metric to monitor\n",
    "#     save_best_only=True,      # Only save when a new best is reached\n",
    "#     save_weights_only=False,  # Save the full model (architecture + weights)\n",
    "#     save_freq='epoch'\n",
    "# )\n",
    "\n",
    "\n",
    "early_stopping_cb = callbacks.EarlyStopping(\n",
    "    # monitor='val_loss',\n",
    "    monitor='val_no_background_f1',\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f256c553-96e4-48da-b6f5-6e7449a0782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_record = \"test.tfrecord.gz\"\n",
    "# train_record = \"train.tfrecord.gz\"\n",
    "# val_record = \"val.tfrecord.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f950157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling train dataset\n",
      "Compiling val dataset\n",
      "Resuming from checkpoint: checkpoints/epoch-001-val_loss-0.0529.keras\n",
      "Epoch 2/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740043076.364973     960 service.cc:146] XLA service 0x7f91b8029520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1740043076.365029     960 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-02-20 02:17:56.605826: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-20 02:17:57.373393: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2025-02-20 02:18:01.113508: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 102.81GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-02-20 02:18:02.342070: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 102.81GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-02-20 02:18:04.714928: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 169.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "I0000 00:00:1740043091.888424     960 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - background_only_f1: 0.9994 - loss: 0.0527 - no_background_f1: 0.2533"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 02:37:12.837404: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-02-20 02:37:12.837484: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-02-20 02:37:12.837504: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1749259023563381204\n",
      "2025-02-20 02:37:12.837538: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 15505549961683037388\n",
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup done at the end of epoch 2\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1168s\u001b[0m 159ms/step - background_only_f1: 0.9994 - loss: 0.0527 - no_background_f1: 0.2533 - val_background_only_f1: 0.9995 - val_loss: 0.0486 - val_no_background_f1: 0.4102\n",
      "Epoch 3/400\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - background_only_f1: 0.9994 - loss: 0.0514 - no_background_f1: 0.2918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 02:55:49.008813: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup done at the end of epoch 3\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1116s\u001b[0m 155ms/step - background_only_f1: 0.9994 - loss: 0.0514 - no_background_f1: 0.2918 - val_background_only_f1: 0.9995 - val_loss: 0.0498 - val_no_background_f1: 0.2326\n",
      "Epoch 4/400\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - background_only_f1: 0.9994 - loss: 0.0508 - no_background_f1: 0.3108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 03:14:29.557966: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1749259023563381204\n",
      "2025-02-20 03:14:29.558055: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 15505549961683037388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup done at the end of epoch 4\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1121s\u001b[0m 156ms/step - background_only_f1: 0.9994 - loss: 0.0508 - no_background_f1: 0.3108 - val_background_only_f1: 0.9995 - val_loss: 0.0496 - val_no_background_f1: 0.3440\n",
      "Epoch 5/400\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - background_only_f1: 0.9994 - loss: 0.0504 - no_background_f1: 0.3245"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 03:33:07.987310: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-02-20 03:33:07.987370: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1749259023563381204\n",
      "2025-02-20 03:33:07.987402: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 15505549961683037388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup done at the end of epoch 5\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1118s\u001b[0m 156ms/step - background_only_f1: 0.9994 - loss: 0.0504 - no_background_f1: 0.3245 - val_background_only_f1: 0.9995 - val_loss: 0.0481 - val_no_background_f1: 0.4408\n",
      "Epoch 6/400\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - background_only_f1: 0.9994 - loss: 0.0502 - no_background_f1: 0.3306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 03:51:44.498826: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1749259023563381204\n",
      "2025-02-20 03:51:44.498911: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 15505549961683037388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup done at the end of epoch 6\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1117s\u001b[0m 156ms/step - background_only_f1: 0.9994 - loss: 0.0502 - no_background_f1: 0.3306 - val_background_only_f1: 0.9995 - val_loss: 0.0494 - val_no_background_f1: 0.2448\n",
      "Epoch 7/400\n",
      "\u001b[1m7178/7178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - background_only_f1: 0.9994 - loss: 0.0501 - no_background_f1: 0.3371"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 04:10:08.562926: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:7: Filling up shuffle buffer (this may take a while): 9571 of 10000\n",
      "2025-02-20 04:10:28.625846: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:7: Filling up shuffle buffer (this may take a while): 9607 of 10000\n",
      "2025-02-20 04:10:39.017116: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:7: Filling up shuffle buffer (this may take a while): 9629 of 10000\n",
      "2025-02-20 04:10:48.511135: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:7: Filling up shuffle buffer (this may take a while): 9648 of 10000\n",
      "2025-02-20 04:11:08.570186: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:7: Filling up shuffle buffer (this may take a while): 9685 of 10000\n",
      "2025-02-20 04:11:18.939840: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:7: Filling up shuffle buffer (this may take a while): 9704 of 10000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(False)\n",
    "# Train: 200985, Val: 23645, Test: 11824    \n",
    "max_time_seconds = 3600*12  # 1 hour is 3600 seconds\n",
    "batch_size = 28\n",
    "epochs = 400  # Set high enough to allow stopping by callback\n",
    "steps_per_epoch = 7178\n",
    "\n",
    "print('Compiling train dataset')\n",
    "train_dataset = prep_dataset_from_tfrecord(\"TestValTrain/train.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=True,\n",
    "                                shuffle_buffer=10000,\n",
    "                                total_records=200985,\n",
    "                                num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "                                )\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "print('Compiling val dataset')\n",
    "val_dataset = prep_dataset_from_tfrecord(\"TestValTrain/val.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=False,\n",
    "                                shuffle_buffer=5000,\n",
    "                                total_records=23645,\n",
    "                                num_to_drop=13, # Batch size 28 leaves remainder of 13 records\n",
    "                                seed=42 # Seed for dropping the same 13 records every time\n",
    "                                )\n",
    "\n",
    "# test_dataset = prep_dataset_from_tfrecord(\"TestValTrain/test.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=False,\n",
    "#                                 shuffle_buffer=5000,\n",
    "#                                 total_records=11824,\n",
    "#                                 num_to_drop=8, # Batch size 28 leaves remainder of 13 records\n",
    "#                                 seed=42 # Seed for dropping the same 8 records every time\n",
    "#                                 )\n",
    "\n",
    "# history = dcnn_model.fit(\n",
    "#         train_dataset, \n",
    "#         validation_data=val_dataset,\n",
    "#         # batch_size=batch_size,\n",
    "#         epochs=epochs,\n",
    "#         steps_per_epoch=steps_per_epoch,\n",
    "#         callbacks=[early_stopping_cb, checkpoint_cb, CleanupCallback(), TimeLimit(max_time_seconds=max_time_seconds)]\n",
    "#         )\n",
    "\n",
    "# print('Saving model...')\n",
    "# dcnn_model.save(\"Finished_Model.keras\")\n",
    "# # upload_file(\"Finished_Model.keras\", s3_bucket, \"checkpoints/Finished_Model.keras)\n",
    "# print(f\"📁 Model saved!\")\n",
    "\n",
    "\n",
    "loss_fn = CustomBinaryCrossentropyLoss(\n",
    "        dominant_class_index=0,\n",
    "        dominant_correct_multiplier=0.98,\n",
    "        dominant_incorrect_multiplier=2.5,\n",
    "        other_class_true_positive_multiplier=0.075,\n",
    "        other_class_false_negative_multiplier=5.5,\n",
    "        other_class_false_positive_multiplier=2.0,\n",
    "        other_class_true_negative_multiplier=0.98,\n",
    "        smoothing_multiplier=0,\n",
    "        smoothing_as_correct=False\n",
    "    )\n",
    "\n",
    "\n",
    "# Define your checkpoint directory and file pattern.\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "# Assume your checkpoint files follow the pattern 'cp-XXXX.ckpt'\n",
    "checkpoint_pattern = os.path.join(checkpoint_dir, \"epoch-*-val_loss-*.keras\")\n",
    "\n",
    "# Find the most recent checkpoint file.\n",
    "checkpoint_files = glob.glob(checkpoint_pattern)\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "    print(\"Resuming from checkpoint:\", latest_checkpoint)\n",
    "    \n",
    "    # Load the entire model from the checkpoint.\n",
    "    dcnn_model = models.load_model(latest_checkpoint)\n",
    "    dcnn_model.compile(\n",
    "                    optimizer=optimizers.Adam(learning_rate=0.00059), # uncomment if transfer learning, keep commented if resuming\n",
    "                    loss=loss_fn,\n",
    "                    metrics=[\n",
    "                        CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5), \n",
    "                        CustomBackgroundOnlyF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "                        CustomFalsePositiveDistance(num_classes=5, threshold=0.5, window=100)\n",
    "                        ]\n",
    "                    )\n",
    "    \n",
    "    epoch_str = os.path.basename(latest_checkpoint).split('-')[1]\n",
    "    # Remove any file extension; adjust the splitting as needed.\n",
    "    epoch_num = int(''.join(filter(str.isdigit, epoch_str)))\n",
    "    initial_epoch = epoch_num\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "    # Build and compile your model as you normally do.\n",
    "    dcnn_model = create_dcnn_model(5, 5000, 5)\n",
    "    dcnn_model.compile(\n",
    "                    optimizer=optimizers.Adam(learning_rate=0.000534),\n",
    "                    loss=loss_fn,\n",
    "                    metrics=[\n",
    "                        CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5), \n",
    "                        CustomBackgroundOnlyF1Score(num_classes=5, average='weighted', threshold=0.5)\n",
    "                        ]\n",
    "                    )\n",
    "    dcnn_model.summary()\n",
    "    initial_epoch = 0\n",
    "\n",
    "# Continue training.\n",
    "history = dcnn_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[early_stopping_cb, checkpoint_cb, CleanupCallback()],\n",
    "    initial_epoch=initial_epoch  # This tells Keras to start counting epochs from here.\n",
    ")\n",
    "\n",
    "# Save final model if needed.\n",
    "dcnn_model.save(\"Finished_Model.keras\")\n",
    "print(\"📁 Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f399e",
   "metadata": {},
   "source": [
    "Transfer Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e705ce-c5a8-45f6-be96-adeed04e3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(False)\n",
    "# # Train: 200985, Val: 23645, Test: 11824    \n",
    "# max_time_seconds = 3600*12  # 1 hour is 3600 seconds\n",
    "# batch_size = 28\n",
    "# epochs = 400  # Set high enough to allow stopping by callback\n",
    "# steps_per_epoch = 7178\n",
    "\n",
    "# print('Compiling train dataset')\n",
    "# train_dataset = prep_dataset_from_tfrecord(\"TestValTrain/train.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=True,\n",
    "#                                 shuffle_buffer=10000,\n",
    "#                                 total_records=200985,\n",
    "#                                 num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "#                                 )\n",
    "# train_dataset = train_dataset.repeat()\n",
    "\n",
    "# print('Compiling val dataset')\n",
    "# val_dataset = prep_dataset_from_tfrecord(\"TestValTrain/val.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=False,\n",
    "#                                 shuffle_buffer=5000,\n",
    "#                                 total_records=23645,\n",
    "#                                 num_to_drop=13, # Batch size 28 leaves remainder of 13 records\n",
    "#                                 seed=42 # Seed for dropping the same 13 records every time\n",
    "#                                 )\n",
    "\n",
    "# # test_dataset = prep_dataset_from_tfrecord(\"TestValTrain/test.tfrecord.gz\",\n",
    "# #                                 batch_size=batch_size, \n",
    "# #                                 compression_type='GZIP', \n",
    "# #                                 shuffled=False,\n",
    "# #                                 shuffle_buffer=5000,\n",
    "# #                                 total_records=11824,\n",
    "# #                                 num_to_drop=8, # Batch size 28 leaves remainder of 13 records\n",
    "# #                                 seed=42 # Seed for dropping the same 8 records every time\n",
    "# #                                 )\n",
    "\n",
    "# # history = dcnn_model.fit(\n",
    "# #         train_dataset, \n",
    "# #         validation_data=val_dataset,\n",
    "# #         # batch_size=batch_size,\n",
    "# #         epochs=epochs,\n",
    "# #         steps_per_epoch=steps_per_epoch,\n",
    "# #         callbacks=[early_stopping_cb, checkpoint_cb, CleanupCallback(), TimeLimit(max_time_seconds=max_time_seconds)]\n",
    "# #         )\n",
    "\n",
    "# # print('Saving model...')\n",
    "# # dcnn_model.save(\"Finished_Model.keras\")\n",
    "# # # upload_file(\"Finished_Model.keras\", s3_bucket, \"checkpoints/Finished_Model.keras)\n",
    "# # print(f\"📁 Model saved!\")\n",
    "\n",
    "\n",
    "# loss_fn = CustomBinaryCrossentropyLoss(\n",
    "#     dominant_class_index=0,\n",
    "#     dominant_correct_multiplier=0.0002, # 0.00131 percent are hits in the other categories so this is free points\n",
    "#     dominant_incorrect_multiplier=4, # More brutal punishment for screwing up labels\n",
    "#     other_class_multiplier=6, # More generous reward for bulls-eye\n",
    "#     smoothing_multiplier=0 # As far as training is concerned, there is no label sampling anymore.\n",
    "# )\n",
    "\n",
    "# # Define your checkpoint directory and file pattern.\n",
    "# checkpoint_dir = \"checkpoints\"\n",
    "# # Assume your checkpoint files follow the pattern 'cp-XXXX.ckpt'\n",
    "# checkpoint_pattern = os.path.join(checkpoint_dir, \"epoch-*-val_loss-*.keras\")\n",
    "\n",
    "# # Find the most recent checkpoint file.\n",
    "# checkpoint_files = glob.glob(checkpoint_pattern)\n",
    "# if checkpoint_files:\n",
    "#     latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "#     print(\"Resuming from checkpoint:\", latest_checkpoint)\n",
    "    \n",
    "#     # Load the entire model from the checkpoint.\n",
    "#     dcnn_model = models.load_model(latest_checkpoint)\n",
    "#     dcnn_model.compile(\n",
    "#                         optimizer=optimizers.Adam(learning_rate=0.000686),  # DELETE THIS BEFORE NEXT USE\n",
    "#                         loss=loss_fn,\n",
    "#                         metrics=[\n",
    "#                             CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "#                             ]\n",
    "#                        )\n",
    "    \n",
    "#     epoch_str = os.path.basename(latest_checkpoint).split('-')[1]\n",
    "#     # Remove any file extension; adjust the splitting as needed.\n",
    "#     epoch_num = int(''.join(filter(str.isdigit, epoch_str)))\n",
    "#     initial_epoch = epoch_num\n",
    "# else:\n",
    "#     print(\"No checkpoint found. Starting training from scratch.\")\n",
    "#     # Build and compile your model as you normally do.\n",
    "#     dcnn_model = create_dcnn_model(5, 5000, 5)\n",
    "#     dcnn_model.compile(\n",
    "#                     optimizer=optimizers.Adam(learning_rate=0.000686),\n",
    "#                     loss=loss_fn,\n",
    "#                     metrics=[\n",
    "#                         CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "#                         ]\n",
    "#                     )\n",
    "#     dcnn_model.summary()\n",
    "#     initial_epoch = 0\n",
    "\n",
    "# # Continue training.\n",
    "# history = dcnn_model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=val_dataset,\n",
    "#     epochs=epochs,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "#     callbacks=[early_stopping_cb, checkpoint_cb, CleanupCallback(), TimeLimit(max_time_seconds=max_time_seconds)],\n",
    "#     initial_epoch=initial_epoch  # This tells Keras to start counting epochs from here.\n",
    "# )\n",
    "\n",
    "# # Save final model if needed.\n",
    "# dcnn_model.save(\"Finished_Model.keras\")\n",
    "# print(\"📁 Model saved!\")"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3739fdfc",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d27ce",
   "metadata": {},
   "source": [
    "Tuning Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be857c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tune_dcnn_model(hp):\n",
    "    \n",
    "#     input_dim=5\n",
    "#     sequence_length=5000\n",
    "#     num_classes=5\n",
    "#     inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "#     # Condensed positional encoding block.  See cnn for description\n",
    "#     positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "#     pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "#     pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "#     def tile_to_batch(z):\n",
    "#         pe, x = z\n",
    "#         return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "#     pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "\n",
    "#     concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "    \n",
    "#     cnn = layers.Conv1D(filters=128, kernel_size=9, activation='relu', padding='same')(concat_input)\n",
    "#     cnn = layers.BatchNormalization()(cnn)\n",
    "#     cnn = layers.Dropout(0.2)(cnn)\n",
    "#     # We use six layers with increasing dilation rates to capture a wider receptive field.\n",
    "#     # Dilating convolutional blocks with dropout (pooling is bad because exact sequence matters)\n",
    "#     skip = concat_input\n",
    "#     skip = layers.Conv1D(filters=128, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=128, kernel_size=9, dilation_rate=1, activation='relu', padding='same')(concat_input)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=128, kernel_size=9, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.2)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     skip = dcnn\n",
    "#     skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.2)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     skip = dcnn\n",
    "#     skip = layers.Conv1D(filters=288, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=288, kernel_size=9, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.2)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=288, kernel_size=9, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.2)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn])\n",
    "\n",
    "#     # Instead of flattening, use Conv1D with kernel_size=1 as dense layers:\n",
    "#     dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "#     dense = layers.BatchNormalization()(dense)\n",
    "#     dense = layers.Dropout(0.2)(dense)\n",
    "    \n",
    "#     dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "#     dense = layers.BatchNormalization()(dense)\n",
    "\n",
    "#     # Final classification layer applied at every time step:\n",
    "#     outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "#     model.load_weights(\"checkpoints/epoch-003-val_loss-0.0247.keras\")\n",
    "    \n",
    "#     learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    \n",
    "#     dominant_correct_multiplier = hp.Float('dominant_correct_multiplier', 0.005, 0.15, step=0.02, default=0.65)\n",
    "#     dominant_incorrect_multiplier = hp.Float('dominant_incorrect_multiplier', 0, 10.0, step=2.0, default=2.0)\n",
    "#     other_class_multiplier = hp.Float('other_class_multiplier', 1.0, 11.0, step=2.0, default=2.0)\n",
    "#     smoothing_multiplier = hp.Float('smoothing_multiplier', -0.5, 1.0, step=0.5, default=0.5)\n",
    "    \n",
    "#     loss_fn = CustomBinaryCrossentropyLoss(\n",
    "#         dominant_class_index=0,\n",
    "#         dominant_correct_multiplier=dominant_correct_multiplier,\n",
    "#         dominant_incorrect_multiplier=dominant_incorrect_multiplier,\n",
    "#         other_class_multiplier=other_class_multiplier,\n",
    "#         smoothing_multiplier=smoothing_multiplier\n",
    "#     )\n",
    "#     model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "#                   loss=loss_fn,\n",
    "#                   metrics=[CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ea400",
   "metadata": {},
   "source": [
    "Transfer Learning Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tune_dcnn_model(hp):\n",
    "    \n",
    "#     input_dim=5\n",
    "#     sequence_length=5000\n",
    "#     num_classes=5\n",
    "#     inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "#     # Condensed positional encoding block.  See cnn for description\n",
    "#     positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "#     pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "#     pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "#     # def tile_to_batch(z):\n",
    "#     #     pe, x = z\n",
    "#     #     return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "#     pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "\n",
    "#     concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "\n",
    "#     cnn = layers.Conv1D(filters=64, kernel_size=9, activation='relu', padding='same')(concat_input)\n",
    "#     cnn = layers.BatchNormalization()(cnn)\n",
    "#     cnn = layers.Dropout(0.4)(cnn)\n",
    "#     # We use six layers with increasing dilation rates to capture a wider receptive field.\n",
    "#     # Dilating convolutional blocks with dropout (pooling is bad because exact sequence matters)\n",
    "#     skip = concat_input\n",
    "#     skip = layers.Conv1D(filters=64, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=1, activation='relu', padding='same')(concat_input)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     skip = dcnn\n",
    "#     skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     skip = dcnn\n",
    "#     skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn])\n",
    "\n",
    "#     # Instead of flattening, use Conv1D with kernel_size=1 as dense layers:\n",
    "#     dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "#     dense = layers.BatchNormalization()(dense)\n",
    "#     dense = layers.Dropout(0.4)(dense)\n",
    "    \n",
    "#     dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "#     dense = layers.BatchNormalization()(dense)\n",
    "\n",
    "#     # Final classification layer applied at every time step:\n",
    "#     outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "#     model.load_weights(\"checkpoints/epoch-001-val_loss-0.0529.keras\")\n",
    "    \n",
    "#     learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    \n",
    "#     dominant_correct_multiplier = hp.Float('dominant_correct_multiplier', 0.95, 1, step=0.01)\n",
    "#     dominant_incorrect_multiplier = hp.Float('dominant_incorrect_multiplier', 1.0, 3.0, step=0.5)\n",
    "#     other_class_true_positive_multiplier = hp.Float('other_class_true_positive_multiplier', 0.025, 0.15, step=0.025)\n",
    "#     other_class_false_negative_multiplier = hp.Float('other_class_false_negative_multiplier', 4.0, 7.0, step=0.5)\n",
    "#     other_class_false_positive_multiplier = hp.Float('other_class_false_positive_multiplier', 1.5, 2.5, step=0.5)\n",
    "#     other_class_true_negative_multiplier = hp.Float('other_class_true_negative_multiplier', 0.95, 1, step=0.015)\n",
    "#     smoothing_multiplier = hp.Float('smoothing_multiplier', 0.8, 2, step=0.2)\n",
    "\n",
    "#     loss_fn = CustomBinaryCrossentropyLoss(\n",
    "#         dominant_class_index=0,\n",
    "#         dominant_correct_multiplier=dominant_correct_multiplier,\n",
    "#         dominant_incorrect_multiplier=dominant_incorrect_multiplier,\n",
    "#         other_class_true_positive_multiplier=other_class_true_positive_multiplier,\n",
    "#         other_class_false_negative_multiplier=other_class_false_negative_multiplier,\n",
    "#         other_class_false_positive_multiplier=other_class_false_positive_multiplier,\n",
    "#         other_class_true_negative_multiplier=other_class_true_negative_multiplier,\n",
    "#         smoothing_multiplier=smoothing_multiplier,\n",
    "#         smoothing_as_correct=False\n",
    "#     )\n",
    "#     model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "#                   loss=loss_fn,\n",
    "#                   metrics=[\n",
    "#                         CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "#                         CustomFalsePositiveDistance(num_classes=5, threshold=0.5, window=100)\n",
    "#                         ]\n",
    "#                   )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0fa63e",
   "metadata": {},
   "source": [
    "Transfer Learning Round 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tune_dcnn_model(hp):\n",
    "    \n",
    "#     input_dim=5\n",
    "#     sequence_length=5000\n",
    "#     num_classes=5\n",
    "#     inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "#     # Condensed positional encoding block.  See cnn for description\n",
    "#     positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "#     pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "#     pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "#     # def tile_to_batch(z):\n",
    "#     #     pe, x = z\n",
    "#     #     return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "#     pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "\n",
    "#     concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "\n",
    "#     cnn = layers.Conv1D(filters=64, kernel_size=9, activation='relu', padding='same')(concat_input)\n",
    "#     cnn = layers.BatchNormalization()(cnn)\n",
    "#     cnn = layers.Dropout(0.4)(cnn)\n",
    "#     # We use six layers with increasing dilation rates to capture a wider receptive field.\n",
    "#     # Dilating convolutional blocks with dropout (pooling is bad because exact sequence matters)\n",
    "#     skip = concat_input\n",
    "#     skip = layers.Conv1D(filters=64, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=1, activation='relu', padding='same')(concat_input)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     skip = dcnn\n",
    "#     skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     skip = dcnn\n",
    "#     skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "#     dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "    \n",
    "#     dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "#     dcnn = layers.BatchNormalization()(dcnn)\n",
    "#     dcnn = layers.Dropout(0.4)(dcnn)\n",
    "#     dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "#     second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn])\n",
    "\n",
    "#     # Instead of flattening, use Conv1D with kernel_size=1 as dense layers:\n",
    "#     dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "#     dense = layers.BatchNormalization()(dense)\n",
    "#     dense = layers.Dropout(0.4)(dense)\n",
    "    \n",
    "#     dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "#     dense = layers.BatchNormalization()(dense)\n",
    "\n",
    "#     # Final classification layer applied at every time step:\n",
    "#     outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "#     model.load_weights(\"checkpoints/epoch-001-val_loss-0.0529.keras\")\n",
    "    \n",
    "#     learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    \n",
    "#     dominant_correct_multiplier = hp.Float('dominant_correct_multiplier', 0.95, 1, step=0.01)\n",
    "#     dominant_incorrect_multiplier = hp.Float('dominant_incorrect_multiplier', 1.0, 3.0, step=0.5)\n",
    "#     other_class_true_positive_multiplier = hp.Float('other_class_true_positive_multiplier', 0.025, 0.15, step=0.025)\n",
    "#     other_class_false_negative_multiplier = hp.Float('other_class_false_negative_multiplier', 4.0, 7.0, step=0.5)\n",
    "#     other_class_false_positive_multiplier = hp.Float('other_class_false_positive_multiplier', 1.5, 2.5, step=0.5)\n",
    "#     other_class_true_negative_multiplier = hp.Float('other_class_true_negative_multiplier', 0.95, 1, step=0.015)\n",
    "#     # smoothing_multiplier = hp.Float('smoothing_multiplier', 0.8, 2, step=0.2)\n",
    "\n",
    "#     loss_fn = CustomBinaryCrossentropyLoss(\n",
    "#         dominant_class_index=0,\n",
    "#         dominant_correct_multiplier=dominant_correct_multiplier,\n",
    "#         dominant_incorrect_multiplier=dominant_incorrect_multiplier,\n",
    "#         other_class_true_positive_multiplier=other_class_true_positive_multiplier,\n",
    "#         other_class_false_negative_multiplier=other_class_false_negative_multiplier,\n",
    "#         other_class_false_positive_multiplier=other_class_false_positive_multiplier,\n",
    "#         other_class_true_negative_multiplier=other_class_true_negative_multiplier,\n",
    "#         smoothing_multiplier=0,\n",
    "#         smoothing_as_correct=False\n",
    "#     )\n",
    "#     model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "#                   loss=loss_fn,\n",
    "#                   metrics=[\n",
    "#                         CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "#                         CustomFalsePositiveDistance(num_classes=5, threshold=0.5, window=100)\n",
    "#                         ]\n",
    "#                   )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8853f",
   "metadata": {},
   "source": [
    "Tuning Round 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7338aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_dcnn_model(hp):\n",
    "    \n",
    "    input_dim=5\n",
    "    sequence_length=5000\n",
    "    num_classes=5\n",
    "    inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "    # Condensed positional encoding block.  See cnn for description\n",
    "    positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    # def tile_to_batch(z):\n",
    "    #     pe, x = z\n",
    "    #     return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "    pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "\n",
    "    concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "    \n",
    "    explore_dropout = hp.Float('explore_dropout', 0.1, 0.5, step=0.1)\n",
    "    explore_dropout_2 = hp.Float('explore_dropout_2', 0.1, 0.5, step=0.1)\n",
    "\n",
    "    cnn = layers.Conv1D(filters=64, kernel_size=9, activation='relu', padding='same')(concat_input)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Dropout(explore_dropout)(cnn)\n",
    "    # We use six layers with increasing dilation rates to capture a wider receptive field.\n",
    "    # Dilating convolutional blocks with dropout (pooling is bad because exact sequence matters)\n",
    "    skip = concat_input\n",
    "    skip = layers.Conv1D(filters=64, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=1, activation='relu', padding='same')(concat_input)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn])\n",
    "\n",
    "    # Instead of flattening, use Conv1D with kernel_size=1 as dense layers:\n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(explore_dropout_2)(dense)\n",
    "    \n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "\n",
    "    # Final classification layer applied at every time step:\n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.load_weights(\"checkpoints/epoch-001-val_loss-0.0529.keras\")\n",
    "    \n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-3, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    \n",
    "    # dominant_correct_multiplier = hp.Float('dominant_correct_multiplier', 0.95, 1, step=0.01)\n",
    "    # dominant_incorrect_multiplier = hp.Float('dominant_incorrect_multiplier', 1.0, 3.0, step=0.5)\n",
    "    # other_class_true_positive_multiplier = hp.Float('other_class_true_positive_multiplier', 0.025, 0.15, step=0.025)\n",
    "    # other_class_false_negative_multiplier = hp.Float('other_class_false_negative_multiplier', 4.0, 7.0, step=0.5)\n",
    "    # other_class_false_positive_multiplier = hp.Float('other_class_false_positive_multiplier', 1.5, 2.5, step=0.5)\n",
    "    # other_class_true_negative_multiplier = hp.Float('other_class_true_negative_multiplier', 0.95, 1, step=0.015)\n",
    "    # smoothing_multiplier = hp.Float('smoothing_multiplier', 0.8, 2, step=0.2)\n",
    "\n",
    "    loss_fn = CustomBinaryCrossentropyLoss(\n",
    "        dominant_class_index=0,\n",
    "        dominant_correct_multiplier=0.98,\n",
    "        dominant_incorrect_multiplier=2.5,\n",
    "        other_class_true_positive_multiplier=0.075,\n",
    "        other_class_false_negative_multiplier=5.5,\n",
    "        other_class_false_positive_multiplier=2.0,\n",
    "        other_class_true_negative_multiplier=0.98,\n",
    "        smoothing_multiplier=0,\n",
    "        smoothing_as_correct=False\n",
    "    )\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=loss_fn,\n",
    "                  metrics=[\n",
    "                        CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "                        CustomFalsePositiveDistance(num_classes=5, threshold=0.5, window=100)\n",
    "                        ]\n",
    "                  )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f19ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 30m 26s]\n",
      "val_no_background_f1: 0.4957515299320221\n",
      "\n",
      "Best val_no_background_f1 So Far: 0.5843488574028015\n",
      "Total elapsed time: 10h 44m 51s\n",
      "Results summary\n",
      "Results in DCNN_Tuner_8/dcnn_hyperparam_opt\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_no_background_f1\", direction=\"max\")\n",
      "\n",
      "Trial 0024 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.2\n",
      "learning_rate: 0.005900736226759916\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0022\n",
      "Score: 0.5843488574028015\n",
      "\n",
      "Trial 0025 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.5\n",
      "learning_rate: 0.0010702500770981775\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0021\n",
      "Score: 0.5813502669334412\n",
      "\n",
      "Trial 0028 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.0022931208485941225\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.5792353749275208\n",
      "\n",
      "Trial 0016 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.006651256099971132\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0012\n",
      "Score: 0.5689898133277893\n",
      "\n",
      "Trial 0022 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.2\n",
      "learning_rate: 0.005900736226759916\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.5523130893707275\n",
      "\n",
      "Trial 0027 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.4\n",
      "learning_rate: 0.007761972964252136\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.5438008904457092\n",
      "\n",
      "Trial 0017 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.4\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.004122308795038672\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0013\n",
      "Score: 0.5401781797409058\n",
      "\n",
      "Trial 0010 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.006651256099971132\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.5376294851303101\n",
      "\n",
      "Trial 0004 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.4\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.004122308795038672\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.5305911898612976\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.1\n",
      "learning_rate: 0.004362602252405516\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.527668833732605\n",
      "Best Hyperparameters:\n",
      "{'explore_dropout': 0.2, 'explore_dropout_2': 0.2, 'learning_rate': 0.005900736226759916, 'tuner/epochs': 10, 'tuner/initial_epoch': 4, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0022'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_time_seconds = 3600/2  # 1 hour is 3600 seconds\n",
    "batch_size = 28\n",
    "epochs = 10  # Set high enough to allow stopping by time\n",
    "steps_per_epoch = 1500\n",
    "\n",
    "print('Compiling train dataset')\n",
    "train_dataset = prep_dataset_from_tfrecord(\"TestValTrain/train.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=True,\n",
    "                                shuffle_buffer=5000,\n",
    "                                total_records=200985,\n",
    "                                num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "                                )\n",
    "train_dataset = train_dataset.repeat()\n",
    "print('Compiling val dataset')\n",
    "val_dataset = prep_dataset_from_tfrecord(\"TestValTrain/val.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=False,\n",
    "                                shuffle_buffer=5000,\n",
    "                                total_records=23645,\n",
    "                                num_to_drop=13, # Batch size 28 leaves remainder of 13 records\n",
    "                                seed=42 # Seed for dropping the same 13 records every time\n",
    "                                )\n",
    "\n",
    "# test_dataset = prep_dataset_from_tfrecord(\"TestValTrain/test.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=False,\n",
    "#                                 shuffle_buffer=5000,\n",
    "#                                 total_records=11824,\n",
    "#                                 num_to_drop=8, # Batch size 28 leaves remainder of 13 records\n",
    "#                                 seed=42 # Seed for dropping the same 8 records every time\n",
    "#                                 )\n",
    "tuner = kt.Hyperband(\n",
    "tune_dcnn_model,\n",
    "# objective='val_loss',\n",
    "objective=kt.Objective('val_no_background_f1', direction='max'),\n",
    "max_epochs=10,\n",
    "factor=3,\n",
    "directory='DCNN_Tuner_8',\n",
    "project_name='dcnn_hyperparam_opt',\n",
    "overwrite=False\n",
    ")\n",
    "\n",
    "# stop_early = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "time_limit_callback = TimeLimit(max_time_seconds=max_time_seconds)\n",
    "\n",
    "tuner.search(train_dataset, epochs=10, steps_per_epoch = steps_per_epoch, validation_data=val_dataset, callbacks=[early_stopping_cb, time_limit_callback])\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model and hyperparameters:\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2975e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441239b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in DCNN_Tuner_8/dcnn_hyperparam_opt\n",
      "Showing 31 best trials\n",
      "Objective(name=\"val_no_background_f1\", direction=\"max\")\n",
      "\n",
      "Trial 0024 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.2\n",
      "learning_rate: 0.005900736226759916\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0022\n",
      "Score: 0.5843488574028015\n",
      "\n",
      "Trial 0025 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.5\n",
      "learning_rate: 0.0010702500770981775\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0021\n",
      "Score: 0.5813502669334412\n",
      "\n",
      "Trial 0028 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.0022931208485941225\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.5792353749275208\n",
      "\n",
      "Trial 0016 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.006651256099971132\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0012\n",
      "Score: 0.5689898133277893\n",
      "\n",
      "Trial 0022 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.2\n",
      "learning_rate: 0.005900736226759916\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.5523130893707275\n",
      "\n",
      "Trial 0027 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.4\n",
      "learning_rate: 0.007761972964252136\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.5438008904457092\n",
      "\n",
      "Trial 0017 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.4\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.004122308795038672\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0013\n",
      "Score: 0.5401781797409058\n",
      "\n",
      "Trial 0010 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.006651256099971132\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.5376294851303101\n",
      "\n",
      "Trial 0004 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.4\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.004122308795038672\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.5305911898612976\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.1\n",
      "learning_rate: 0.004362602252405516\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.527668833732605\n",
      "\n",
      "Trial 0021 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.5\n",
      "learning_rate: 0.0010702500770981775\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.5243450999259949\n",
      "\n",
      "Trial 0005 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.4\n",
      "learning_rate: 0.0023214931760823115\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.5164474844932556\n",
      "\n",
      "Trial 0012 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.006651256099971132\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0010\n",
      "Score: 0.5126562118530273\n",
      "\n",
      "Trial 0011 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.4\n",
      "learning_rate: 0.00732569337556932\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.5122022032737732\n",
      "\n",
      "Trial 0013 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.4\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.004122308795038672\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0004\n",
      "Score: 0.5121635794639587\n",
      "\n",
      "Trial 0018 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.30000000000000004\n",
      "explore_dropout_2: 0.2\n",
      "learning_rate: 0.00538944408805645\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.5077386498451233\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.0025691493164111258\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.505907416343689\n",
      "\n",
      "Trial 0029 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.5\n",
      "explore_dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.0025263102121763268\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.4957515299320221\n",
      "\n",
      "Trial 0020 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.1\n",
      "learning_rate: 0.0054831699499040655\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.49240532517433167\n",
      "\n",
      "Trial 0019 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.2\n",
      "learning_rate: 0.004005814502191219\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.4913380444049835\n",
      "\n",
      "Trial 0008 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.1\n",
      "learning_rate: 0.008691532143065594\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.4911457896232605\n",
      "\n",
      "Trial 0009 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.1\n",
      "learning_rate: 0.005713424362702735\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.46731632947921753\n",
      "\n",
      "Trial 0014 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.1\n",
      "learning_rate: 0.004362602252405516\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0000\n",
      "Score: 0.4519297480583191\n",
      "\n",
      "Trial 0026 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.4\n",
      "explore_dropout_2: 0.5\n",
      "learning_rate: 0.002131388217256942\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.44907626509666443\n",
      "\n",
      "Trial 0006 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.4\n",
      "explore_dropout_2: 0.1\n",
      "learning_rate: 0.004881220361860484\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.4396269619464874\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.5\n",
      "explore_dropout_2: 0.4\n",
      "learning_rate: 0.0041363729199069815\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.428279846906662\n",
      "\n",
      "Trial 0007 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.5\n",
      "explore_dropout_2: 0.4\n",
      "learning_rate: 0.005495257907879351\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.42466360330581665\n",
      "\n",
      "Trial 0015 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.1\n",
      "explore_dropout_2: 0.4\n",
      "learning_rate: 0.0023214931760823115\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0005\n",
      "Score: 0.41601434350013733\n",
      "\n",
      "Trial 0023 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.5\n",
      "explore_dropout_2: 0.5\n",
      "learning_rate: 0.002536150476497026\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.34922221302986145\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "explore_dropout: 0.2\n",
      "explore_dropout_2: 0.5\n",
      "learning_rate: 0.007934965984510992\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.3404049575328827\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tuner.results_summary(31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79443184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling train dataset\n",
      "Counts by column (indices 1-4): [265705 265703 392866 392715]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:22:43.383447: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 28\n",
    "# epochs = 400  # Set high enough to allow stopping by callback\n",
    "# steps_per_epoch = 7178\n",
    "\n",
    "# print('Compiling train dataset')\n",
    "# train_dataset = prep_dataset_from_tfrecord(\"TestValTrain/train.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=True,\n",
    "#                                 shuffle_buffer=1000,\n",
    "#                                 total_records=200985,\n",
    "#                                 num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "#                                 )\n",
    "\n",
    "# # for X_batch, y_batch in train_dataset.take(1):\n",
    "#     # print(\"X shape:\", X_batch.shape)\n",
    "#     # print(\"y shape:\", y_batch.shape)\n",
    "#     # print(\"record_id:\", record_id_batch)\n",
    "#     # print(\"cstart:\", cstart_batch)\n",
    "#     # print(\"cend:\", cend_batch)\n",
    "#     # print(\"strand:\", strand_batch)\n",
    "    \n",
    "# # Create an initial state for the counts (one count per column, shape: (4,))\n",
    "# ones_by_column = None\n",
    "# for batch in train_dataset:\n",
    "#     # Assume each batch is a tuple: (features, labels)\n",
    "#     _, labels = batch  \n",
    "#     # Slice to get the sub-tensor for columns 1-4 from the 1x5 portion\n",
    "#     # If labels shape is (batch_size, 1, 5, 5000):\n",
    "#     relevant_labels = labels[:, :, 1:5]  # shape: (batch_size, 4, 5000)\n",
    "    \n",
    "#     # Create a boolean mask where entries equal to 1.0 are True\n",
    "#     ones_mask = tf.equal(relevant_labels, 1.0)\n",
    "    \n",
    "#     # Convert booleans to int and sum over the batch dimension (axis 0) and the 5000 dimension (axis 2)\n",
    "#     batch_column_counts = tf.reduce_sum(tf.cast(ones_mask, tf.int64), axis=[0, 1])  # shape: (4,)\n",
    "    \n",
    "#     if ones_by_column is None:\n",
    "#         ones_by_column = batch_column_counts\n",
    "#     else:\n",
    "#         ones_by_column += batch_column_counts\n",
    "\n",
    "# print(\"Counts by column (indices 1-4):\", ones_by_column.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf817c4",
   "metadata": {},
   "source": [
    "Number of labels in each column: [265705, 265703, 392866, 392715], sum = 1316989, sparseness = 1316989/(200984*5000) = 0.00131, Intron boundary labels: 40.3%, Exon Boundary Labels: 59.7%\n",
    "\n",
    "Prior to transfer learning, the model had roughly a 50:50 shot at landing a bullseye for something that occurs 1.3 times out of 1000 in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35835b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling val dataset\n",
      "Evaluating epoch-001-val_loss-0.0529.keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739865857.336202    8142 service.cc:146] XLA service 0x7f4e4c013b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739865857.336262    8142 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-02-18 01:04:17.385474: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-18 01:04:17.511622: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2025-02-18 01:04:20.274926: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng30{k2=1,k4=2,k5=1,k6=0,k7=0} for conv (f32[448,192,1,313]{3,2,1,0}, u8[0]{0}) custom-call(f32[448,160,1,321]{3,2,1,0}, f32[192,160,1,9]{3,2,1,0}), window={size=1x9}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-02-18 01:04:20.277508: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.130130697s\n",
      "Trying algorithm eng30{k2=1,k4=2,k5=1,k6=0,k7=0} for conv (f32[448,192,1,313]{3,2,1,0}, u8[0]{0}) custom-call(f32[448,160,1,321]{3,2,1,0}, f32[192,160,1,9]{3,2,1,0}), window={size=1x9}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-02-18 01:04:21.502193: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 102.81GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3/Unknown \u001b[1m6s\u001b[0m 29ms/step - background_only_f1: 0.9996 - loss: 0.0442 - no_background_f1: 0.3160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739865862.762288    8142 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 27ms/step - background_only_f1: 0.9995 - loss: 0.0532 - no_background_f1: 0.3967\n",
      "Evaluating epoch-002-val_loss-0.0534.keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 01:04:45.578552: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-02-18 01:04:45.578675: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 29ms/step - background_only_f1: 0.9995 - loss: 0.0537 - no_background_f1: 0.3418\n",
      "Evaluating epoch-003-val_loss-0.0534.keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 01:05:11.958751: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 29ms/step - background_only_f1: 0.9995 - loss: 0.0538 - no_background_f1: 0.2851\n",
      "Evaluating epoch-004-val_loss-0.0538.keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 01:05:38.272482: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11415485918501342945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - background_only_f1: 0.9995 - loss: 0.0541 - no_background_f1: 0.2421\n",
      "Evaluating epoch-005-val_loss-0.0522.keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 01:06:04.977580: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 28ms/step - background_only_f1: 0.9995 - loss: 0.0525 - no_background_f1: 0.3581\n",
      "Evaluating epoch-006-val_loss-0.0526.keras...\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 29ms/step - background_only_f1: 0.9995 - loss: 0.0529 - no_background_f1: 0.2879\n",
      "                        Checkpoint  \\\n",
      "0  epoch-001-val_loss-0.0529.keras   \n",
      "1  epoch-002-val_loss-0.0534.keras   \n",
      "2  epoch-003-val_loss-0.0534.keras   \n",
      "3  epoch-004-val_loss-0.0538.keras   \n",
      "4  epoch-005-val_loss-0.0522.keras   \n",
      "5  epoch-006-val_loss-0.0526.keras   \n",
      "\n",
      "                                             Results  \n",
      "0  [0.05286713317036629, 0.40154725313186646, 0.9...  \n",
      "1  [0.05337876081466675, 0.34328600764274597, 0.9...  \n",
      "2  [0.05342801287770271, 0.2842392325401306, 0.99...  \n",
      "3  [0.053841929882764816, 0.24166183173656464, 0....  \n",
      "4  [0.052173957228660583, 0.3586251735687256, 0.9...  \n",
      "5  [0.05256197229027748, 0.2876722812652588, 0.99...  \n",
      "Validation results saved to 'validation_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "max_time_seconds = 3600*12  # 1 hour is 3600 seconds\n",
    "batch_size = 28\n",
    "epochs = 400  # Set high enough to allow stopping by callback\n",
    "steps_per_epoch = 7178\n",
    "\n",
    "# print('Compiling train dataset')\n",
    "# train_dataset = prep_dataset_from_tfrecord(\"TestValTrain/train.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=True,\n",
    "#                                 shuffle_buffer=10000,\n",
    "#                                 total_records=200985,\n",
    "#                                 num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "#                                 )\n",
    "# train_dataset = train_dataset.repeat()\n",
    "\n",
    "print('Compiling val dataset')\n",
    "val_dataset = prep_dataset_from_tfrecord(\"TestValTrain/val.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=False,\n",
    "                                shuffle_buffer=5000,\n",
    "                                total_records=23645,\n",
    "                                num_to_drop=13, # Batch size 28 leaves remainder of 13 records\n",
    "                                seed=42 # Seed for dropping the same 13 records every time\n",
    "                                )\n",
    "# val_dataset = val_dataset.repeat()\n",
    "# test_dataset = prep_dataset_from_tfrecord(\"TestValTrain/test.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=False,\n",
    "#                                 shuffle_buffer=5000,\n",
    "#                                 total_records=11824,\n",
    "#                                 num_to_drop=8, # Batch size 28 leaves remainder of 13 records\n",
    "#                                 seed=42 # Seed for dropping the same 8 records every time\n",
    "#                                 )\n",
    "\n",
    "# history = dcnn_model.fit(\n",
    "#         train_dataset, \n",
    "#         validation_data=val_dataset,\n",
    "#         # batch_size=batch_size,\n",
    "#         epochs=epochs,\n",
    "#         steps_per_epoch=steps_per_epoch,\n",
    "#         callbacks=[early_stopping_cb, checkpoint_cb, CleanupCallback(), TimeLimit(max_time_seconds=max_time_seconds)]\n",
    "#         )\n",
    "\n",
    "# print('Saving model...')\n",
    "# dcnn_model.save(\"Finished_Model.keras\")\n",
    "# # upload_file(\"Finished_Model.keras\", s3_bucket, \"checkpoints/Finished_Model.keras)\n",
    "# print(f\"📁 Model saved!\")\n",
    "\n",
    "\n",
    "# loss_fn = CustomBinaryCrossentropyLoss(\n",
    "#     dominant_class_index=0,\n",
    "#     dominant_correct_multiplier=0.07,\n",
    "#     dominant_incorrect_multiplier=2.5,\n",
    "#     other_class_multiplier=2.0,\n",
    "#     smoothing_multiplier=0.5\n",
    "# )\n",
    "\n",
    "loss_fn = CustomBinaryCrossentropyLoss(\n",
    "        dominant_class_index=0,\n",
    "        dominant_correct_multiplier=0.95,\n",
    "        dominant_incorrect_multiplier=2.0,\n",
    "        other_class_true_positive_multiplier=0.125,\n",
    "        other_class_false_negative_multiplier=5.0,\n",
    "        other_class_false_positive_multiplier=2.0,\n",
    "        other_class_true_negative_multiplier=1.0,\n",
    "        smoothing_multiplier=0.3,\n",
    "        smoothing_as_correct=True\n",
    "    )\n",
    "\n",
    "# Define your checkpoint directory\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "# Store results in a list\n",
    "results_list = []\n",
    "\n",
    "# Loop through all saved model files\n",
    "for filename in sorted(os.listdir(checkpoint_dir)):  # Ensure sorted order\n",
    "    if filename.endswith(\".keras\"):  # Adjust if using TensorFlow checkpoints\n",
    "        model_path = os.path.join(checkpoint_dir, filename)\n",
    "        print(f\"Evaluating {filename}...\")\n",
    "\n",
    "        # Load the model (Include custom loss/metrics if necessary)\n",
    "        model = models.load_model(model_path) \n",
    "        model.compile(\n",
    "                    loss=loss_fn,\n",
    "                    metrics=[\n",
    "                        CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "                        # CustomNoBackgroundF1Score(num_classes=5, average='macro', threshold=0.5),\n",
    "                        CustomBackgroundOnlyF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "                        # CustomBackgroundOnlyF1Score(num_classes=5, average='macro', threshold=0.5)\n",
    "                        ]\n",
    "                    )\n",
    "            \n",
    "\n",
    "        # Evaluate on validation dataset\n",
    "        results = model.evaluate(val_dataset, verbose=1)  # Suppress output\n",
    "\n",
    "        # Store results (Modify column names as needed)\n",
    "        results_list.append({\"Checkpoint\": filename, \"Results\" : results})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_results = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)\n",
    "\n",
    "# Save the DataFrame to a CSV file for later analysis\n",
    "df_results.to_csv(\"validation_results.csv\", index=False)\n",
    "\n",
    "print(\"Validation results saved to 'validation_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0bc6ffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Checkpoint': 'epoch-001-val_loss-0.0263.keras', 'Results': [0.02629547193646431, 0.4233812093734741, 0.4233812093734741, 0.9988483190536499, 0.9988483190536499, 0.9988483190536499, 0.4332113564014435]}, {'Checkpoint': 'epoch-003-val_loss-0.0247.keras', 'Results': [0.02472471073269844, 0.37515681982040405, 0.37515681982040405, 0.9988055229187012, 0.9988055229187012, 0.9988055229187012, 0.3802448809146881]}, {'Checkpoint': 'epoch-004-val_loss-0.0244.keras', 'Results': [0.02436007373034954, 0.518424928188324, 0.518424928188324, 0.9988994598388672, 0.9988994598388672, 0.9988994002342224, 0.5233819484710693]}, {'Checkpoint': 'epoch-005-val_loss-0.0243.keras', 'Results': [0.024295059964060783, 0.5021491050720215, 0.5021491050720215, 0.9988505244255066, 0.9988505244255066, 0.9988505244255066, 0.5132834911346436]}, {'Checkpoint': 'epoch-006-val_loss-0.0241.keras', 'Results': [0.024132538586854935, 0.41637763381004333, 0.41637763381004333, 0.9988260269165039, 0.9988260269165039, 0.9988260269165039, 0.42628878355026245]}, {'Checkpoint': 'epoch-007-val_loss-0.0240.keras', 'Results': [0.024034935981035233, 0.4626420736312866, 0.4626420736312866, 0.9987021684646606, 0.9987021684646606, 0.9987021684646606, 0.47210386395454407]}, {'Checkpoint': 'epoch-008-val_loss-0.0241.keras', 'Results': [0.024082716554403305, 0.4362534284591675, 0.4362534284591675, 0.9988309144973755, 0.9988309144973755, 0.9988309144973755, 0.4443570375442505]}, {'Checkpoint': 'epoch-009-val_loss-0.0240.keras', 'Results': [0.02401990070939064, 0.4754262864589691, 0.4754262864589691, 0.9987407326698303, 0.9987407326698303, 0.9987407326698303, 0.48662564158439636]}, {'Checkpoint': 'epoch-010-val_loss-0.0240.keras', 'Results': [0.02400152012705803, 0.45425015687942505, 0.45425015687942505, 0.9987397789955139, 0.9987397789955139, 0.9987397789955139, 0.46593669056892395]}, {'Checkpoint': 'epoch-011-val_loss-0.0243.keras', 'Results': [0.024292444810271263, 0.5033577084541321, 0.5033577084541321, 0.9989486336708069, 0.9989486336708069, 0.9989486336708069, 0.5133631229400635]}, {'Checkpoint': 'epoch-012-val_loss-0.0240.keras', 'Results': [0.02401198446750641, 0.4855882227420807, 0.4855882227420807, 0.9988089203834534, 0.9988089203834534, 0.9988089203834534, 0.49280065298080444]}, {'Checkpoint': 'epoch-013-val_loss-0.0240.keras', 'Results': [0.02396787889301777, 0.495547354221344, 0.495547354221344, 0.9987152814865112, 0.9987152814865112, 0.9987152814865112, 0.5069794654846191]}, {'Checkpoint': 'epoch-016-val_loss-0.0240.keras', 'Results': [0.023957543075084686, 0.47803619503974915, 0.47803619503974915, 0.9987313747406006, 0.9987313747406006, 0.9987313747406006, 0.49239155650138855]}]\n"
     ]
    }
   ],
   "source": [
    "print(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d6be1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your checkpoint directory and file pattern.\n",
    "# checkpoint_dir = \"checkpoints\"\n",
    "# # Assume your checkpoint files follow the pattern 'cp-XXXX.ckpt'\n",
    "# checkpoint_pattern = os.path.join(checkpoint_dir, \"epoch-*-val_loss-*.keras\")\n",
    "\n",
    "# # Find the most recent checkpoint file.\n",
    "# checkpoint_files = glob.glob(checkpoint_pattern)\n",
    "# if checkpoint_files:\n",
    "#     latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "#     print(\"Resuming from checkpoint:\", latest_checkpoint)\n",
    "    \n",
    "#     # Load the entire model from the checkpoint.\n",
    "#     dcnn_model = models.load_model(latest_checkpoint)\n",
    "#     dcnn_model.compile()\n",
    "    \n",
    "#     epoch_str = os.path.basename(latest_checkpoint).split('-')[1]\n",
    "#     # Remove any file extension; adjust the splitting as needed.\n",
    "#     epoch_num = int(''.join(filter(str.isdigit, epoch_str)))\n",
    "#     initial_epoch = epoch_num\n",
    "# else:\n",
    "#     print(\"No checkpoint found. Starting training from scratch.\")\n",
    "#     # Build and compile your model as you normally do.\n",
    "#     dcnn_model = create_dcnn_model(5, 5000, 5)\n",
    "#     dcnn_model.compile(optimizer=optimizers.Adam(learning_rate=0.000686),\n",
    "#                     loss=custom_binary_crossentropy_loss(\n",
    "#                         dominant_class_index=0,\n",
    "#                         dominant_correct_multiplier=0.07,\n",
    "#                         dominant_incorrect_multiplier=2.5,\n",
    "#                         other_class_multiplier=2.0,\n",
    "#                         smoothing_multiplier=0.5\n",
    "#                     ),\n",
    "#                     metrics=[\n",
    "#                         CustomNoBackgroundF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "#                         CustomNoBackgroundF1Score(num_classes=5, average='macro', threshold=0.5),\n",
    "#                         CustomBackgroundOnlyF1Score(num_classes=5, average='weighted', threshold=0.5),\n",
    "#                         CustomBackgroundOnlyF1Score(num_classes=5, average='macro', threshold=0.5)\n",
    "#                         ]\n",
    "#                     )\n",
    "#     dcnn_model.summary()\n",
    "#     initial_epoch = 0\n",
    "\n",
    "# # Continue training.\n",
    "# history = dcnn_model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=val_dataset,\n",
    "#     epochs=epochs,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "#     callbacks=[early_stopping_cb, checkpoint_cb, CleanupCallback(), TimeLimit(max_time_seconds=max_time_seconds)],\n",
    "#     initial_epoch=initial_epoch  # This tells Keras to start counting epochs from here.\n",
    "# )\n",
    "\n",
    "# # Save final model if needed.\n",
    "# dcnn_model.save(\"Finished_Model.keras\")\n",
    "# print(\"📁 Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e58e17a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'summarize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdcnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'summarize'"
     ]
    }
   ],
   "source": [
    "dcnn_model.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff6beb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
