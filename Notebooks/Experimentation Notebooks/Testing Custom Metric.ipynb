{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains testing for the various custom metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 01:30:58.996611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-26 01:30:59.185169: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-26 01:30:59.239137: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 01:30:59.609060: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-26 01:31:01.074801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundF1Score(metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', threshold=0.5, name='no_background_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers non-dominant classes (ignoring index 0).\n",
    "        \n",
    "        This version is designed for multi-encoded labels where:\n",
    "          - The dominant class (index 0) is represented as a hard label [1, 0, 0, ...]\n",
    "          - For non-dominant classes (indices 1 to num_classes-1), only an exact label of 1 is considered positive.\n",
    "            (Any partial credit/smoothed values below 1 are treated as 0.)\n",
    "          - Predictions are thresholded (default threshold = 0.5) to decide 1 vs. 0.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "\n",
    "        # Create state variables to accumulate counts for each class.\n",
    "        # We use a vector of length num_classes but we will update only indices 1...num_classes-1.\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the metric state.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, num_classes). These are multi-encoded labels.\n",
    "                    For non-dominant classes, a label is considered positive only if it is exactly 1.\n",
    "            y_pred: Tensor of shape (batch_size, num_classes) with predictions (e.g. probabilities).\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Flatten all dimensions except the last one (which should be num_classes).\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "        \n",
    "        # We want to ignore the dominant class (index 0) and work on classes 1...num_classes-1.\n",
    "        # Assume y_true and y_pred are both of shape (batch_size, num_classes).\n",
    "        y_true_non_dominant = y_true[:, 1:]\n",
    "        y_pred_non_dominant = y_pred[:, 1:]\n",
    "        \n",
    "        # For ground truth: treat a class as positive only if its value is exactly 1.\n",
    "        one_value = tf.cast(1.0, dtype=y_true_non_dominant.dtype)\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_non_dominant, one_value), tf.int32)\n",
    "        # For predictions: apply thresholding.\n",
    "        y_pred_bin = tf.cast(y_pred_non_dominant >= self.threshold, tf.int32)\n",
    "        \n",
    "        # (Optionally) apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.int32)\n",
    "            sample_weight = tf.reshape(sample_weight, (-1, 1))\n",
    "            y_true_bin = y_true_bin * sample_weight\n",
    "            y_pred_bin = y_pred_bin * sample_weight\n",
    "        \n",
    "        # Compute per-class true positives, false positives, and false negatives for non-dominant classes.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_bin) * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        \n",
    "        # Our state variables have length num_classes. We want to update only indices 1... with our computed values.\n",
    "        zeros = tf.zeros([1], dtype=tf.float32)\n",
    "        tp_update = tf.concat([zeros, tp], axis=0)\n",
    "        fp_update = tf.concat([zeros, fp], axis=0)\n",
    "        fn_update = tf.concat([zeros, fn], axis=0)\n",
    "        \n",
    "        self.true_positives.assign_add(tp_update)\n",
    "        self.false_positives.assign_add(fp_update)\n",
    "        self.false_negatives.assign_add(fn_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score over the non-dominant classes (indices 1...num_classes-1).\n",
    "        \"\"\"\n",
    "        # Select non-dominant classes only.\n",
    "        tp = self.true_positives[1:]\n",
    "        fp = self.false_positives[1:]\n",
    "        fn = self.false_negatives[1:]\n",
    "        \n",
    "        precision = tf.math.divide_no_nan(tp, tp + fp)\n",
    "        recall = tf.math.divide_no_nan(tp, tp + fn)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            support = tp + fn\n",
    "            weighted_f1 = tf.reduce_sum(f1 * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro\n",
    "            return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n",
    "            \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the metric, so it can be recreated later.\n",
    "        \"\"\"\n",
    "        config = super(CustomNoBackgroundF1Score, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'average': self.average,\n",
    "            'threshold': self.threshold,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomConditionalF1Score(metrics.Metric):\n",
    "    def __init__(self, threshold=0.5, average='weighted', filter_mode='either', name='conditional_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that computes the F1 score only for target columns (columns 1-4).\n",
    "        Additionally, only rows meeting a filtering criterion are included in the calculation.\n",
    "        \n",
    "        Args:\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default = 0.5).\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            filter_mode (str): Determines which rows to include based on the target columns.\n",
    "                               Options:\n",
    "                                  - 'pred': Only rows where y_pred (after thresholding) has at least one 1.\n",
    "                                  - 'true': Only rows where y_true (exactly equal to 1) has at least one 1.\n",
    "                                  - 'either': Rows where either y_true or y_pred has at least one 1.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \n",
    "        Note:\n",
    "            This metric only tracks columns 1-4 (0-indexed). Column 0 (the dominant background class)\n",
    "            is ignored completely.\n",
    "        \"\"\"\n",
    "        metric_name = f'{name}_{filter_mode}'\n",
    "        \n",
    "        super(CustomConditionalF1Score, self).__init__(name=metric_name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "        \n",
    "        if filter_mode not in ['pred', 'true', 'either']:\n",
    "            raise ValueError(\"filter_mode must be 'pred', 'true', or 'either'\")\n",
    "        self.filter_mode = filter_mode\n",
    "        \n",
    "        # We are tracking only 4 target columns (columns 1 to 4).\n",
    "        self.num_target_columns = 4\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Reshape inputs so that the last dimension is the number of classes.\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "        # Only consider columns 1-4 (ignoring index 0).\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        \n",
    "        # For ground truth, treat a label as positive only if its value is exactly 1.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_subset, 1.0), tf.int32)\n",
    "        # For predictions, apply the threshold to decide 1 vs. 0.\n",
    "        y_pred_bin = tf.cast(y_pred_subset >= self.threshold, tf.int32)\n",
    "        \n",
    "        # Compute a row-level mask based on the filter_mode.\n",
    "        if self.filter_mode == 'pred':\n",
    "            mask = tf.reduce_any(tf.equal(y_pred_bin, 1), axis=1)\n",
    "        elif self.filter_mode == 'true':\n",
    "            mask = tf.reduce_any(tf.equal(y_true_bin, 1), axis=1)\n",
    "        else:  # 'either'\n",
    "            mask = tf.logical_or(\n",
    "                tf.reduce_any(tf.equal(y_pred_bin, 1), axis=1),\n",
    "                tf.reduce_any(tf.equal(y_true_bin, 1), axis=1)\n",
    "            )\n",
    "        \n",
    "        # Apply the mask so only selected rows are used for the metric update.\n",
    "        y_true_filtered = tf.boolean_mask(y_true_bin, mask)\n",
    "        y_pred_filtered = tf.boolean_mask(y_pred_bin, mask)\n",
    "        \n",
    "        # Optionally apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.reshape(sample_weight, [-1, 1])\n",
    "            y_true_filtered = y_true_filtered * sample_weight\n",
    "            y_pred_filtered = y_pred_filtered * sample_weight\n",
    "        \n",
    "        # Compute per-column true positives, false positives, and false negatives.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_filtered * y_pred_filtered, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_filtered) * y_pred_filtered, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_filtered * (1 - y_pred_filtered), tf.float32), axis=0)\n",
    "        \n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        precision = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
    "        recall = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            support = self.true_positives + self.false_negatives\n",
    "            return tf.reduce_sum(f1 * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "        else:  # 'macro'\n",
    "            return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = super(CustomConditionalF1Score, self).get_config()\n",
    "        config.update({\n",
    "            'threshold': self.threshold,\n",
    "            'average': self.average,\n",
    "            'filter_mode': self.filter_mode,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomFalsePositiveDistance(metrics.Metric):\n",
    "    def __init__(self, num_classes, threshold=0.5, window=100, name='false_positive_distance', **kwargs):\n",
    "        \"\"\"\n",
    "        Metric that accumulates a running average “distance” error for false positive predictions,\n",
    "        ignoring the dominant (background) class (index 0).\n",
    "\n",
    "        For each false positive (i.e. a prediction >= threshold when the strict label is not 1),\n",
    "        the distance is computed from the raw label value (which encodes proximity to an actual annotation)\n",
    "        as follows:\n",
    "\n",
    "            distance = 1 + ((max_credit - v) * (window / max_credit))\n",
    "\n",
    "        where:\n",
    "            - v is the raw label value at that position,\n",
    "            - max_credit is the maximum smoothing credit (0.5 in our scheme), so that if v == 0.5 the distance is 1,\n",
    "              and if v == 0 the distance is 1 + window (i.e. 101 for window=100).\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            threshold (float): Threshold on y_pred to decide a positive.\n",
    "            window (int): Window size used in the smoothing scheme.\n",
    "            name (str): Name of the metric.\n",
    "        \"\"\"\n",
    "        super(CustomFalsePositiveDistance, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        self.window = float(window)\n",
    "        self.max_credit = 0.5  # Based on your smoothing scheme.\n",
    "\n",
    "        # State variables to accumulate total distance and count of false positives.\n",
    "        self.total_distance = self.add_weight(\n",
    "            name='total_distance', initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positive_count = self.add_weight(\n",
    "            name='false_positive_count', initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        For non-dominant classes (indices 1:), this method:\n",
    "          - thresholds predictions,\n",
    "          - identifies false positives (prediction is positive while strict label != 1),\n",
    "          - computes the distance error from the raw (smoothed) label value, and\n",
    "          - accumulates the sum of distances and count of false positives.\n",
    "        \"\"\"\n",
    "        # Ensure shape (batch_size, num_classes)\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "\n",
    "        # Ignore the dominant/background class (index 0)\n",
    "        y_true_non = y_true[:, 1:]\n",
    "        y_pred_non = y_pred[:, 1:]\n",
    "\n",
    "        # Threshold predictions\n",
    "        y_pred_bin = tf.cast(y_pred_non >= self.threshold, tf.float32)\n",
    "\n",
    "        # For strict classification, a label is positive only if it is exactly 1.\n",
    "        # So a false positive is when y_pred_bin==1 but y_true (strict) is not 1.\n",
    "        # (This is similar to your F1 metric, i.e. smoothing values are treated as negatives.)\n",
    "        false_positive_mask = tf.logical_and(\n",
    "            tf.equal(y_pred_bin, 1.0),\n",
    "            tf.not_equal(y_true_non, 1.0)\n",
    "        )\n",
    "        false_positive_mask = tf.cast(false_positive_mask, tf.float32)\n",
    "\n",
    "        # Compute distance per element.\n",
    "        # In our smoothing scheme:\n",
    "        #   - At a true annotation (v = 1), we wouldn’t count a false positive.\n",
    "        #   - In a smoothed region, the maximum credit is 0.5.\n",
    "        #   - We define:\n",
    "        #       distance = 1 + ((max_credit - v) * (window / max_credit))\n",
    "        #     so that if v == 0.5, distance = 1, and if v == 0, distance = 1 + window.\n",
    "        distance = 1.0 + (self.max_credit - y_true_non) * (self.window / self.max_credit)\n",
    "        distance = tf.where(distance >= 101.0, tf.constant(125.0, dtype=distance.dtype), distance)\n",
    "\n",
    "        # Only include entries that are false positives.\n",
    "        false_positive_distance = distance * false_positive_mask\n",
    "\n",
    "        # Sum distances and count false positives.\n",
    "        sum_distance = tf.reduce_sum(false_positive_distance)\n",
    "        count = tf.reduce_sum(false_positive_mask)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.reshape(sample_weight, [-1, 1])\n",
    "            sum_distance = tf.reduce_sum(false_positive_distance * sample_weight)\n",
    "            count = tf.reduce_sum(false_positive_mask * sample_weight)\n",
    "\n",
    "        self.total_distance.assign_add(sum_distance)\n",
    "        self.false_positive_count.assign_add(count)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"Returns the average distance error over all false positives (or 0 if none).\"\"\"\n",
    "        return tf.math.divide_no_nan(self.total_distance, self.false_positive_count)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"Resets the accumulated total distance and count.\"\"\"\n",
    "        self.total_distance.assign(0.0)\n",
    "        self.false_positive_count.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomFalsePositiveDistance, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'threshold': self.threshold,\n",
    "            'window': self.window,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundAUC(metrics.Metric):\n",
    "    def __init__(self, curve='PR', name='no_background_auc', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom AUC metric computed only for columns 1-4.\n",
    "\n",
    "        Args:\n",
    "            curve (str): The type of AUC curve to use, e.g. 'ROC' (default) or 'PR'.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundAUC, self).__init__(name=name, **kwargs)\n",
    "        # Store the curve parameter as a string to aid serialization.\n",
    "        self.curve = curve  \n",
    "        # Create one AUC metric per target column (columns 1-4).\n",
    "        self.auc_metrics = [\n",
    "            metrics.AUC(curve=self.curve, name=f'auc_col_{i+1}')\n",
    "            for i in range(4)\n",
    "        ]\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Ensure inputs are 2D tensors with shape (batch_size, num_classes).\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        # Select target columns (1-4) and ignore background (column 0).\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        # For each target column, update the corresponding AUC metric.\n",
    "        for i, auc_metric in enumerate(self.auc_metrics):\n",
    "            # Ground truth: positive only if exactly equal to 1.\n",
    "            y_true_col = tf.cast(tf.equal(y_true_subset[:, i], 1.0), tf.float32)\n",
    "            y_pred_col = y_pred_subset[:, i]\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = tf.reshape(sample_weight, [-1])\n",
    "                auc_metric.update_state(y_true_col, y_pred_col, sample_weight=sample_weight)\n",
    "            else:\n",
    "                auc_metric.update_state(y_true_col, y_pred_col)\n",
    "\n",
    "    def result(self):\n",
    "        # Average AUC over all target columns.\n",
    "        auc_results = [auc_metric.result() for auc_metric in self.auc_metrics]\n",
    "        return tf.reduce_mean(auc_results)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for auc_metric in self.auc_metrics:\n",
    "            auc_metric.reset_states()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomNoBackgroundAUC, self).get_config()\n",
    "        # Return the curve as a string.\n",
    "        config.update({\n",
    "            'curve': self.curve,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundAccuracy(metrics.Metric):\n",
    "    def __init__(self, threshold=0.5, name='no_background_accuracy', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom accuracy metric computed only for columns 1-4.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Threshold for y_pred (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.total_correct = self.add_weight(name='total_correct', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_count = self.add_weight(name='total_count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Reshape inputs to 2D tensors.\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        # Extract columns 1-4.\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        # Binarize ground truth: positive if exactly 1.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_subset, 1.0), tf.int32)\n",
    "        # Binarize predictions using the threshold.\n",
    "        y_pred_bin = tf.cast(y_pred_subset >= self.threshold, tf.int32)\n",
    "        # Element-wise correctness.\n",
    "        correct = tf.cast(tf.equal(y_true_bin, y_pred_bin), tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            # Tile sample weights to match the shape of correct.\n",
    "            sample_weight = tf.tile(sample_weight, [1, tf.shape(correct)[1]])\n",
    "            correct = correct * sample_weight\n",
    "            count = tf.reduce_sum(sample_weight)\n",
    "        else:\n",
    "            count = tf.cast(tf.size(correct), tf.float32)\n",
    "        self.total_correct.assign_add(tf.reduce_sum(correct))\n",
    "        self.total_count.assign_add(count)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.total_correct, self.total_count)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total_correct.assign(0)\n",
    "        self.total_count.assign(0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomNoBackgroundAccuracy, self).get_config()\n",
    "        config.update({'threshold': self.threshold})\n",
    "        return config\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundPrecision(metrics.Metric):\n",
    "    def __init__(self, threshold=0.5, average='weighted', name='no_background_precision', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom precision metric computed only for columns 1-4.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Threshold for y_pred (default 0.5).\n",
    "            average (str): 'weighted' (default) or 'macro'.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundPrecision, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "        self.num_target_columns = 4\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        # For weighted averaging, we also need the support (true positives + false negatives).\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Reshape inputs.\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        # Extract target columns (1-4).\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        # Binarize ground truth and predictions.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_subset, 1.0), tf.int32)\n",
    "        y_pred_bin = tf.cast(y_pred_subset >= self.threshold, tf.int32)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.tile(sample_weight, [1, tf.shape(y_true_bin)[1]])\n",
    "            y_true_bin = y_true_bin * tf.cast(sample_weight, tf.int32)\n",
    "            y_pred_bin = y_pred_bin * tf.cast(sample_weight, tf.int32)\n",
    "        # Compute counts per column.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_bin) * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
    "        if self.average == 'weighted':\n",
    "            # Weight each column by its support (TP + FN).\n",
    "            support = self.true_positives + self.false_negatives\n",
    "            weighted_precision = tf.reduce_sum(precision * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_precision\n",
    "        else:  # macro\n",
    "            return tf.reduce_mean(precision)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "        self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
    "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomNoBackgroundPrecision, self).get_config()\n",
    "        config.update({\n",
    "            'threshold': self.threshold,\n",
    "            'average': self.average,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomNoBackgroundRecall(metrics.Metric):\n",
    "    def __init__(self, threshold=0.5, average='weighted', name='no_background_recall', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom recall metric computed only for columns 1-4.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Threshold for y_pred (default 0.5).\n",
    "            average (str): 'weighted' (default) or 'macro'.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNoBackgroundRecall, self).__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "        self.num_target_columns = 4\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(self.num_target_columns,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Reshape inputs.\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        # Extract target columns (1-4).\n",
    "        y_true_subset = y_true[:, 1:5]\n",
    "        y_pred_subset = y_pred[:, 1:5]\n",
    "        # Binarize ground truth and predictions.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_subset, 1.0), tf.int32)\n",
    "        y_pred_bin = tf.cast(y_pred_subset >= self.threshold, tf.int32)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            sample_weight = tf.tile(sample_weight, [1, tf.shape(y_true_bin)[1]])\n",
    "            y_true_bin = y_true_bin * tf.cast(sample_weight, tf.int32)\n",
    "            y_pred_bin = y_pred_bin * tf.cast(sample_weight, tf.int32)\n",
    "        # Compute per-column true positives and false negatives.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall = tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
    "        if self.average == 'weighted':\n",
    "            support = self.true_positives + self.false_negatives\n",
    "            weighted_recall = tf.reduce_sum(recall * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_recall\n",
    "        else:\n",
    "            return tf.reduce_mean(recall)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomNoBackgroundRecall, self).get_config()\n",
    "        config.update({\n",
    "            'threshold': self.threshold,\n",
    "            'average': self.average,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 - y_true: [[1. 0. 1. 0. 0.]] \n",
      "y_pred: [[0.  0.2 0.9 0.1 0. ]]\n",
      "Test Case 2 - y_true: [[1. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 0.]] \n",
      "y_pred: [[0.  0.8 0.2 0.1 0.1]\n",
      " [0.  0.3 0.7 0.9 0.2]]\n",
      "Test Case 3 - y_true: [[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]] \n",
      "y_pred: [[0.  0.1 0.2 0.3 0.4]\n",
      " [0.  0.1 0.2 0.3 0.4]]\n",
      "Test Case 4 - y_true: [[1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]] \n",
      "y_pred: [[0.  0.9 0.8 0.9 0.9]\n",
      " [0.  0.9 0.9 0.8 0.9]]\n",
      "Test Case 5 - y_true: [[1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1.]] \n",
      "y_pred: [[0.  0.2 0.2 0.2 0.2]\n",
      " [0.  0.2 0.2 0.2 0.2]]\n",
      "Test Case 6 - y_true: [[1.  0.  0.5 1.  0. ]\n",
      " [1.  0.  0.  1.  0. ]] \n",
      "y_pred: [[0.   0.6  0.6  0.95 0.3 ]\n",
      " [0.   0.4  0.4  0.9  0.3 ]]\n",
      "Test Case 7 - y_true: [[1. 1. 0. 1. 0.]] \n",
      "y_pred: [[0.    0.5   0.5   0.499 0.5  ]]\n",
      "Test Case 8 - y_true: [[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]] \n",
      "y_pred: [[0.  0.2 0.3 0.1 0.4]\n",
      " [0.  0.2 0.3 0.1 0.4]\n",
      " [0.  0.2 0.3 0.1 0.4]]\n",
      "Test Case 9 - y_true: [[1. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 1.]] \n",
      "y_pred: [[0.   0.9  0.2  0.1  0.3 ]\n",
      " [0.   0.3  0.8  0.2  0.1 ]\n",
      " [0.   0.2  0.1  0.85 0.4 ]\n",
      " [0.   0.95 0.95 0.95 0.95]]\n",
      "Test Case 10 - y_true: [[1. 0. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 0.]] \n",
      "y_pred: [[0.   0.1  0.8  0.2  0.9 ]\n",
      " [0.   0.85 0.2  0.9  0.1 ]]\n"
     ]
    }
   ],
   "source": [
    "# Each test case defines y_true and y_pred as np.arrays with shape (batch_size, 5).\n",
    "\n",
    "# Test Case 1: Perfect prediction on a single sample.\n",
    "y_true1 = np.array([[1, 0, 1, 0, 0]], dtype=np.float32)\n",
    "y_pred1 = np.array([[0.0, 0.2, 0.9, 0.1, 0.0]], dtype=np.float32)\n",
    "# Explanation: Only the non-background at index 2 is positive, and y_pred2 >= 0.5.\n",
    "\n",
    "# Test Case 2: Two samples with mixed positives in target columns.\n",
    "y_true2 = np.array([\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 1, 0]\n",
    "], dtype=np.float32)\n",
    "y_pred2 = np.array([\n",
    "    [0.0, 0.8, 0.2, 0.1, 0.1],\n",
    "    [0.0, 0.3, 0.7, 0.9, 0.2]\n",
    "], dtype=np.float32)\n",
    "# Explanation: Some rows have extra positives while others miss a target.\n",
    "\n",
    "# Test Case 3: All negatives on target columns.\n",
    "y_true3 = np.array([\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0]\n",
    "], dtype=np.float32)\n",
    "y_pred3 = np.array([\n",
    "    [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "], dtype=np.float32)\n",
    "# Explanation: No target positive exists in either sample.\n",
    "\n",
    "# Test Case 4: Over-prediction scenario with many false positives.\n",
    "y_true4 = np.array([\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 1, 0]\n",
    "], dtype=np.float32)\n",
    "y_pred4 = np.array([\n",
    "    [0.0, 0.9, 0.8, 0.9, 0.9],\n",
    "    [0.0, 0.9, 0.9, 0.8, 0.9]\n",
    "], dtype=np.float32)\n",
    "# Explanation: High prediction values across the board lead to extra positives.\n",
    "\n",
    "# Test Case 5: Under-prediction scenario with many false negatives.\n",
    "y_true5 = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 1]\n",
    "], dtype=np.float32)\n",
    "y_pred5 = np.array([\n",
    "    [0.0, 0.2, 0.2, 0.2, 0.2],\n",
    "    [0.0, 0.2, 0.2, 0.2, 0.2]\n",
    "], dtype=np.float32)\n",
    "# Explanation: Even though y_true has several positives, y_pred values are too low to cross the threshold.\n",
    "\n",
    "# Test Case 6: Mixed values with smoothing (note: only exact 1 counts as positive).\n",
    "y_true6 = np.array([\n",
    "    [1, 0, 0.5, 1, 0],\n",
    "    [1, 0, 0, 1, 0]\n",
    "], dtype=np.float32)\n",
    "y_pred6 = np.array([\n",
    "    [0.0, 0.6, 0.6, 0.95, 0.3],\n",
    "    [0.0, 0.4, 0.4, 0.9, 0.3]\n",
    "], dtype=np.float32)\n",
    "# Explanation: The 0.5 in y_true6 is not counted as positive by the metric (only an exact 1 does).\n",
    "\n",
    "# Test Case 7: Borderline threshold values.\n",
    "y_true7 = np.array([[1, 1, 0, 1, 0]], dtype=np.float32)\n",
    "y_pred7 = np.array([[0.0, 0.5, 0.5, 0.499, 0.5]], dtype=np.float32)\n",
    "# Explanation: Values exactly equal to 0.5 should be interpreted as positive; those below remain negative.\n",
    "# TP, FP, FN, FP Precision = 1/3, recall = 1/2, accuracy = 0.25, support = 2 \n",
    "\n",
    "# Test Case 8: Multiple samples where no sample has a target positive.\n",
    "y_true8 = np.array([\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0]\n",
    "], dtype=np.float32)\n",
    "y_pred8 = np.array([\n",
    "    [0.0, 0.2, 0.3, 0.1, 0.4],\n",
    "    [0.0, 0.2, 0.3, 0.1, 0.4],\n",
    "    [0.0, 0.2, 0.3, 0.1, 0.4]\n",
    "], dtype=np.float32)\n",
    "# Explanation: All rows are “negative” for target classes.\n",
    "\n",
    "# Test Case 9: Larger batch with a mix of correct and error cases.\n",
    "y_true9 = np.array([\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 1, 0],\n",
    "    [1, 1, 1, 1, 1]\n",
    "], dtype=np.float32)\n",
    "y_pred9 = np.array([\n",
    "    [0.0, 0.9, 0.2, 0.1, 0.3],\n",
    "    [0.0, 0.3, 0.8, 0.2, 0.1],\n",
    "    [0.0, 0.2, 0.1, 0.85, 0.4],\n",
    "    [0.0, 0.95, 0.95, 0.95, 0.95]\n",
    "], dtype=np.float32)\n",
    "# Explanation: Varying behavior across rows; the last row has all targets positive.\n",
    "\n",
    "# Test Case 10: Mixed predictions with different pattern across two samples.\n",
    "y_true10 = np.array([\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [1, 1, 0, 1, 0]\n",
    "], dtype=np.float32)\n",
    "y_pred10 = np.array([\n",
    "    [0.0, 0.1, 0.8, 0.2, 0.9],\n",
    "    [0.0, 0.85, 0.2, 0.9, 0.1]\n",
    "], dtype=np.float32)\n",
    "# Explanation: One row has a positive at index 2 and index 4; the other at indices 1 and 3.\n",
    "\n",
    "# You can now use the pairs (y_true1, y_pred1) through (y_true10, y_pred10)\n",
    "# to run your tests on each of the custom metrics.\n",
    "# For example:\n",
    "print(\"Test Case 1 - y_true:\", y_true1, \"\\ny_pred:\", y_pred1)\n",
    "print(\"Test Case 2 - y_true:\", y_true2, \"\\ny_pred:\", y_pred2)\n",
    "print(\"Test Case 3 - y_true:\", y_true3, \"\\ny_pred:\", y_pred3)\n",
    "print(\"Test Case 4 - y_true:\", y_true4, \"\\ny_pred:\", y_pred4)\n",
    "print(\"Test Case 5 - y_true:\", y_true5, \"\\ny_pred:\", y_pred5)\n",
    "print(\"Test Case 6 - y_true:\", y_true6, \"\\ny_pred:\", y_pred6)\n",
    "print(\"Test Case 7 - y_true:\", y_true7, \"\\ny_pred:\", y_pred7)\n",
    "print(\"Test Case 8 - y_true:\", y_true8, \"\\ny_pred:\", y_pred8)\n",
    "print(\"Test Case 9 - y_true:\", y_true9, \"\\ny_pred:\", y_pred9)\n",
    "print(\"Test Case 10 - y_true:\", y_true10, \"\\ny_pred:\", y_pred10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CustomNoBackgroundF1Score name=no_background_f1>\n",
      "Test Array 1\n",
      "0.9999999\n",
      "Test Array 2\n",
      "1.0\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "0.6666667\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "1.0\n",
      "Test Array 7\n",
      "0.5\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n",
      "<CustomConditionalF1Score name=conditional_f1_pred>\n",
      "Test Array 1\n",
      "0.9999999\n",
      "Test Array 2\n",
      "1.0\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "0.6666667\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "1.0\n",
      "Test Array 7\n",
      "0.5\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n",
      "<CustomConditionalF1Score name=conditional_f1_true>\n",
      "Test Array 1\n",
      "0.9999999\n",
      "Test Array 2\n",
      "1.0\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "0.6666667\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "1.0\n",
      "Test Array 7\n",
      "0.5\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n",
      "<CustomFalsePositiveDistance name=false_positive_distance>\n",
      "Test Array 1\n",
      "0.0\n",
      "Test Array 2\n",
      "0.0\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "125.0\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "63.0\n",
      "Test Array 7\n",
      "125.0\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "0.0\n",
      "Test Array 10\n",
      "0.0\n",
      "<CustomNoBackgroundAccuracy name=no_background_accuracy>\n",
      "Test Array 1\n",
      "1.0\n",
      "Test Array 2\n",
      "1.0\n",
      "Test Array 3\n",
      "1.0\n",
      "Test Array 4\n",
      "0.25\n",
      "Test Array 5\n",
      "0.25\n",
      "Test Array 6\n",
      "0.75\n",
      "Test Array 7\n",
      "0.25\n",
      "Test Array 8\n",
      "1.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n",
      "<CustomNoBackgroundPrecision name=no_background_precision>\n",
      "Test Array 1\n",
      "0.25\n",
      "Test Array 2\n",
      "0.75\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "0.25\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "0.25\n",
      "Test Array 7\n",
      "0.25\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n",
      "<CustomNoBackgroundRecall name=no_background_recall>\n",
      "Test Array 1\n",
      "0.25\n",
      "Test Array 2\n",
      "0.75\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "0.5\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "0.25\n",
      "Test Array 7\n",
      "0.25\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n",
      "<CustomNoBackgroundF1Score name=no_background_f1>\n",
      "Test Array 1\n",
      "0.25\n",
      "Test Array 2\n",
      "0.75\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "0.33333334\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "0.25\n",
      "Test Array 7\n",
      "0.25\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n",
      "<CustomConditionalF1Score name=conditional_f1_pred>\n",
      "Test Array 1\n",
      "0.25\n",
      "Test Array 2\n",
      "0.75\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "0.33333334\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "0.25\n",
      "Test Array 7\n",
      "0.25\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n",
      "<CustomConditionalF1Score name=conditional_f1_true>\n",
      "Test Array 1\n",
      "0.25\n",
      "Test Array 2\n",
      "0.75\n",
      "Test Array 3\n",
      "0.0\n",
      "Test Array 4\n",
      "0.33333334\n",
      "Test Array 5\n",
      "0.0\n",
      "Test Array 6\n",
      "0.25\n",
      "Test Array 7\n",
      "0.25\n",
      "Test Array 8\n",
      "0.0\n",
      "Test Array 9\n",
      "1.0\n",
      "Test Array 10\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "metrics_lst=[\n",
    "        CustomNoBackgroundF1Score(num_classes=5, threshold=0.5, average='weighted'),  # your existing F1 metric\n",
    "        CustomConditionalF1Score(threshold=0.5, average='weighted', filter_mode='pred'),  # 'pred' 'true' or 'either'\n",
    "        CustomConditionalF1Score(threshold=0.5, average='weighted', filter_mode='true'),  # 'pred' 'true' or 'either'\n",
    "        CustomFalsePositiveDistance(num_classes=5, threshold=0.5, window=100),\n",
    "        # CustomNoBackgroundAUC(curve='ROC'),\n",
    "        CustomNoBackgroundAccuracy(threshold=0.5),\n",
    "        CustomNoBackgroundPrecision(threshold=0.5, average='macro'),\n",
    "        CustomNoBackgroundRecall(threshold=0.5, average='macro'),\n",
    "        CustomNoBackgroundF1Score(num_classes=5, threshold=0.5, average='macro'),  # your existing F1 metric\n",
    "        CustomConditionalF1Score(threshold=0.5, average='macro', filter_mode='pred'),  # 'pred' 'true' or 'either'\n",
    "        CustomConditionalF1Score(threshold=0.5, average='macro', filter_mode='true')\n",
    "    ]\n",
    "\n",
    "test_arrays = [(y_true1, y_pred1), (y_true2, y_pred2), (y_true3, y_pred3), (y_true4, y_pred4), (y_true5, y_pred5), (y_true6, y_pred6), (y_true7, y_pred7), (y_true8, y_pred8), (y_true9, y_pred9), (y_true10, y_pred10)]\n",
    "\n",
    "for metric in metrics_lst:\n",
    "    print(metric)\n",
    "    i = 1\n",
    "    for test in test_arrays:\n",
    "        print(f'Test Array {i}')\n",
    "        i += 1\n",
    "        metric.update_state(test[0], test[1])\n",
    "        print(metric.result().numpy())\n",
    "        metric.reset_states()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonZeroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', name='non_zero_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers non-zero classes.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Total number of classes. Class 0 is assumed to be the \"background\" class.\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNonZeroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "        \n",
    "        # Accumulate counts per class\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the confusion matrix statistics.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, seq_length) with integer class labels.\n",
    "            y_pred: Tensor of shape (batch_size, seq_length, num_classes) with probability distributions.\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        # Convert predictions to class labels using argmax along the last axis.\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Flatten the batch and sequence dimensions.\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_pred = tf.reshape(y_pred, [-1])\n",
    "        \n",
    "        # Compute confusion matrix over all predictions.\n",
    "        cm = tf.math.confusion_matrix(\n",
    "            y_true, y_pred, num_classes=self.num_classes, dtype=tf.float32\n",
    "        )\n",
    "        tp = tf.linalg.diag_part(cm)\n",
    "        fp = tf.reduce_sum(cm, axis=0) - tp\n",
    "        fn = tf.reduce_sum(cm, axis=1) - tp\n",
    "        \n",
    "        # Update state variables.\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "    \n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score for non-zero classes.\n",
    "        \n",
    "        Returns:\n",
    "            F1 score computed over the non-zero classes.\n",
    "        \"\"\"\n",
    "        precision = tf.math.divide_no_nan(\n",
    "            self.true_positives, self.true_positives + self.false_positives\n",
    "        )\n",
    "        recall = tf.math.divide_no_nan(\n",
    "            self.true_positives, self.true_positives + self.false_negatives\n",
    "        )\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        # Exclude class 0 (the background) from the evaluation.\n",
    "        f1_non_zero = f1[1:]\n",
    "        support_non_zero = (self.true_positives + self.false_negatives)[1:]\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            # Weight F1 by the support of each class.\n",
    "            weighted_f1 = tf.reduce_sum(f1_non_zero * support_non_zero) / (tf.reduce_sum(support_non_zero) + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro\n",
    "            return tf.reduce_mean(f1_non_zero)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    ...\n",
    "    tf.print(\"Confusion matrix:\", cm)\n",
    "    tf.print(\"True Positives:\", tp)\n",
    "    tf.print(\"False Positives:\", fp)\n",
    "    tf.print(\"False Negatives:\", fn)\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738565800.403144     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738565800.597694     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738565800.597805     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738565800.602279     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738565800.602337     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738565800.602366     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738565800.868423     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738565800.868499     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-02 23:56:40.868511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1738565800.868565     713 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-02 23:56:40.869899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 (Perfect Predictions) - Non-zero F1: 1.0\n",
      "Test 2 (Imperfect Predictions) - Non-zero F1: 0.6666667\n"
     ]
    }
   ],
   "source": [
    "# Assume CustomNonZeroF1Score is already defined as provided earlier.\n",
    "\n",
    "def test_custom_nonzero_f1():\n",
    "    num_classes = 3  # e.g., class 0 is background, classes 1 and 2 are \"interesting\"\n",
    "    \n",
    "    # Instantiate the metric (using weighted average for non-zero classes)\n",
    "    metric = CustomNonZeroF1Score(num_classes=num_classes, average='weighted')\n",
    "    \n",
    "    ### Test Case 1: Perfect Predictions\n",
    "    # Create a small example where predictions are exactly correct.\n",
    "    # For instance, a single batch with a sequence of 4 values.\n",
    "    # Let y_true be: [0, 1, 2, 1]\n",
    "    y_true = tf.constant([[0, 1, 2, 1]], dtype=tf.int32)\n",
    "    # Create one-hot predictions corresponding exactly to y_true.\n",
    "    y_pred = tf.one_hot(y_true, depth=num_classes)  # shape: (1, 4, 3)\n",
    "    \n",
    "    # Update metric state and get result.\n",
    "    metric.update_state(y_true, y_pred)\n",
    "    result = metric.result().numpy()\n",
    "    print(\"Test 1 (Perfect Predictions) - Non-zero F1:\", result)\n",
    "    # Expected: Since predictions are perfect, F1 for classes 1 and 2 should be 1.\n",
    "    \n",
    "    # Reset metric for the next test.\n",
    "    metric.reset_states()\n",
    "    \n",
    "    ### Test Case 2: Imperfect Predictions\n",
    "    # Construct a small example with some errors.\n",
    "    # For example, consider:\n",
    "    #   y_true: [0, 1, 1, 2, 0]\n",
    "    #   y_pred: [0, 1, 2, 2, 0]\n",
    "    # Here, for class 1: one correct and one misclassification,\n",
    "    #       for class 2: one correct and one false positive (predicted instead of a 1).\n",
    "    y_true = tf.constant([[0, 1, 1, 2, 0]], dtype=tf.int32)\n",
    "    \n",
    "    # Manually build the one-hot predictions.\n",
    "    # For each position, the vector is one-hot for the predicted class.\n",
    "    y_pred = tf.constant([[\n",
    "        [1, 0, 0],  # Correctly predicts class 0.\n",
    "        [0, 1, 0],  # Correctly predicts class 1.\n",
    "        [0, 0, 1],  # Incorrectly predicts class 2 (should be class 1).\n",
    "        [0, 0, 1],  # Correctly predicts class 2.\n",
    "        [1, 0, 0]   # Correctly predicts class 0.\n",
    "    ]], dtype=tf.float32)\n",
    "    \n",
    "    metric.update_state(y_true, y_pred)\n",
    "    result = metric.result().numpy()\n",
    "    print(\"Test 2 (Imperfect Predictions) - Non-zero F1:\", result)\n",
    "    # Expected (manually computed):\n",
    "    #   For class 1: true positives = 1, false negatives = 1, precision=1, recall=0.5, F1 ~ 0.6667.\n",
    "    #   For class 2: true positives = 1, false positives = 1, precision=0.5, recall=1, F1 ~ 0.6667.\n",
    "    # Weighted average F1 = (0.6667*2 + 0.6667*1) / 3 ~ 0.6667.\n",
    "    \n",
    "    # Optionally, reset states again.\n",
    "    metric.reset_states()\n",
    "\n",
    "# Run the test function\n",
    "test_custom_nonzero_f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def custom_binary_crossentropy_loss(\n",
    "    dominant_class_index=0,\n",
    "    dominant_correct_multiplier=0.1,    # reward factor when the dominant class is correct\n",
    "    dominant_incorrect_multiplier=2.0,    # penalty factor when the dominant class is predicted incorrectly\n",
    "    other_class_multiplier=1.0,           # multiplier for non-dominant classes when y_true == 1\n",
    "    smoothing_multiplier=1.0              # multiplier for non-dominant classes when y_true is a smoothed value (0 < y_true < 1)\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a custom binary crossentropy loss function that treats the dominant class specially,\n",
    "    and applies different multipliers for non-dominant classes based on their true label values.\n",
    "    \n",
    "    For the dominant class (specified by dominant_class_index):\n",
    "      - If y_true == 1, the loss is scaled by dominant_correct_multiplier.\n",
    "      - Otherwise, it is scaled by dominant_incorrect_multiplier.\n",
    "    \n",
    "    For non-dominant classes:\n",
    "      - If y_true == 1, the loss is scaled by other_class_multiplier.\n",
    "      - If 0 < y_true < 1 (e.g. label-smoothed values, typically in (0, 0.5]), the loss is scaled by smoothing_multiplier.\n",
    "      - If y_true == 0, no additional scaling is applied.\n",
    "      \n",
    "    Parameters:\n",
    "      dominant_class_index (int): Index of the dominant class in the output vector.\n",
    "      dominant_correct_multiplier (float): Multiplier for the loss when the dominant class is correctly predicted.\n",
    "      dominant_incorrect_multiplier (float): Multiplier for the loss when the dominant class is incorrectly predicted.\n",
    "      other_class_multiplier (float): Multiplier for non-dominant classes when the true label is 1.\n",
    "      smoothing_multiplier (float): Multiplier for non-dominant classes when the true label is a smoothed value (0 < y_true < 1).\n",
    "      \n",
    "    Returns:\n",
    "      A callable loss function usable with model.compile(loss=...).\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # Prevent issues with log(0)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Compute standard element-wise binary crossentropy.\n",
    "        base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "                       (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "        # Determine the number of classes.\n",
    "        num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Create a one-hot mask for the dominant class.\n",
    "        dominant_mask = tf.one_hot(dominant_class_index, depth=num_classes, dtype=y_true.dtype)\n",
    "        # The complement selects all non-dominant columns.\n",
    "        non_dominant_mask = 1 - dominant_mask\n",
    "        \n",
    "        # --- Dominant Class Weighting ---\n",
    "        # For the dominant class: if y_true == 1 use dominant_correct_multiplier; otherwise use dominant_incorrect_multiplier.\n",
    "        dominant_true = y_true[:, dominant_class_index]  # Shape: (batch_size,)\n",
    "        dominant_weight = tf.where(tf.equal(dominant_true, 1.0),\n",
    "                                   dominant_correct_multiplier,\n",
    "                                   dominant_incorrect_multiplier)  # Shape: (batch_size,)\n",
    "        dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        # --- Non-Dominant Class Weighting ---\n",
    "        # For non-dominant classes, apply:\n",
    "        #   - other_class_multiplier if y_true == 1\n",
    "        #   - smoothing_multiplier if 0 < y_true < 1 (i.e. a smoothed value)\n",
    "        #   - otherwise (y_true == 0) leave as 1.\n",
    "        non_dominant_weight = tf.where(\n",
    "            tf.equal(y_true, 1.0),\n",
    "            other_class_multiplier,\n",
    "            tf.where(tf.greater(y_true, 0.0),\n",
    "                     smoothing_multiplier,\n",
    "                     1.0)\n",
    "        )\n",
    "        \n",
    "        # Combine the weights for each class.\n",
    "        weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "        # Compute and return the weighted loss.\n",
    "        weighted_loss = base_loss * weights\n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the loss with desired multipliers.\n",
    "loss_fn = custom_binary_crossentropy_loss(\n",
    "    dominant_class_index=0,\n",
    "    dominant_correct_multiplier=0.1,    # barely reward a correct dominant guess\n",
    "    dominant_incorrect_multiplier=2.0,    # strongly penalize an incorrect dominant guess\n",
    "    other_class_multiplier=1.0,           # multiplier when a non-dominant class's y_true is 1\n",
    "    smoothing_multiplier=0.8              # multiplier for non-dominant classes with smoothed values (0 < y_true < 1)\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "class CustomNonZeroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', threshold=0.5, name='non_zero_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers non-dominant classes (ignoring index 0).\n",
    "        \n",
    "        This version is designed for multi-encoded labels where:\n",
    "          - The dominant class (index 0) is represented as a hard label [1, 0, 0, ...]\n",
    "          - For non-dominant classes (indices 1 to num_classes-1), only an exact label of 1 is considered positive.\n",
    "            (Any partial credit/smoothed values below 1 are treated as 0.)\n",
    "          - Predictions are thresholded (default threshold = 0.5) to decide 1 vs. 0.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNonZeroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "\n",
    "        # Create state variables to accumulate counts for each class.\n",
    "        # We use a vector of length num_classes but we will update only indices 1...num_classes-1.\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the metric state.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, num_classes). These are multi-encoded labels.\n",
    "                    For non-dominant classes, a label is considered positive only if it is exactly 1.\n",
    "            y_pred: Tensor of shape (batch_size, num_classes) with predictions (e.g. probabilities).\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        # We want to ignore the dominant class (index 0) and work on classes 1...num_classes-1.\n",
    "        # Assume y_true and y_pred are both of shape (batch_size, num_classes).\n",
    "        y_true_non_dominant = y_true[:, 1:]\n",
    "        y_pred_non_dominant = y_pred[:, 1:]\n",
    "        \n",
    "        # For ground truth: treat a class as positive only if its value is exactly 1.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_non_dominant, 1.0), tf.int32)\n",
    "        # For predictions: apply thresholding.\n",
    "        y_pred_bin = tf.cast(y_pred_non_dominant >= self.threshold, tf.int32)\n",
    "        \n",
    "        # (Optionally) apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.int32)\n",
    "            sample_weight = tf.reshape(sample_weight, (-1, 1))\n",
    "            y_true_bin = y_true_bin * sample_weight\n",
    "            y_pred_bin = y_pred_bin * sample_weight\n",
    "        \n",
    "        # Compute per-class true positives, false positives, and false negatives for non-dominant classes.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_bin) * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        \n",
    "        # Our state variables have length num_classes. We want to update only indices 1... with our computed values.\n",
    "        zeros = tf.zeros([1], dtype=tf.float32)\n",
    "        tp_update = tf.concat([zeros, tp], axis=0)\n",
    "        fp_update = tf.concat([zeros, fp], axis=0)\n",
    "        fn_update = tf.concat([zeros, fn], axis=0)\n",
    "        \n",
    "        self.true_positives.assign_add(tp_update)\n",
    "        self.false_positives.assign_add(fp_update)\n",
    "        self.false_negatives.assign_add(fn_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score over the non-dominant classes (indices 1...num_classes-1).\n",
    "        \"\"\"\n",
    "        # Select non-dominant classes only.\n",
    "        tp = self.true_positives[1:]\n",
    "        fp = self.false_positives[1:]\n",
    "        fn = self.false_negatives[1:]\n",
    "        \n",
    "        precision = tf.math.divide_no_nan(tp, tp + fp)\n",
    "        recall = tf.math.divide_no_nan(tp, tp + fn)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            support = tp + fn\n",
    "            weighted_f1 = tf.reduce_sum(f1 * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro\n",
    "            return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='your_loss_function',  # e.g., your custom loss\n",
    "              metrics=[CustomNonZeroF1Score(num_classes=5, average='weighted', threshold=0.5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Non-Zero F1 Score (weighted average): 0.8\n",
      "Custom Non-Zero F1 Score (macro average): 0.875\n",
      "Custom Non-Zero F1 Score after reset (weighted average): 0.75\n"
     ]
    }
   ],
   "source": [
    "class CustomNonZeroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', threshold=0.5, name='non_zero_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers non-dominant classes (ignoring index 0).\n",
    "        \n",
    "        This version is designed for multi-encoded labels where:\n",
    "          - The dominant class (index 0) is represented as a hard label (e.g. [1, 0, 0, ...]).\n",
    "          - For non-dominant classes (indices 1 to num_classes-1), only an exact label of 1 is considered positive.\n",
    "            (Any partial credit/smoothed values below 1 are treated as 0.)\n",
    "          - Predictions are thresholded (default threshold = 0.5) to decide a positive.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNonZeroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "\n",
    "        # State variables: accumulate true positives, false positives, and false negatives per class.\n",
    "        # We will update only indices 1...num_classes-1.\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the metric state.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, num_classes) with multi-encoded labels.\n",
    "                    For non-dominant classes, a label is considered positive only if it is exactly 1.\n",
    "            y_pred: Tensor of shape (batch_size, num_classes) with predicted scores/probabilities.\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        # We want to ignore the dominant class (index 0); work only on indices 1...num_classes-1.\n",
    "        y_true_non_dominant = y_true[:, 1:]\n",
    "        y_pred_non_dominant = y_pred[:, 1:]\n",
    "        \n",
    "        # For ground truth: treat a class as positive only if its value is exactly 1.\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_non_dominant, 1.0), tf.int32)\n",
    "        # For predictions: threshold to decide positive.\n",
    "        y_pred_bin = tf.cast(y_pred_non_dominant >= self.threshold, tf.int32)\n",
    "        \n",
    "        # Optionally, apply sample weights (if provided).\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.int32)\n",
    "            sample_weight = tf.reshape(sample_weight, (-1, 1))\n",
    "            y_true_bin = y_true_bin * sample_weight\n",
    "            y_pred_bin = y_pred_bin * sample_weight\n",
    "        \n",
    "        # Compute per-class true positives, false positives, and false negatives for non-dominant classes.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_bin) * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        \n",
    "        # Our state variables are of shape (num_classes,).\n",
    "        # Since we are ignoring the dominant class (index 0), prepend zeros to match the full shape.\n",
    "        zeros = tf.zeros([1], dtype=tf.float32)\n",
    "        tp_update = tf.concat([zeros, tp], axis=0)\n",
    "        fp_update = tf.concat([zeros, fp], axis=0)\n",
    "        fn_update = tf.concat([zeros, fn], axis=0)\n",
    "        \n",
    "        self.true_positives.assign_add(tp_update)\n",
    "        self.false_positives.assign_add(fp_update)\n",
    "        self.false_negatives.assign_add(fn_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score over the non-dominant classes (indices 1 to num_classes-1).\n",
    "        \"\"\"\n",
    "        # Use only non-dominant classes.\n",
    "        tp = self.true_positives[1:]\n",
    "        fp = self.false_positives[1:]\n",
    "        fn = self.false_negatives[1:]\n",
    "        \n",
    "        precision = tf.math.divide_no_nan(tp, tp + fp)\n",
    "        recall = tf.math.divide_no_nan(tp, tp + fn)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            support = tp + fn\n",
    "            weighted_f1 = tf.reduce_sum(f1 * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro average\n",
    "            return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))\n",
    "\n",
    "# --- Testing the Custom Metric ---\n",
    "\n",
    "def test_custom_non_zero_f1():\n",
    "    num_classes = 5\n",
    "    # Create an instance of the metric (we'll test both weighted and macro averaging).\n",
    "    metric_weighted = CustomNonZeroF1Score(num_classes=num_classes, average='weighted', threshold=0.5)\n",
    "    metric_macro = CustomNonZeroF1Score(num_classes=num_classes, average='macro', threshold=0.5)\n",
    "    \n",
    "    # Create some synthetic data.\n",
    "    # Each row is a multi-encoded label vector.\n",
    "    # The dominant class (index 0) is a hard label (always 1 if active).\n",
    "    # For non-dominant classes, only an exact 1 is treated as positive.\n",
    "    # Here are 3 samples:\n",
    "    #\n",
    "    # Sample 1:\n",
    "    #   y_true: [1, 1, 0, 0, 0]\n",
    "    #   y_pred: [0.9, 0.8, 0.3, 0.2, 0.1] -> after threshold (non-dominant): [1, 0, 0, 0]\n",
    "    #\n",
    "    # Sample 2:\n",
    "    #   y_true: [1, 0, 1, 0, 1]\n",
    "    #   y_pred: [0.95, 0.4, 0.6, 0.2, 0.7] -> after threshold (non-dominant): [0, 1, 0, 1]\n",
    "    #\n",
    "    # Sample 3:\n",
    "    #   y_true: [1, 0, 0, 1, 0]\n",
    "    #   y_pred: [0.8, 0.3, 0.4, 0.55, 0.2] -> after threshold (non-dominant): [0, 0, 1, 0]\n",
    "    \n",
    "    y_true = np.array([\n",
    "        [1, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 1, 1],\n",
    "        [1, 0, 0, 1, 0]\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    # y_pred = np.array([\n",
    "    #     [0.9, 0.8, 0.3, 0.6, 0.1],\n",
    "    #     [0.95, 0.4, 0.6, 0.2, 0.7],\n",
    "    #     [0.8, 0.3, 0.4, 0.55, 0.2]\n",
    "    # ], dtype=np.float32)\n",
    "    \n",
    "    y_pred = np.array([\n",
    "        [1, 1, 0, 1, 0],\n",
    "        [1, 0, 1, 0, 1],\n",
    "        [1, 0, 0, 1, 0]\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    # Update the metric state with our batch.\n",
    "    metric_weighted.update_state(y_true, y_pred)\n",
    "    metric_macro.update_state(y_true, y_pred)\n",
    "    \n",
    "    # Get the results.\n",
    "    f1_weighted = metric_weighted.result().numpy()\n",
    "    f1_macro = metric_macro.result().numpy()\n",
    "    \n",
    "    print(\"Custom Non-Zero F1 Score (weighted average):\", f1_weighted)\n",
    "    print(\"Custom Non-Zero F1 Score (macro average):\", f1_macro)\n",
    "    \n",
    "    # Optionally, reset the states and test with a new batch.\n",
    "    metric_weighted.reset_states()\n",
    "    # New synthetic data with 2 samples.\n",
    "    y_true2 = np.array([\n",
    "        [1, 0, 1, 0, 1],\n",
    "        [1, 1, 0, 1, 0]\n",
    "    ], dtype=np.float32)\n",
    "    # y_pred2 = np.array([\n",
    "    #     [0.99, 0.2, 0.8, 0.6, 0.9],\n",
    "    #     [0.9, 0.7, 0.1, 0.2, 0.1]\n",
    "    # ], dtype=np.float32)\n",
    "    y_pred2 = np.array([\n",
    "        [1, 0, 1, 1, 1],\n",
    "        [1, 1, 0, 0, 0]\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    metric_weighted.update_state(y_true2, y_pred2)\n",
    "    f1_weighted_new = metric_weighted.result().numpy()\n",
    "    print(\"Custom Non-Zero F1 Score after reset (weighted average):\", f1_weighted_new)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_custom_non_zero_f1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth (non-dominant classes):\n",
      "[[0 1 0 1]\n",
      " [1 0 1 0]]\n",
      "Predictions (non-dominant classes):\n",
      "[[0 1 1 1]\n",
      " [1 0 0 0]]\n",
      "Sklearn F1 Score (macro average): 0.75\n",
      "Sklearn F1 Score (weighted average): 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Synthetic test data (each row is one sample).\n",
    "# The dominant class is in column 0 (this column will be ignored).\n",
    "# For non-dominant classes, only an exact 1 is treated as positive.\n",
    "# y_true = np.array([\n",
    "#     [1, 1, 0, 0, 0],  # Sample 1\n",
    "#     [1, 0, 1, 1, 1],  # Sample 2\n",
    "#     [1, 0, 0, 1, 0]   # Sample 3\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# # Predicted probabilities for each class.\n",
    "# y_pred = np.array([\n",
    "#     [0.9, 0.8, 0.3, 0.9, 0.1],  # Sample 1 predictions\n",
    "#     [0.95, 0.4, 0.6, 0.2, 0.7],  # Sample 2 predictions\n",
    "#     [0.8, 0.3, 0.4, 0.55, 0.2]   # Sample 3 predictions\n",
    "# ], dtype=np.float32)\n",
    "y_true = np.array([\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [1, 1, 0, 1, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Predicted probabilities for each class.\n",
    "y_pred = np.array([\n",
    "    [1, 0, 1, 1, 1],\n",
    "    [1, 1, 0, 0, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Define the threshold for converting probabilities to binary predictions.\n",
    "threshold = 0.5\n",
    "\n",
    "# Since the dominant class is at index 0 and should be ignored, work only on columns 1 to end.\n",
    "y_true_non_dom = y_true[:, 1:]\n",
    "y_pred_non_dom = y_pred[:, 1:]\n",
    "\n",
    "# Binarize the ground truth: Only a value exactly equal to 1 counts as positive.\n",
    "y_true_bin = (y_true_non_dom == 1).astype(int)\n",
    "\n",
    "# Binarize the predictions using the threshold.\n",
    "y_pred_bin = (y_pred_non_dom >= threshold).astype(int)\n",
    "\n",
    "print(\"Ground truth (non-dominant classes):\")\n",
    "print(y_true_bin)\n",
    "print(\"Predictions (non-dominant classes):\")\n",
    "print(y_pred_bin)\n",
    "\n",
    "# Compute the F1 score using scikit-learn.\n",
    "# Use 'macro' to simply average the F1 scores of each class,\n",
    "# or 'weighted' to weight them by the support (number of true instances).\n",
    "f1_macro = f1_score(y_true_bin, y_pred_bin, average='macro')\n",
    "f1_weighted = f1_score(y_true_bin, y_pred_bin, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Sklearn F1 Score (macro average):\", f1_macro)\n",
    "print(\"Sklearn F1 Score (weighted average):\", f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
