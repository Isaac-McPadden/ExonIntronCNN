{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "Complete Binary, w/ background - sizes 1 and 10 and highly mixed 5\n",
    "Complete Binary, no background - sizes 1 and 10 and incremental 5\n",
    "Custom Smoothing, w/ background - sizes 1 and 10 - This is the baseline\n",
    "Custom Smoothing, no background - sizes 1 and 10\n",
    "\n",
    "\n",
    "Loss settings:\n",
    "Custom Smoothing, enabled - Loss A -This is the baseline\n",
    "Custom Smoothing, disabled - Loss A\n",
    "No Smoothing, disabled - Loss A or B \n",
    "Proper Smoothing, disabled - Loss B\n",
    "\n",
    "\n",
    "Model Types: \n",
    "Standard Dilation, No Attention - This is the baseline\n",
    "Standard Dilation, Attention\n",
    "Reduced Dilation, No Attention\n",
    "Reduced Dilation, Attention\n",
    "\n",
    "Todo: Need to make a reduced dilation version of the model - done, model takes dilation scaler\n",
    "Confirm that if no smoothing and early reward disabled, Loss A and Loss B are equivalent - confirmed, so long as parameters and thresholds are the same\n",
    "\n",
    "Checklist of datasets:\n",
    "\n",
    " Complete Binary, w/ background\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10\n",
    "\n",
    " Highly mixed 5\n",
    "\n",
    "\n",
    " Complete Binary, no background\n",
    "\n",
    " Size 1\n",
    "\n",
    " Size 10\n",
    "\n",
    " Incremental 5 - have a custom smoothed version w/ background that can be converted\n",
    "\n",
    " Custom Smoothing, w/ background (This is the baseline)\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Custom Smoothing, no background\n",
    "\n",
    " Size 1\n",
    "\n",
    " Size 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 00:31:57.361050: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-08 00:31:57.452228: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-08 00:31:57.478923: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-08 00:31:57.644693: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-08 00:31:59.193106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import IEModules as iem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered Howdy! into test_function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Howdy!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.test_function(\"Howdy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "_bytes_feature\n",
      "_float_feature\n",
      "_int64_feature\n",
      "build_chunk_data_for_indices\n",
      "build_dataset_from_tfrecords\n",
      "bytes_feature\n",
      "calculate_introns\n",
      "compute_chunk_indices\n",
      "convert_and_write_tfrecord\n",
      "convert_labels_to_binary\n",
      "float_feature_list\n",
      "int_feature_list\n",
      "label_sequence_local\n",
      "load_gtf_annotations\n",
      "main\n",
      "one_hot_encode_reference\n",
      "pad_encoded_seq\n",
      "pad_labels\n",
      "parse_chunk_example\n",
      "search_gtf_by_range\n",
      "serialize_chunk_example\n",
      "serialize_example_with_metadata_no_convert\n",
      "split_tfrecords\n",
      "stream_shuffled_records\n",
      "swap_columns_if_needed\n",
      "test_dataset_from_tfrecords\n",
      "test_function\n",
      "trim_chr_genome\n",
      "tvt_split_tfrecords\n",
      "write_shuffled_records_to_single_tfrecord\n",
      "write_tfrecord_in_shards_hybrid\n",
      "write_to_shard_with_threads\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "functions = inspect.getmembers(iem.Genetic_Data_Pipeline, inspect.isfunction)\n",
    "for name, func in functions:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Genetic_Data_Pipeline for correctness\n",
    "\n",
    "Check Custom_Models for correctness\n",
    "\n",
    "Need to make a shard shuffler module probably that goes from output of genetic_data_pipeline to fully shuffled test-train-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          1. build_initial_shards function generates 4 tfrecord shards at each shift fed to it.\n",
      "          I recommend giving a list of 1 window shift.  Currently paths are hardcoded.  \n",
      "          The first 4*n characters of the shards will be the shift numbers where n is \n",
      "          the number of shifts fed to the function.\n",
      "          \n",
      "          2. split_tfrecords takes an input and output directory and number of file splits\n",
      "          and splits each TFRecord in the input_directory (assumed to be gzipped TFRecords)\n",
      "          into 'num_splits' smaller TFRecords.\n",
      "          \n",
      "          3. write_shuffled_records_to_single_tfrecord builds a single big, shuffled, \n",
      "          gzip-compressed TFRecord file \n",
      "          \n",
      "          Args:\n",
      "            input_dir (str): Directory containing the source TFRecord files.\n",
      "            allowed_indices (list): List of allowed starting indices.\n",
      "            output_filepath (str): Full path to the output TFRecord file.\n",
      "            \n",
      "          4. tvt_split_records test, validate, train splits the big tfrecord into \n",
      "          test, validate, and train datasets\n",
      "          \n",
      "          5. convert_and_write_tfrecord writes a binary only version of the dataset fed to it\n",
      "          \n",
      "          6. Removing background happens when loading the data into the model\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.dataset_pipeline_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist of datasets:\n",
    "\n",
    " Complete Binary, w/ background\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Highly mixed 5                                 Need to build highly mixed 5 and convert to binary\n",
    "\n",
    "\n",
    " Complete Binary, no background\n",
    "\n",
    " Size 1 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Size 10 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Incremental 5 - have a custom smoothed version w/ background that can be converted   conversion ongoing\n",
    "\n",
    " Custom Smoothing, w/ background (This is the baseline)\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Custom Smoothing, no background\n",
    "\n",
    " Size 1 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Size 10 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    "\n",
    "\n",
    " Size 10 has 2365261 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744095490.509284 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.699680 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.699799 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704425 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704488 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704525 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.994540 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.994740 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-08 00:58:10.994829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744095490.995069 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-08 00:58:10.995990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-08 09:35:11.668060: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/all_ten_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_ten_shuffled.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/fifths_incremental_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_fifths_incrememntal.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iem.Genetic_Data_Pipeline.transform_tfdataset(datasets_path+\"AugDataSets/AllTen\", datasets_path+\"AugDataSets\", 45, 0.5)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
