{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "Complete Binary, w/ background - sizes 1 and 10 and highly mixed 5\n",
    "Complete Binary, no background - sizes 1 and 10 and incremental 5\n",
    "Custom Smoothing, w/ background - sizes 1 and 10 - This is the baseline\n",
    "Custom Smoothing, no background - sizes 1 and 10\n",
    "\n",
    "\n",
    "Loss settings:\n",
    "Custom Smoothing, enabled - Loss A -This is the baseline\n",
    "Custom Smoothing, disabled - Loss A\n",
    "No Smoothing, disabled - Loss A or B \n",
    "Proper Smoothing, disabled - Loss B\n",
    "\n",
    "\n",
    "Model Types: \n",
    "Standard Dilation, No Attention - This is the baseline\n",
    "Standard Dilation, Attention\n",
    "Reduced Dilation, No Attention\n",
    "Reduced Dilation, Attention\n",
    "\n",
    "Todo: Need to make a reduced dilation version of the model - done, model takes dilation scaler\n",
    "Confirm that if no smoothing and early reward disabled, Loss A and Loss B are equivalent - confirmed, so long as parameters and thresholds are the same\n",
    "\n",
    "Checklist of datasets:\n",
    "\n",
    " Complete Binary, w/ background\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10\n",
    "\n",
    " Highly mixed 5\n",
    "\n",
    "\n",
    " Complete Binary, no background\n",
    "\n",
    " Size 1\n",
    "\n",
    " Size 10\n",
    "\n",
    " Incremental 5 - have a custom smoothed version w/ background that can be converted\n",
    "\n",
    " Custom Smoothing, w/ background (This is the baseline)\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Custom Smoothing, no background\n",
    "\n",
    " Size 1\n",
    "\n",
    " Size 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 21:51:14.932147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-12 21:51:15.028668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-12 21:51:15.056663: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-12 21:51:15.232283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-12 21:51:16.827781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IEModules as iem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered Howdy! into test_function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Howdy!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.test_function(\"Howdy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "_bytes_feature\n",
      "_float_feature\n",
      "_int64_feature\n",
      "build_chunk_data_for_indices\n",
      "build_dataset_from_tfrecords\n",
      "bytes_feature\n",
      "calculate_introns\n",
      "compute_chunk_indices\n",
      "convert_and_write_tfrecord\n",
      "convert_labels_to_binary\n",
      "float_feature_list\n",
      "int_feature_list\n",
      "label_sequence_local\n",
      "load_gtf_annotations\n",
      "main\n",
      "one_hot_encode_reference\n",
      "pad_encoded_seq\n",
      "pad_labels\n",
      "parse_chunk_example\n",
      "search_gtf_by_range\n",
      "serialize_chunk_example\n",
      "serialize_example_with_metadata_no_convert\n",
      "split_tfrecords\n",
      "stream_shuffled_records\n",
      "swap_columns_if_needed\n",
      "test_dataset_from_tfrecords\n",
      "test_function\n",
      "trim_chr_genome\n",
      "tvt_split_tfrecords\n",
      "write_shuffled_records_to_single_tfrecord\n",
      "write_tfrecord_in_shards_hybrid\n",
      "write_to_shard_with_threads\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "functions = inspect.getmembers(iem.Genetic_Data_Pipeline, inspect.isfunction)\n",
    "for name, func in functions:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Genetic_Data_Pipeline for correctness\n",
    "\n",
    "Check Custom_Models for correctness\n",
    "\n",
    "Need to make a shard shuffler module probably that goes from output of genetic_data_pipeline to fully shuffled test-train-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          1. build_initial_shards function generates 4 tfrecord shards at each shift fed to it.\n",
      "          I recommend giving a list of 1 window shift.  Currently paths are hardcoded.  \n",
      "          The first 4*n characters of the shards will be the shift numbers where n is \n",
      "          the number of shifts fed to the function.\n",
      "          \n",
      "          2. split_tfrecords takes an input and output directory and number of file splits\n",
      "          and splits each TFRecord in the input_directory (assumed to be gzipped TFRecords)\n",
      "          into 'num_splits' smaller TFRecords.\n",
      "          \n",
      "          3. write_shuffled_records_to_single_tfrecord builds a single big, shuffled, \n",
      "          gzip-compressed TFRecord file \n",
      "          \n",
      "          Args:\n",
      "            input_dir (str): Directory containing the source TFRecord files.\n",
      "            allowed_indices (list): List of allowed starting indices.\n",
      "            output_filepath (str): Full path to the output TFRecord file.\n",
      "            \n",
      "          4. tvt_split_records test, validate, train splits the big tfrecord into \n",
      "          test, validate, and train datasets\n",
      "          \n",
      "          5. convert_and_write_tfrecord writes a binary only version of the dataset fed to it\n",
      "          \n",
      "          6. Removing background happens when loading the data into the model\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.dataset_pipeline_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist of datasets:\n",
    "\n",
    " Complete Binary, w/ background\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Highly mixed 5                                 Need to build highly mixed 5 and convert to binary\n",
    "\n",
    "\n",
    " Complete Binary, no background\n",
    "\n",
    " Size 1 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Size 10 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Incremental 5 - have a custom smoothed version w/ background that can be converted   conversion ongoing\n",
    "\n",
    " Custom Smoothing, w/ background (This is the baseline)\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Custom Smoothing, no background\n",
    "\n",
    " Size 1 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Size 10 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    "\n",
    "\n",
    " Size 10 has 2365261 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744095490.509284 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.699680 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.699799 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704425 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704488 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704525 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.994540 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.994740 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-08 00:58:10.994829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744095490.995069 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-08 00:58:10.995990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-08 09:35:11.668060: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/all_ten_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_ten_shuffled.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/fifths_incremental_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_fifths_incrememntal.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder created at: ../../Datasets/AugDataSets/AllTen/temp_shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744246104.885834  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.075605  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.075745  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078746  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078828  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078872  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.373822  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.373927  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-09 18:48:25.373940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744246105.374015  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-09 18:48:25.374434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-09 19:01:38.755912: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records found in input dataset: 2365261\n",
      "Sampling 1182630 records (fraction = 0.5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 19:14:36.772563: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split all_ten_shuffled.tfrecord.gz into 45 shards.\n",
      "10000 records written so far...\n",
      "20000 records written so far...\n",
      "30000 records written so far...\n",
      "40000 records written so far...\n",
      "50000 records written so far...\n",
      "60000 records written so far...\n",
      "70000 records written so far...\n",
      "80000 records written so far...\n",
      "90000 records written so far...\n",
      "100000 records written so far...\n",
      "110000 records written so far...\n",
      "120000 records written so far...\n",
      "130000 records written so far...\n",
      "140000 records written so far...\n",
      "150000 records written so far...\n",
      "160000 records written so far...\n",
      "170000 records written so far...\n",
      "180000 records written so far...\n",
      "190000 records written so far...\n",
      "200000 records written so far...\n",
      "210000 records written so far...\n",
      "220000 records written so far...\n",
      "230000 records written so far...\n",
      "240000 records written so far...\n",
      "250000 records written so far...\n",
      "260000 records written so far...\n",
      "270000 records written so far...\n",
      "280000 records written so far...\n",
      "290000 records written so far...\n",
      "300000 records written so far...\n",
      "310000 records written so far...\n",
      "320000 records written so far...\n",
      "330000 records written so far...\n",
      "340000 records written so far...\n",
      "350000 records written so far...\n",
      "360000 records written so far...\n",
      "370000 records written so far...\n",
      "380000 records written so far...\n",
      "390000 records written so far...\n",
      "400000 records written so far...\n",
      "410000 records written so far...\n",
      "420000 records written so far...\n",
      "430000 records written so far...\n",
      "440000 records written so far...\n",
      "450000 records written so far...\n",
      "460000 records written so far...\n",
      "470000 records written so far...\n",
      "480000 records written so far...\n",
      "490000 records written so far...\n",
      "500000 records written so far...\n",
      "510000 records written so far...\n",
      "520000 records written so far...\n",
      "530000 records written so far...\n",
      "540000 records written so far...\n",
      "550000 records written so far...\n",
      "560000 records written so far...\n",
      "570000 records written so far...\n",
      "580000 records written so far...\n",
      "590000 records written so far...\n",
      "600000 records written so far...\n",
      "610000 records written so far...\n",
      "620000 records written so far...\n",
      "630000 records written so far...\n",
      "640000 records written so far...\n",
      "650000 records written so far...\n",
      "660000 records written so far...\n",
      "670000 records written so far...\n",
      "680000 records written so far...\n",
      "690000 records written so far...\n",
      "700000 records written so far...\n",
      "710000 records written so far...\n",
      "720000 records written so far...\n",
      "730000 records written so far...\n",
      "740000 records written so far...\n",
      "750000 records written so far...\n",
      "760000 records written so far...\n",
      "770000 records written so far...\n",
      "780000 records written so far...\n",
      "790000 records written so far...\n",
      "800000 records written so far...\n",
      "810000 records written so far...\n",
      "820000 records written so far...\n",
      "830000 records written so far...\n",
      "840000 records written so far...\n",
      "850000 records written so far...\n",
      "860000 records written so far...\n",
      "870000 records written so far...\n",
      "880000 records written so far...\n",
      "890000 records written so far...\n",
      "900000 records written so far...\n",
      "910000 records written so far...\n",
      "920000 records written so far...\n",
      "930000 records written so far...\n",
      "940000 records written so far...\n",
      "950000 records written so far...\n",
      "960000 records written so far...\n",
      "970000 records written so far...\n",
      "980000 records written so far...\n",
      "990000 records written so far...\n",
      "1000000 records written so far...\n",
      "1010000 records written so far...\n",
      "1020000 records written so far...\n",
      "1030000 records written so far...\n",
      "1040000 records written so far...\n",
      "1050000 records written so far...\n",
      "1060000 records written so far...\n",
      "1070000 records written so far...\n",
      "1080000 records written so far...\n",
      "1090000 records written so far...\n",
      "1100000 records written so far...\n",
      "1110000 records written so far...\n",
      "1120000 records written so far...\n",
      "1130000 records written so far...\n",
      "1140000 records written so far...\n",
      "1150000 records written so far...\n",
      "1160000 records written so far...\n",
      "1170000 records written so far...\n",
      "1180000 records written so far...\n",
      "Finished writing 1182630 records to ../../Datasets/AugDataSets/50_all_ten_shuffled.tfrecord.gz\n",
      "Temporary shards have been removed.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.transform_tfdataset(datasets_path+\"AugDataSets/AllTen\", datasets_path+\"AugDataSets\", 45, 0.5)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 04:28:15.601091: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/50_all_ten_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_5x_mixed.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset New Naming Scheme: \n",
    "Epoch Units: 01-10, Style: M for Mixed or I for Incremental, Smoothing: C for custom smoothing, B for Binary\n",
    "Epoch-Units_Style_Smoothing_list-of-shifts-if-incremental-divided-by-1000_IEData.tfrecord.gz\n",
    "ex: 02_I_B_0-2.5_IEData.tfrecord.gz\n",
    "\n",
    "Per line, this is input from trial csv:\n",
    "Trial,Smoothing,Background,Early Rewarding,Size,Style,Dilation,Attention\n",
    "\n",
    "Trial becomes folder where model data are saved\n",
    "\n",
    "Smoothing, Size, Style define the dataset\n",
    "\n",
    "Background is a choice handled in the data loader which returns a dataset generator pretending to be a dataset (Data_Functions.prep_dataset_from_tfrecord())\n",
    "\n",
    "Early rewarding: if true, need an option in the loss functions or something that checks epoch number.  Rewarding==True for more than 3 epoch units is overkill so the switch should flip after 3 epoch units\n",
    "\n",
    "Dilation and Attention are model hyperparameters\n",
    "\n",
    "What I need then:\n",
    "Experiment handler function (class?) that reads a line from the csv and trains for 10 epoch units.  Needs a working folder given to it (Experiment Name is probably the folder name) and a subfolder for each trial (Trial_01, etc).  Checkpoints get saved in there, as does the model history which gets saved to json every epoch unit but overwritten, as should the StatefulReduceLROnPlateau callback state that I have yet to implement.\n",
    "The experiment should track which trial and which epoch was most recently saved and be able to resume from that trial and checkpoint easily.  Maybe have it save a jpg of the train validate curve at the end of the trial using the function in Helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744516349.562100 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.758718 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.758835 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762785 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762879 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762919 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516350.064039 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516350.064137 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-12 21:52:30.064148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744516350.064212 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-12 21:52:30.065583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-12 21:58:43.898655: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records found: 1182630\n",
      "Splitting into -> Train: 946104, Val: 118263, Test: 118263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 21:58:53.921334: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:6: Filling up shuffle buffer (this may take a while): 22613 of 25000\n",
      "2025-04-12 21:58:55.008046: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "2025-04-12 22:51:29.014481: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Split Counts -> Train: 946104, Val: 118263, Test: 118263\n",
      "05_M_B_IEData.tfrecord.gz split\n",
      "Total records found: 2365261\n",
      "Splitting into -> Train: 1892208, Val: 236526, Test: 236527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 23:04:33.559500: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:15: Filling up shuffle buffer (this may take a while): 20358 of 25000\n",
      "2025-04-12 23:04:35.240383: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Split Counts -> Train: 1892208, Val: 236526, Test: 236527\n",
      "10_M_C_IEData.tfrecord.gz split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:55:15.264840: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "data_directory = datasets_path+\"Experiment 01/\"\n",
    "save_directory = data_directory+\"TestValTrain/\"\n",
    "for root, dirs, files in os.walk(data_directory):\n",
    "    if 'TestValTrain' in dirs:\n",
    "        dirs.remove('TestValTrain')\n",
    "    for file in files:\n",
    "        file_path = data_directory+os.path.basename(file)\n",
    "        train_save_file_path = save_directory+\"Train_\"+os.path.basename(file)\n",
    "        val_save_file_path = save_directory+\"Val_\"+os.path.basename(file)\n",
    "        test_save_file_path = save_directory+\"Test_\"+os.path.basename(file)\n",
    "        iem.Genetic_Data_Pipeline.tvt_split_tfrecords(file_path, train_save_file_path, val_save_file_path,test_save_file_path)\n",
    "        print(os.path.basename(file)+\" split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Experiment Handler Module\n",
    "\n",
    "This module defines an ExperimentHandler class that:\n",
    "  • Reads a single trial’s configuration (with keys: Trial, Smoothing, Background,\n",
    "    Early Rewarding, Size, Style, Dilation, Attention) from a CSV file.\n",
    "  • Creates an experiment folder under a common experiment root for each trial.\n",
    "  • Loads a dataset from a tfrecord using a constructed filename based on the configuration.\n",
    "  • Builds a model via a provided model builder or a default placeholder.\n",
    "  • Trains the model for 10 \"epoch units\" (one epoch per unit in this example).\n",
    "  • Saves checkpoints, training history (using Helper_Functions.save_history_to_json),\n",
    "    and a train/validation curve plot (using Helper_Functions.plot_train_val_curve) after each epoch unit.\n",
    "  • Supports resuming progress from a saved state.\n",
    "\"\"\"\n",
    "\n",
    "import IEModules as iem\n",
    "\n",
    "# Optionally import custom callbacks from Custom_Callbacks if available\n",
    "# from Custom_Callbacks import StatefulReduceLROnPlateau  # placeholder for stateful LR scheduling\n",
    "\n",
    "class ExperimentHandler:\n",
    "    def __init__(self, trial_config, experiment_root=\"ExperimentResults\", model_builder=None):\n",
    "        \"\"\"\n",
    "        Initialize experiment for a single trial.\n",
    "        \n",
    "        Parameters:\n",
    "          trial_config (dict): Dictionary with keys:\n",
    "             \"Trial\", \"Smoothing\", \"Background\", \"Early Rewarding\", \"Size\", \"Style\", \"Dilation\", \"Attention\"\n",
    "          experiment_root (str): The root directory for experiment results.\n",
    "          model_builder (callable, optional): Function that builds and compiles the model.\n",
    "               Defaults to a simple placeholder model.\n",
    "        \"\"\"\n",
    "        self.trial_config = trial_config\n",
    "        self.total_epoch_units = 10\n",
    "        self.current_epoch_unit = 0\n",
    "        self.model = None\n",
    "        self.model_builder = model_builder if model_builder is not None else self.default_model_builder\n",
    "\n",
    "        # Create a subfolder for the trial using the \"Trial\" field under the experiment root.\n",
    "        self.trial_folder = os.path.join(experiment_root, self.trial_config[\"Trial\"])\n",
    "        os.makedirs(self.trial_folder, exist_ok=True)\n",
    "        self.state_file = os.path.join(self.trial_folder, \"state.json\")\n",
    "        self.dataset = None\n",
    "\n",
    "    def default_model_builder(self):\n",
    "        \"\"\"\n",
    "        Build a default placeholder model.\n",
    "        Replace this with a call to your custom model builder (e.g., Custom_Models.create_dcnn_model).\n",
    "        \"\"\"\n",
    "        model = models.Sequential([\n",
    "            layers.InputLayer(input_shape=(5000, 5)),\n",
    "            layers.Conv1D(32, 3, activation='relu', padding='same'),\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dense(5, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"f1_score\"])\n",
    "        return model\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Constructs the tfrecord filename based on trial configuration and loads the dataset.\n",
    "        Modify the logic below to reflect your specific naming convention.\n",
    "        \"\"\"\n",
    "        # For example: \"Epoch-Units_Style_Smoothing_list-of-shifts-if-incremental-divided-by-1000_IEData.tfrecord.gz\"\n",
    "        epoch_unit_placeholder = \"XX\"  # Placeholder: replace or update with epoch unit if needed.\n",
    "        style = self.trial_config[\"Style\"]  # e.g., \"M\" or \"I\"\n",
    "        smoothing = self.trial_config[\"Smoothing\"]  # e.g., \"C\" (custom) or \"B\" (binary)\n",
    "        # For incremental style, include shifts (placeholder here as \"0-2.5\")\n",
    "        shifts = \"0-2.5\" if style.upper() == \"I\" else \"\"\n",
    "        file_name = f\"{epoch_unit_placeholder}_{style}_{smoothing}_{shifts}_IEData.tfrecord.gz\"\n",
    "        # Adjust the base path for your tfrecords:\n",
    "        tfrecord_path = os.path.join(\"path_to_tfrecords\", file_name)\n",
    "        \n",
    "        # Determine if the Background should be removed (interpreting string to boolean).\n",
    "        cut_background = str(self.trial_config[\"Background\"]).lower() in [\"true\", \"1\"]\n",
    "        # Use the \"Size\" field for the batch size.\n",
    "        batch_size = int(self.trial_config[\"Size\"])\n",
    "        self.dataset = iem.Data_Functions.prep_dataset_from_tfrecord(\n",
    "            tfrecord_path,\n",
    "            batch_size=batch_size,\n",
    "            shuffled=True,\n",
    "            cut_background=cut_background\n",
    "        )\n",
    "    \n",
    "    def resume_progress(self):\n",
    "        \"\"\"\n",
    "        Resumes from a previously saved state if it exists.\n",
    "        Loads the current epoch unit and last saved checkpoint.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.state_file):\n",
    "            with open(self.state_file, \"r\") as f:\n",
    "                state = json.load(f)\n",
    "            self.current_epoch_unit = state.get(\"current_epoch_unit\", 0)\n",
    "            last_checkpoint = state.get(\"last_checkpoint\", None)\n",
    "            if last_checkpoint and os.path.exists(last_checkpoint):\n",
    "                self.model = tf.keras.models.load_model(last_checkpoint, compile=False)\n",
    "            print(f\"Resumed trial {self.trial_config['Trial']} at epoch unit {self.current_epoch_unit}\")\n",
    "        else:\n",
    "            print(f\"Starting new trial {self.trial_config['Trial']}\")\n",
    "    \n",
    "    def save_state(self, checkpoint_path):\n",
    "        \"\"\"\n",
    "        Saves the current experiment state to allow resuming later.\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            \"current_epoch_unit\": self.current_epoch_unit,\n",
    "            \"last_checkpoint\": checkpoint_path\n",
    "        }\n",
    "        with open(self.state_file, \"w\") as f:\n",
    "            json.dump(state, f)\n",
    "        print(f\"Saved state for trial {self.trial_config['Trial']} to {self.state_file}\")\n",
    "\n",
    "    def train_epoch_unit(self):\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch unit.\n",
    "        Implements early rewarding logic (active only for the first 3 epoch units),\n",
    "        saves a checkpoint, training history, and a plot of the train/validation curve.\n",
    "        \"\"\"\n",
    "        checkpoint_path = os.path.join(self.trial_folder, f\"checkpoint_epochunit_{self.current_epoch_unit+1}.h5\")\n",
    "        cp_callback = callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            save_weights_only=False,\n",
    "            save_freq=\"epoch\",\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks_list = [cp_callback]\n",
    "        \n",
    "        # Early Rewarding: Only effective if set in the config and for the first 3 epoch units.\n",
    "        early_reward = str(self.trial_config[\"Early Rewarding\"]).lower() in [\"true\", \"1\"]\n",
    "        if early_reward and self.current_epoch_unit >= 3:\n",
    "            early_reward = False\n",
    "        print(f\"Trial {self.trial_config['Trial']}: Early Rewarding set to {early_reward} at epoch unit {self.current_epoch_unit+1}\")\n",
    "        \n",
    "        # Train for one epoch unit. (Here, one epoch = one epoch unit.)\n",
    "        history = self.model.fit(\n",
    "            self.dataset,\n",
    "            epochs=1,\n",
    "            callbacks=callbacks_list,\n",
    "            validation_data=self.dataset,  # Adjust for a dedicated validation dataset if available.\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save the training history to JSON\n",
    "        iem.Helper_Functions.save_history_to_json(history, metadata=f\"epochunit_{self.current_epoch_unit+1}\", save_path=self.trial_folder)\n",
    "        \n",
    "        # Plot and save the training and validation curve\n",
    "        fig = iem.Helper_Functions.plot_train_val_curve(history, training_target_variable=\"loss\")  # Modify target variable as needed.\n",
    "        plot_path = os.path.join(self.trial_folder, f\"train_val_curve_epochunit_{self.current_epoch_unit+1}.jpg\")\n",
    "        fig.savefig(plot_path)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        self.current_epoch_unit += 1\n",
    "        self.save_state(checkpoint_path)\n",
    "    \n",
    "    def run_trial(self):\n",
    "        \"\"\"\n",
    "        Runs the trial by loading the dataset, initializing (or resuming) the model,\n",
    "        and training for 10 epoch units.\n",
    "        \"\"\"\n",
    "        self.load_dataset()\n",
    "        if self.model is None:\n",
    "            self.model = self.model_builder()\n",
    "        self.resume_progress()\n",
    "        while self.current_epoch_unit < self.total_epoch_units:\n",
    "            print(f\"Trial {self.trial_config['Trial']}: Starting epoch unit {self.current_epoch_unit+1}/{self.total_epoch_units}\")\n",
    "            self.train_epoch_unit()\n",
    "        print(f\"Trial {self.trial_config['Trial']} completed.\")\n",
    "\n",
    "def run_trials_from_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Reads the CSV file containing trial configurations and runs the experiment for each trial.\n",
    "    \n",
    "    CSV file is expected to have the following columns:\n",
    "      Trial, Smoothing, Background, Early Rewarding, Size, Style, Dilation, Attention\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Define a root directory to hold all trial folders.\n",
    "    experiment_root = \"ExperimentResults\"\n",
    "    os.makedirs(experiment_root, exist_ok=True)\n",
    "    \n",
    "    # Loop over each row (trial) and run the experiment.\n",
    "    for idx, row in df.iterrows():\n",
    "        trial_config = row.to_dict()\n",
    "        print(f\"Starting trial: {trial_config.get('Trial', 'Unknown')}\")\n",
    "        trial_handler = ExperimentHandler(trial_config, experiment_root=experiment_root)\n",
    "        trial_handler.run_trial()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this with the path to your CSV file containing trial configurations.\n",
    "    csv_path = \"trial_config.csv\"\n",
    "    run_trials_from_csv(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
