{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "Complete Binary, w/ background - sizes 1 and 10 and highly mixed 5\n",
    "Complete Binary, no background - sizes 1 and 10 and incremental 5\n",
    "Custom Smoothing, w/ background - sizes 1 and 10 - This is the baseline\n",
    "Custom Smoothing, no background - sizes 1 and 10\n",
    "\n",
    "\n",
    "Loss settings:\n",
    "Custom Smoothing, enabled - Loss A -This is the baseline\n",
    "Custom Smoothing, disabled - Loss A\n",
    "No Smoothing, disabled - Loss A or B \n",
    "Proper Smoothing, disabled - Loss B\n",
    "\n",
    "\n",
    "Model Types: \n",
    "Standard Dilation, No Attention - This is the baseline\n",
    "Standard Dilation, Attention\n",
    "Reduced Dilation, No Attention\n",
    "Reduced Dilation, Attention\n",
    "\n",
    "Todo: Need to make a reduced dilation version of the model - done, model takes dilation scaler\n",
    "Confirm that if no smoothing and early reward disabled, Loss A and Loss B are equivalent - confirmed, so long as parameters and thresholds are the same\n",
    "\n",
    "Checklist of datasets:\n",
    "\n",
    " Complete Binary, w/ background\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10\n",
    "\n",
    " Highly mixed 5\n",
    "\n",
    "\n",
    " Complete Binary, no background\n",
    "\n",
    " Size 1\n",
    "\n",
    " Size 10\n",
    "\n",
    " Incremental 5 - have a custom smoothed version w/ background that can be converted\n",
    "\n",
    " Custom Smoothing, w/ background (This is the baseline)\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Custom Smoothing, no background\n",
    "\n",
    " Size 1\n",
    "\n",
    " Size 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 21:51:14.932147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-12 21:51:15.028668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-12 21:51:15.056663: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-12 21:51:15.232283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-12 21:51:16.827781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IEModules as iem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered Howdy! into test_function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Howdy!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.test_function(\"Howdy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "_bytes_feature\n",
      "_float_feature\n",
      "_int64_feature\n",
      "build_chunk_data_for_indices\n",
      "build_dataset_from_tfrecords\n",
      "bytes_feature\n",
      "calculate_introns\n",
      "compute_chunk_indices\n",
      "convert_and_write_tfrecord\n",
      "convert_labels_to_binary\n",
      "float_feature_list\n",
      "int_feature_list\n",
      "label_sequence_local\n",
      "load_gtf_annotations\n",
      "main\n",
      "one_hot_encode_reference\n",
      "pad_encoded_seq\n",
      "pad_labels\n",
      "parse_chunk_example\n",
      "search_gtf_by_range\n",
      "serialize_chunk_example\n",
      "serialize_example_with_metadata_no_convert\n",
      "split_tfrecords\n",
      "stream_shuffled_records\n",
      "swap_columns_if_needed\n",
      "test_dataset_from_tfrecords\n",
      "test_function\n",
      "trim_chr_genome\n",
      "tvt_split_tfrecords\n",
      "write_shuffled_records_to_single_tfrecord\n",
      "write_tfrecord_in_shards_hybrid\n",
      "write_to_shard_with_threads\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "functions = inspect.getmembers(iem.Genetic_Data_Pipeline, inspect.isfunction)\n",
    "for name, func in functions:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Genetic_Data_Pipeline for correctness\n",
    "\n",
    "Check Custom_Models for correctness\n",
    "\n",
    "Need to make a shard shuffler module probably that goes from output of genetic_data_pipeline to fully shuffled test-train-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          1. build_initial_shards function generates 4 tfrecord shards at each shift fed to it.\n",
      "          I recommend giving a list of 1 window shift.  Currently paths are hardcoded.  \n",
      "          The first 4*n characters of the shards will be the shift numbers where n is \n",
      "          the number of shifts fed to the function.\n",
      "          \n",
      "          2. split_tfrecords takes an input and output directory and number of file splits\n",
      "          and splits each TFRecord in the input_directory (assumed to be gzipped TFRecords)\n",
      "          into 'num_splits' smaller TFRecords.\n",
      "          \n",
      "          3. write_shuffled_records_to_single_tfrecord builds a single big, shuffled, \n",
      "          gzip-compressed TFRecord file \n",
      "          \n",
      "          Args:\n",
      "            input_dir (str): Directory containing the source TFRecord files.\n",
      "            allowed_indices (list): List of allowed starting indices.\n",
      "            output_filepath (str): Full path to the output TFRecord file.\n",
      "            \n",
      "          4. tvt_split_records test, validate, train splits the big tfrecord into \n",
      "          test, validate, and train datasets\n",
      "          \n",
      "          5. convert_and_write_tfrecord writes a binary only version of the dataset fed to it\n",
      "          \n",
      "          6. Removing background happens when loading the data into the model\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.dataset_pipeline_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist of datasets:\n",
    "\n",
    " Complete Binary, w/ background\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Highly mixed 5                                 Need to build highly mixed 5 and convert to binary\n",
    "\n",
    "\n",
    " Complete Binary, no background\n",
    "\n",
    " Size 1 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Size 10 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Incremental 5 - have a custom smoothed version w/ background that can be converted   conversion ongoing\n",
    "\n",
    " Custom Smoothing, w/ background (This is the baseline)\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Custom Smoothing, no background\n",
    "\n",
    " Size 1 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Size 10 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    "\n",
    "\n",
    " Size 10 has 2365261 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744095490.509284 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.699680 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.699799 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704425 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704488 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704525 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.994540 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.994740 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-08 00:58:10.994829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744095490.995069 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-08 00:58:10.995990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-08 09:35:11.668060: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/all_ten_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_ten_shuffled.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/fifths_incremental_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_fifths_incrememntal.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder created at: ../../Datasets/AugDataSets/AllTen/temp_shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744246104.885834  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.075605  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.075745  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078746  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078828  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078872  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.373822  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.373927  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-09 18:48:25.373940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744246105.374015  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-09 18:48:25.374434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-09 19:01:38.755912: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records found in input dataset: 2365261\n",
      "Sampling 1182630 records (fraction = 0.5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 19:14:36.772563: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split all_ten_shuffled.tfrecord.gz into 45 shards.\n",
      "10000 records written so far...\n",
      "20000 records written so far...\n",
      "30000 records written so far...\n",
      "40000 records written so far...\n",
      "50000 records written so far...\n",
      "60000 records written so far...\n",
      "70000 records written so far...\n",
      "80000 records written so far...\n",
      "90000 records written so far...\n",
      "100000 records written so far...\n",
      "110000 records written so far...\n",
      "120000 records written so far...\n",
      "130000 records written so far...\n",
      "140000 records written so far...\n",
      "150000 records written so far...\n",
      "160000 records written so far...\n",
      "170000 records written so far...\n",
      "180000 records written so far...\n",
      "190000 records written so far...\n",
      "200000 records written so far...\n",
      "210000 records written so far...\n",
      "220000 records written so far...\n",
      "230000 records written so far...\n",
      "240000 records written so far...\n",
      "250000 records written so far...\n",
      "260000 records written so far...\n",
      "270000 records written so far...\n",
      "280000 records written so far...\n",
      "290000 records written so far...\n",
      "300000 records written so far...\n",
      "310000 records written so far...\n",
      "320000 records written so far...\n",
      "330000 records written so far...\n",
      "340000 records written so far...\n",
      "350000 records written so far...\n",
      "360000 records written so far...\n",
      "370000 records written so far...\n",
      "380000 records written so far...\n",
      "390000 records written so far...\n",
      "400000 records written so far...\n",
      "410000 records written so far...\n",
      "420000 records written so far...\n",
      "430000 records written so far...\n",
      "440000 records written so far...\n",
      "450000 records written so far...\n",
      "460000 records written so far...\n",
      "470000 records written so far...\n",
      "480000 records written so far...\n",
      "490000 records written so far...\n",
      "500000 records written so far...\n",
      "510000 records written so far...\n",
      "520000 records written so far...\n",
      "530000 records written so far...\n",
      "540000 records written so far...\n",
      "550000 records written so far...\n",
      "560000 records written so far...\n",
      "570000 records written so far...\n",
      "580000 records written so far...\n",
      "590000 records written so far...\n",
      "600000 records written so far...\n",
      "610000 records written so far...\n",
      "620000 records written so far...\n",
      "630000 records written so far...\n",
      "640000 records written so far...\n",
      "650000 records written so far...\n",
      "660000 records written so far...\n",
      "670000 records written so far...\n",
      "680000 records written so far...\n",
      "690000 records written so far...\n",
      "700000 records written so far...\n",
      "710000 records written so far...\n",
      "720000 records written so far...\n",
      "730000 records written so far...\n",
      "740000 records written so far...\n",
      "750000 records written so far...\n",
      "760000 records written so far...\n",
      "770000 records written so far...\n",
      "780000 records written so far...\n",
      "790000 records written so far...\n",
      "800000 records written so far...\n",
      "810000 records written so far...\n",
      "820000 records written so far...\n",
      "830000 records written so far...\n",
      "840000 records written so far...\n",
      "850000 records written so far...\n",
      "860000 records written so far...\n",
      "870000 records written so far...\n",
      "880000 records written so far...\n",
      "890000 records written so far...\n",
      "900000 records written so far...\n",
      "910000 records written so far...\n",
      "920000 records written so far...\n",
      "930000 records written so far...\n",
      "940000 records written so far...\n",
      "950000 records written so far...\n",
      "960000 records written so far...\n",
      "970000 records written so far...\n",
      "980000 records written so far...\n",
      "990000 records written so far...\n",
      "1000000 records written so far...\n",
      "1010000 records written so far...\n",
      "1020000 records written so far...\n",
      "1030000 records written so far...\n",
      "1040000 records written so far...\n",
      "1050000 records written so far...\n",
      "1060000 records written so far...\n",
      "1070000 records written so far...\n",
      "1080000 records written so far...\n",
      "1090000 records written so far...\n",
      "1100000 records written so far...\n",
      "1110000 records written so far...\n",
      "1120000 records written so far...\n",
      "1130000 records written so far...\n",
      "1140000 records written so far...\n",
      "1150000 records written so far...\n",
      "1160000 records written so far...\n",
      "1170000 records written so far...\n",
      "1180000 records written so far...\n",
      "Finished writing 1182630 records to ../../Datasets/AugDataSets/50_all_ten_shuffled.tfrecord.gz\n",
      "Temporary shards have been removed.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.transform_tfdataset(datasets_path+\"AugDataSets/AllTen\", datasets_path+\"AugDataSets\", 45, 0.5)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 04:28:15.601091: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/50_all_ten_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_5x_mixed.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset New Naming Scheme: \n",
    "Epoch Units: 01-10, Style: M for Mixed or I for Incremental, Smoothing: C for custom smoothing, B for Binary\n",
    "Epoch-Units_Style_Smoothing_list-of-shifts-if-incremental-divided-by-1000_IEData.tfrecord.gz\n",
    "ex: 02_I_B_0-2.5_IEData.tfrecord.gz\n",
    "\n",
    "Per line, this is input from trial csv:\n",
    "Trial,Smoothing,Background,Early Rewarding,Size,Style,Dilation,Attention\n",
    "\n",
    "Trial becomes folder where model data are saved\n",
    "\n",
    "Smoothing, Size, Style define the dataset\n",
    "\n",
    "Background is a choice handled in the data loader which returns a dataset generator pretending to be a dataset (Data_Functions.prep_dataset_from_tfrecord())\n",
    "\n",
    "Early rewarding: if true, need an option in the loss functions or something that checks epoch number.  Rewarding==True for more than 3 epoch units is overkill so the switch should flip after 3 epoch units\n",
    "\n",
    "Dilation and Attention are model hyperparameters\n",
    "\n",
    "What I need then:\n",
    "Experiment handler function (class?) that reads a line from the csv and trains for 10 epoch units.  Needs a working folder given to it (Experiment Name is probably the folder name) and a subfolder for each trial (Trial_01, etc).  Checkpoints get saved in there, as does the model history which gets saved to json every epoch unit but overwritten, as should the StatefulReduceLROnPlateau callback state that I have yet to implement.\n",
    "The experiment should track which trial and which epoch was most recently saved and be able to resume from that trial and checkpoint easily.  Maybe have it save a jpg of the train validate curve at the end of the trial using the function in Helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744516349.562100 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.758718 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.758835 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762785 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762879 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762919 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516350.064039 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516350.064137 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-12 21:52:30.064148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744516350.064212 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-12 21:52:30.065583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-12 21:58:43.898655: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records found: 1182630\n",
      "Splitting into -> Train: 946104, Val: 118263, Test: 118263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 21:58:53.921334: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:6: Filling up shuffle buffer (this may take a while): 22613 of 25000\n",
      "2025-04-12 21:58:55.008046: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "2025-04-12 22:51:29.014481: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Split Counts -> Train: 946104, Val: 118263, Test: 118263\n",
      "05_M_B_IEData.tfrecord.gz split\n",
      "Total records found: 2365261\n",
      "Splitting into -> Train: 1892208, Val: 236526, Test: 236527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 23:04:33.559500: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:15: Filling up shuffle buffer (this may take a while): 20358 of 25000\n",
      "2025-04-12 23:04:35.240383: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Split Counts -> Train: 1892208, Val: 236526, Test: 236527\n",
      "10_M_C_IEData.tfrecord.gz split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:55:15.264840: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "data_directory = datasets_path+\"Experiment 01/\"\n",
    "save_directory = data_directory+\"TestValTrain/\"\n",
    "for root, dirs, files in os.walk(data_directory):\n",
    "    if 'TestValTrain' in dirs:\n",
    "        dirs.remove('TestValTrain')\n",
    "    for file in files:\n",
    "        file_path = data_directory+os.path.basename(file)\n",
    "        train_save_file_path = save_directory+\"Train_\"+os.path.basename(file)\n",
    "        val_save_file_path = save_directory+\"Val_\"+os.path.basename(file)\n",
    "        test_save_file_path = save_directory+\"Test_\"+os.path.basename(file)\n",
    "        iem.Genetic_Data_Pipeline.tvt_split_tfrecords(file_path, train_save_file_path, val_save_file_path,test_save_file_path)\n",
    "        print(os.path.basename(file)+\" split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# \"\"\"\n",
    "# Experiment Handler Module\n",
    "\n",
    "# This module defines an ExperimentHandler class that:\n",
    "#   • Reads a single trial’s configuration (with keys: Trial, Smoothing, Background,\n",
    "#     Early Rewarding, Size, Style, Dilation, Attention) from a CSV file.\n",
    "#   • Creates an experiment folder under a common experiment root for each trial.\n",
    "#   • Loads a dataset from a tfrecord using a constructed filename based on the configuration.\n",
    "#   • Builds a model via a provided model builder or a default placeholder.\n",
    "#   • Trains the model for 10 \"epoch units\" (one epoch per unit in this example).\n",
    "#   • Saves checkpoints, training history (using Helper_Functions.save_history_to_json),\n",
    "#     and a train/validation curve plot (using Helper_Functions.plot_train_val_curve) after each epoch unit.\n",
    "#   • Supports resuming progress from a saved state.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# import time\n",
    "# import sys\n",
    "# import os\n",
    "# import glob\n",
    "# import math\n",
    "# import threading\n",
    "# import concurrent.futures as cf\n",
    "# import random\n",
    "# import re\n",
    "# import json\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "# from keras import backend as K\n",
    "# import gc\n",
    "# import keras_tuner as kt\n",
    "# from pyfaidx import Fasta\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# datasets_path = \"../../Datasets/\"\n",
    "# models_path = \"../../Models/\"\n",
    "# import IEModules as iem\n",
    "\n",
    "\n",
    "# # Optionally import custom callbacks from Custom_Callbacks if available\n",
    "# # from Custom_Callbacks import StatefulReduceLROnPlateau  # placeholder for stateful LR scheduling\n",
    "\n",
    "# class ExperimentHandler:\n",
    "#     def __init__(self, trial_config, experiment_root=\"ExperimentResults\", model_builder=None):\n",
    "#         \"\"\"\n",
    "#         Initialize experiment for a single trial.\n",
    "        \n",
    "#         Parameters:\n",
    "#           trial_config (dict): Dictionary with keys:\n",
    "#              \"Trial\", \"Smoothing\", \"Background\", \"Early Rewarding\", \"Size\", \"Style\", \"Dilation\", \"Attention\"\n",
    "#           experiment_root (str): The root directory for experiment results.\n",
    "#           model_builder (callable, optional): Function that builds and compiles the model.\n",
    "#                Defaults to a simple placeholder model.\n",
    "#         \"\"\"\n",
    "#         self.trial_config = trial_config\n",
    "#         self.total_epoch_units = 10\n",
    "#         self.current_epoch_unit = 0\n",
    "#         self.model = None\n",
    "#         self.model_builder = model_builder if model_builder is not None else self.default_model_builder\n",
    "\n",
    "#         # Create a subfolder for the trial using the \"Trial\" field under the experiment root.\n",
    "#         self.trial_folder = os.path.join(experiment_root, self.trial_config[\"Trial\"])\n",
    "#         os.makedirs(self.trial_folder, exist_ok=True)\n",
    "#         self.state_file = os.path.join(self.trial_folder, \"state.json\")\n",
    "#         self.dataset = None\n",
    "\n",
    "#     def default_model_builder(self):\n",
    "#         \"\"\"\n",
    "#         Build a default placeholder model.\n",
    "#         Replace this with a call to your custom model builder (e.g., Custom_Models.create_dcnn_model).\n",
    "#         \"\"\"\n",
    "#         model = models.Sequential([\n",
    "#             layers.InputLayer(input_shape=(5000, 5)),\n",
    "#             layers.Conv1D(32, 3, activation='relu', padding='same'),\n",
    "#             layers.GlobalAveragePooling1D(),\n",
    "#             layers.Dense(5, activation='sigmoid')\n",
    "#         ])\n",
    "#         model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"f1_score\"])\n",
    "#         return model\n",
    "\n",
    "#     def load_dataset(self):\n",
    "#         \"\"\"\n",
    "#         Constructs the tfrecord filename based on trial configuration and loads the dataset.\n",
    "#         Modify the logic below to reflect your specific naming convention.\n",
    "#         \"\"\"\n",
    "#         # For example: \"Epoch-Units_Style_Smoothing_list-of-shifts-if-incremental-divided-by-1000_IEData.tfrecord.gz\"\n",
    "#         epoch_unit_placeholder = \"XX\"  # Placeholder: replace or update with epoch unit if needed.\n",
    "#         style = self.trial_config[\"Style\"]  # e.g., \"M\" or \"I\"\n",
    "#         smoothing = self.trial_config[\"Smoothing\"]  # e.g., \"C\" (custom) or \"B\" (binary)\n",
    "#         # For incremental style, include shifts (placeholder here as \"0-2.5\")\n",
    "#         shifts = \"0-2.5\" if style.upper() == \"I\" else \"\"\n",
    "#         file_name = f\"{epoch_unit_placeholder}_{style}_{smoothing}_{shifts}_IEData.tfrecord.gz\"\n",
    "#         # Adjust the base path for your tfrecords:\n",
    "#         tfrecord_path = os.path.join(\"path_to_tfrecords\", file_name)\n",
    "        \n",
    "#         # Determine if the Background should be removed (interpreting string to boolean).\n",
    "#         cut_background = str(self.trial_config[\"Background\"]).lower() in [\"true\", \"1\"]\n",
    "#         # Use the \"Size\" field for the batch size.\n",
    "#         batch_size = int(self.trial_config[\"Size\"])\n",
    "#         self.dataset = iem.Data_Functions.prep_dataset_from_tfrecord(\n",
    "#             tfrecord_path,\n",
    "#             batch_size=batch_size,\n",
    "#             shuffled=True,\n",
    "#             cut_background=cut_background\n",
    "#         )\n",
    "    \n",
    "#     def resume_progress(self):\n",
    "#         \"\"\"\n",
    "#         Resumes from a previously saved state if it exists.\n",
    "#         Loads the current epoch unit and last saved checkpoint.\n",
    "#         \"\"\"\n",
    "#         if os.path.exists(self.state_file):\n",
    "#             with open(self.state_file, \"r\") as f:\n",
    "#                 state = json.load(f)\n",
    "#             self.current_epoch_unit = state.get(\"current_epoch_unit\", 0)\n",
    "#             last_checkpoint = state.get(\"last_checkpoint\", None)\n",
    "#             if last_checkpoint and os.path.exists(last_checkpoint):\n",
    "#                 self.model = models.load_model(last_checkpoint, compile=False)\n",
    "#             print(f\"Resumed trial {self.trial_config['Trial']} at epoch unit {self.current_epoch_unit}\")\n",
    "#         else:\n",
    "#             print(f\"Starting new trial {self.trial_config['Trial']}\")\n",
    "    \n",
    "#     def save_state(self, checkpoint_path):\n",
    "#         \"\"\"\n",
    "#         Saves the current experiment state to allow resuming later.\n",
    "#         \"\"\"\n",
    "#         state = {\n",
    "#             \"current_epoch_unit\": self.current_epoch_unit,\n",
    "#             \"last_checkpoint\": checkpoint_path\n",
    "#         }\n",
    "#         with open(self.state_file, \"w\") as f:\n",
    "#             json.dump(state, f)\n",
    "#         print(f\"Saved state for trial {self.trial_config['Trial']} to {self.state_file}\")\n",
    "\n",
    "#     def train_epoch_unit(self):\n",
    "#         \"\"\"\n",
    "#         Trains the model for one epoch unit.\n",
    "#         Implements early rewarding logic (active only for the first 3 epoch units),\n",
    "#         saves a checkpoint, training history, and a plot of the train/validation curve.\n",
    "#         \"\"\"\n",
    "#         checkpoint_path = os.path.join(self.trial_folder, f\"checkpoint_epochunit_{self.current_epoch_unit+1}.h5\")\n",
    "#         cp_callback = callbacks.ModelCheckpoint(\n",
    "#             filepath=checkpoint_path,\n",
    "#             save_weights_only=False,\n",
    "#             save_freq=\"epoch\",\n",
    "#             verbose=1\n",
    "#         )\n",
    "#         callbacks_list = [cp_callback]\n",
    "        \n",
    "#         # Early Rewarding: Only effective if set in the config and for the first 3 epoch units.\n",
    "#         early_reward = str(self.trial_config[\"Early Rewarding\"]).lower() in [\"true\", \"1\"]\n",
    "#         if early_reward and self.current_epoch_unit >= 3:\n",
    "#             early_reward = False\n",
    "#         print(f\"Trial {self.trial_config['Trial']}: Early Rewarding set to {early_reward} at epoch unit {self.current_epoch_unit+1}\")\n",
    "        \n",
    "#         # Train for one epoch unit. (Here, one epoch = one epoch unit.)\n",
    "#         history = self.model.fit(\n",
    "#             self.dataset,\n",
    "#             epochs=1,\n",
    "#             callbacks=callbacks_list,\n",
    "#             validation_data=self.dataset,  # Adjust for a dedicated validation dataset if available.\n",
    "#             verbose=1\n",
    "#         )\n",
    "        \n",
    "#         # Save the training history to JSON\n",
    "#         iem.Helper_Functions.save_history_to_json(history, metadata=f\"epochunit_{self.current_epoch_unit+1}\", save_path=self.trial_folder)\n",
    "        \n",
    "#         # Plot and save the training and validation curve\n",
    "#         fig = iem.Helper_Functions.plot_train_val_curve(history, training_target_variable=\"loss\")  # Modify target variable as needed.\n",
    "#         plot_path = os.path.join(self.trial_folder, f\"train_val_curve_epochunit_{self.current_epoch_unit+1}.jpg\")\n",
    "#         fig.savefig(plot_path)\n",
    "#         plt.close(fig)\n",
    "        \n",
    "#         self.current_epoch_unit += 1\n",
    "#         self.save_state(checkpoint_path)\n",
    "    \n",
    "#     def run_trial(self):\n",
    "#         \"\"\"\n",
    "#         Runs the trial by loading the dataset, initializing (or resuming) the model,\n",
    "#         and training for 10 epoch units.\n",
    "#         \"\"\"\n",
    "#         self.load_dataset()\n",
    "#         if self.model is None:\n",
    "#             self.model = self.model_builder()\n",
    "#         self.resume_progress()\n",
    "#         while self.current_epoch_unit < self.total_epoch_units:\n",
    "#             print(f\"Trial {self.trial_config['Trial']}: Starting epoch unit {self.current_epoch_unit+1}/{self.total_epoch_units}\")\n",
    "#             self.train_epoch_unit()\n",
    "#         print(f\"Trial {self.trial_config['Trial']} completed.\")\n",
    "\n",
    "# def run_trials_from_csv(csv_path):\n",
    "#     \"\"\"\n",
    "#     Reads the CSV file containing trial configurations and runs the experiment for each trial.\n",
    "    \n",
    "#     CSV file is expected to have the following columns:\n",
    "#       Trial, Smoothing, Background, Early Rewarding, Size, Style, Dilation, Attention\n",
    "#     \"\"\"\n",
    "#     df = pd.read_csv(csv_path)\n",
    "    \n",
    "#     # Define a root directory to hold all trial folders.\n",
    "#     experiment_root = \"ExperimentResults\"\n",
    "#     os.makedirs(experiment_root, exist_ok=True)\n",
    "    \n",
    "#     # Loop over each row (trial) and run the experiment.\n",
    "#     for idx, row in df.iterrows():\n",
    "#         trial_config = row.to_dict()\n",
    "#         print(f\"Starting trial: {trial_config.get('Trial', 'Unknown')}\")\n",
    "#         trial_handler = ExperimentHandler(trial_config, experiment_root=experiment_root)\n",
    "#         trial_handler.run_trial()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Update this with the path to your CSV file containing trial configurations.\n",
    "#     csv_path = \"trial_config.csv\"\n",
    "#     run_trials_from_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  User/CLI                                                             │\n",
    "│  $ python experiment_framework.py spec.csv EXP_NAME                  │\n",
    "└────────────┬──────────────────────────────────────────────────────────┘\n",
    "             │\n",
    "             ▼\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  ExperimentHandler(csv_path, experiment_name, …)                      │\n",
    "│  ├─ reads CSV into DataFrame                                          │\n",
    "│  └─ creates   <MODEL_DIR>/<EXP_NAME>/                                 │\n",
    "└────────────┬──────────────────────────────────────────────────────────┘\n",
    "             │\n",
    "             ▼\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  run()                                                                │\n",
    "│  loop rows ▸ trial_row                                                │\n",
    "│     │                                                                 │\n",
    "│     ├─ _prepare_trial_folder()    →  …/Trial_XX/ (+ checkpoints/)     │\n",
    "│     ├─ discover latest checkpoint ─┐                                  │\n",
    "│     │                             └─▶  models.load_model()   │\n",
    "│     └─ if none → _build_model()   →  create_modular_dcnn_model()      │\n",
    "│                                                                       │\n",
    "│     loop epoch-unit 0-9                                               │\n",
    "│        ├─ _load_datasets()        →  Data_Functions.prep_dataset…     │\n",
    "│        ├─ _make_callbacks()       →  {ModelCheckpoint, LR, TB…}       │\n",
    "│        └─ model.fit(epochs=1)     →  train 1 *epoch-unit*             │\n",
    "│               └─ Helper_Functions.save_history_to_json() (overwrite)  │\n",
    "│                                                                       │\n",
    "│     after 10 units                                                    │\n",
    "│        └─ _finalise_trial()                                           │\n",
    "│            ├─ plot_train_val_curve() → Trial_XX/train_val_curve.jpg   │\n",
    "│            └─ model.save(\"final_model.keras\")                         │\n",
    "└────────────────────────────────────────────────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_framework.py\n",
    "\"\"\"\n",
    "High-level experiment orchestration for the Intron-Exon project.\n",
    "\n",
    "* 1 CSV == 1 experiment folder ⤏ each row is a *trial* that runs **10 epoch-units**.\n",
    "* Handles automatic folder creation, checkpointing, history/curve saving, and resume logic.\n",
    "* Designed to work with the existing IEModules package (Data_Functions, Custom_Models, etc.).\n",
    "\n",
    "The implementation purposefully leaves a handful of 🔲 *fill-in-the-blank* areas where deeper\n",
    "project knowledge is needed.  Search for the keyword `TODO` to complete the details.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# ── In-house modules ───────────────────────────────────────────────────────────\n",
    "from IEModules import (\n",
    "    Data_Functions,\n",
    "    Custom_Models,\n",
    "    Custom_Callbacks,\n",
    "    Custom_Losses,\n",
    "    Helper_Functions,\n",
    ")\n",
    "from IEModules.config import DATA_DIR, MODEL_DIR, LOG_DIR, EPOCH_UNIT_SIZE, EPOCH_UNITS_PER_TRIAL, DEFAULT_BATCH_SIZE, STEPS_PER_EPOCH_UNIT, METRICS\n",
    "\n",
    "# # ── Constants ──────────────────────────────────────────────────────────────────\n",
    "# EPOCH_UNIT_SIZE = int(1182630/5)\n",
    "# EPOCH_UNITS_PER_TRIAL = 10          # <- Each trial always runs exactly 10 units\n",
    "# DEFAULT_BATCH_SIZE = 28\n",
    "# STEPS_PER_EPOCH_UNIT = int(EPOCH_UNIT_SIZE/DEFAULT_BATCH_SIZE) # Becomes steps_per_epoch\n",
    "\n",
    "\n",
    "# Dataset naming helpers -------------------------------------------------------\n",
    "\n",
    "def build_dataset_filepath(\n",
    "    split: str,              # 'train' | 'val' | 'test'\n",
    "    units: int,              # 1-10  (will be zero-padded to 02)\n",
    "    style_flag: str,         # 'M' or 'I'\n",
    "    smoothing_flag: str,     # 'C' or 'B'\n",
    "    base_data_dir: str | Path = DATA_DIR # + Experiment XX/ ############ NEED TO MAKE THIS PART OF CONFIG ##############\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return the *full* path to the TFRecord whose filename starts with\n",
    "    '{split}_{units:02d}_{style_flag}_{smoothing_flag}' (case-insensitive).\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError - no matching file in base_data_dir\n",
    "        RuntimeError      - more than one match (ambiguous)\n",
    "    \"\"\"\n",
    "    split   = split.lower()\n",
    "    prefix  = f\"{split}_{units:02d}_{style_flag.upper()}_{smoothing_flag.upper()}\"\n",
    "    pattern = f\"{prefix}*IEData.tfrecord.gz\"        # wildcard swallows shift part\n",
    "    matches = glob.glob(str(Path(base_data_dir) / pattern), recursive=False)\n",
    "\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No TFRecord found for prefix '{prefix}' in {base_data_dir}\")\n",
    "    if len(matches) > 1:\n",
    "        raise RuntimeError(f\"Ambiguous prefix '{prefix}': {matches}\")\n",
    "\n",
    "    return matches[0]   # the unique match\n",
    "\n",
    "\n",
    "\n",
    "# ── Utility functions ─────────────────────────────────────────────────────────\n",
    "\n",
    "def latest_checkpoint(ckpt_dir: Path) -> Optional[Path]:\n",
    "    \"\"\"Return the most recent .keras checkpoint inside *ckpt_dir* (None if absent).\"\"\"\n",
    "    checkpoints = sorted(ckpt_dir.glob(\"*.keras\"))\n",
    "    return checkpoints[-1] if checkpoints else None\n",
    "\n",
    "\n",
    "def parse_epoch_from_ckpt(ckpt_path: Path) -> int:\n",
    "    \"\"\"Extract integer epoch-unit from a filename like 'epoch-05-val_metric-0.1234.keras'.\"\"\"\n",
    "    m = re.search(r\"epoch[-_](\\d+)\", ckpt_path.name)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "\n",
    "# ── Main class ────────────────────────────────────────────────────────────────\n",
    "\n",
    "class ExperimentHandler:\n",
    "    \"\"\"Drive the end-to-end training process described by a CSV file.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Path | str, # The csv with trial parameters\n",
    "        experiment_name: str,\n",
    "        model_dir: Path | str = MODEL_DIR,\n",
    "        data_subfolder: Path | str = \"\",  ######### PROBABLY NEED TO MAKE COMPATIBLE WITH CONFIG ###########\n",
    "        resume: bool = True,\n",
    "        batch_size: int = DEFAULT_BATCH_SIZE,\n",
    "    ) -> None:\n",
    "        self.csv_path      = Path(csv_path)\n",
    "        self.experiment    = experiment_name\n",
    "        self.batch_size    = batch_size\n",
    "        self.resume_flag   = resume\n",
    "        self.experiment_dir = Path(model_dir) / experiment_name\n",
    "        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.trials_df = pd.read_csv(self.csv_path)\n",
    "        # Normalise column names just in case\n",
    "        self.trials_df.columns = [c.strip() for c in self.trials_df.columns]\n",
    "        self.base_data_dir = DATA_DIR + data_subfolder\n",
    "\n",
    "    # ── Public API ──────────────────────────────────────────────────────────\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"Loop over trials and epoch-units, honouring resume logic.\"\"\"\n",
    "        for row_idx, trial_row in self.trials_df.iterrows():\n",
    "            trial_id  = int(trial_row[\"Trial\"])  # e.g. 1 ➜ folder Trial_01\n",
    "            trial_dir = self._prepare_trial_folder(trial_id)\n",
    "\n",
    "            # ── Determine starting point ---------------------------------------------------\n",
    "            ckpt_dir = trial_dir / \"checkpoints\"\n",
    "            ckpt_dir.mkdir(exist_ok=True)\n",
    "            start_unit = 0\n",
    "            model = None\n",
    "            if self.resume_flag:\n",
    "                latest_ckpt = latest_checkpoint(ckpt_dir)\n",
    "                if latest_ckpt is not None:\n",
    "                    start_unit = parse_epoch_from_ckpt(latest_ckpt) + 1\n",
    "                    model = models.load_model(latest_ckpt, compile=False)\n",
    "\n",
    "            # ── Build model from scratch if needed ----------------------------------------\n",
    "            if model is None:\n",
    "                model = self._build_model(trial_row)\n",
    "\n",
    "            # ── Training loop over *epoch units* -----------------------------------------\n",
    "            for unit in range(start_unit, EPOCH_UNITS_PER_TRIAL):\n",
    "                print(f\"\\n⚙️  Trial {trial_id:02d}  •  Epoch-Unit {unit+1}/{EPOCH_UNITS_PER_TRIAL}\")\n",
    "\n",
    "                train_ds, val_ds = self._load_datasets(trial_row)\n",
    "                \n",
    "                '''#################### GOTTA UPDATE THIS FOR CONFIG ##############'''\n",
    "                callbacks = self._make_callbacks(trial_row, ckpt_dir, unit)\n",
    "\n",
    "                \n",
    "                history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=EPOCH_UNITS_PER_TRIAL,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH_UNIT,\n",
    "                    callbacks=callbacks,\n",
    "                )\n",
    "\n",
    "                # Overwrite history JSON each unit (requirement)\n",
    "                Helper_Functions.save_history_to_json(\n",
    "                    history,\n",
    "                    metadata=\"history\",  # single file always named history_*.json\n",
    "                    save_path=str(trial_dir),\n",
    "                )\n",
    "\n",
    "            # ── End-of-trial artifacts ----------------------------------------------------\n",
    "            self._finalise_trial(model, trial_dir)\n",
    "\n",
    "        print(\"\\n✅ All trials complete!\")\n",
    "\n",
    "    # ── Internals ───────────────────────────────────────────────────────────\n",
    "\n",
    "    def _prepare_trial_folder(self, trial_num: int) -> Path:\n",
    "        trial_dir = self.experiment_dir / f\"Trial_{trial_num:02d}\"\n",
    "        trial_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (trial_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
    "        return trial_dir\n",
    "\n",
    "    \n",
    "    def _dataset_path_from_row(self, row: pd.Series, split: str, base_data_dir: str) -> str:\n",
    "        \"\"\"\n",
    "        Given a CSV row and a split ('train', 'val', 'test'),\n",
    "        return the full path to the TFRecord.\n",
    "        \"\"\"\n",
    "        units_flag     = int(row[\"Size\"])                       # 1-10\n",
    "        style_flag     = str(row[\"Style\"]).upper()[0]           # 'M' | 'I'\n",
    "        smoothing_flag = (\n",
    "            \"C\" if str(row[\"Smoothing\"]).upper().startswith(\"C\") else \"B\"\n",
    "        )\n",
    "\n",
    "        # shift descriptor intentionally ignored\n",
    "        return build_dataset_filepath(\n",
    "            split          = split,\n",
    "            units          = units_flag,\n",
    "            style_flag     = style_flag,\n",
    "            smoothing_flag = smoothing_flag,\n",
    "            base_data_dir  = base_data_dir\n",
    "        )\n",
    "\n",
    "    def _load_datasets(self, row: pd.Series, test: bool = False):\n",
    "        \"\"\"\n",
    "        Instantiate *separate* train / validation datasets based on the CSV row.\n",
    "        Also works as a train-test-set loader, set test = True when using _load_datasets\n",
    "        test dataset pretends to be the val set when test = True\n",
    "        \"\"\"\n",
    "        train_path = self._dataset_path_from_row(row, split=\"train\")\n",
    "        if test:\n",
    "            val_path = self._dataset_path_from_row(row, split=\"test\")\n",
    "        else:\n",
    "            val_path = self._dataset_path_from_row(row, split=\"val\")\n",
    "\n",
    "        train_ds = Data_Functions.prep_dataset_from_tfrecord(\n",
    "            train_path,\n",
    "            batch_size   = self.batch_size,\n",
    "            shuffled     = True,                       # shuffle only training\n",
    "            cut_background = not bool(row[\"Background\"]),\n",
    "        )\n",
    "        train_ds = train_ds.repeat()\n",
    "\n",
    "        val_ds = Data_Functions.prep_dataset_from_tfrecord(\n",
    "            val_path,\n",
    "            batch_size   = self.batch_size,\n",
    "            shuffled     = False,                      # never shuffle validation\n",
    "            cut_background = not bool(row[\"Background\"]),\n",
    "        )\n",
    "        return train_ds, val_ds\n",
    "\n",
    "    def _build_model(self, row: pd.Series) -> Model:\n",
    "        \"\"\"Create and compile a new model according to hyper-params in *row*.\"\"\"\n",
    "        # 🔲 TODO - Make compatible with config\n",
    "        dilation_mult = float(row.get(\"Dilation\", 1.0))\n",
    "        use_attention = bool(row.get(\"Attention\", False))\n",
    "\n",
    "        model = Custom_Models.create_modular_dcnn_model(\n",
    "            dilation_multiplier=dilation_mult,\n",
    "            use_local_attention=use_attention,\n",
    "            use_long_range_attention=use_attention,\n",
    "            use_final_attention=use_attention,\n",
    "        )\n",
    "\n",
    "        loss_fn = self._select_loss(row)\n",
    "        metrics = METRICS\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(),\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def _select_loss(self, row: pd.Series):\n",
    "        \"\"\"Return the correct loss function instance (handles Early Rewarding switch).\"\"\"\n",
    "        early_rewarding = bool(row[\"Early Rewarding\"]) # True or False\n",
    "        smoothing = str(row[\"Smoothing\"]) # None (as a string), Custom, or Proper\n",
    "        background = bool(row[\"Background\"]) # True (Included) or False (Removed)\n",
    "        # Construct a config for loss function params\n",
    "        # pull that in\n",
    "        # update the others based on the three variables read in above\n",
    "        # choose loss fn with hyper params\n",
    "        chosen_loss = \"FILL IN CHOSEN LOSS STUFF\"\n",
    "        return chosen_loss\n",
    "\n",
    "# ########### NEED TO MAKE THIS COMPATIBLE WITH CONFIG ################\n",
    "    def _make_callbacks(\n",
    "        self,\n",
    "        row: pd.Series,\n",
    "        ckpt_dir: Path,\n",
    "        epoch_unit_idx: int,\n",
    "    ) -> List[callbacks.Callback]:\n",
    "        \"\"\"Create per-epoch-unit callback list.\"\"\"\n",
    "        ckpt_cb = callbacks.ModelCheckpoint(\n",
    "            filepath=str(ckpt_dir / f\"epoch-{epoch_unit_idx:03d}.keras\"),\n",
    "            save_weights_only=False,\n",
    "            save_best_only=False,\n",
    "        )\n",
    "\n",
    "        reduce_lr_cb = Custom_Callbacks.StatefulReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",             # 🔲 or val_no_background_f1\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            state_save_filepath=str(ckpt_dir / \"lr_state.json\"),\n",
    "        )\n",
    "\n",
    "        tensorboard_dir = LOG_DIR + f\"/{self.experiment}/trial_{row['Trial']:02d}/unit_{epoch_unit_idx:02d}\"\n",
    "        tb_cb = callbacks.TensorBoard(log_dir=tensorboard_dir, histogram_freq=1)\n",
    "\n",
    "        cleanup_cb = Custom_Callbacks.CleanupCallback()\n",
    "        return [ckpt_cb, reduce_lr_cb, tb_cb, cleanup_cb]\n",
    "\n",
    "    def _finalise_trial(self, model: Model, trial_dir: Path) -> None:\n",
    "        \"\"\"Plot train/val curves & save a compact *production* checkpoint.\"\"\"\n",
    "        # 🔲 TODO - capture the accumulated History over 10 units (requires persistence).\n",
    "        history_json = trial_dir + \"history_training_history.json\"\n",
    "        if history_json.exists():\n",
    "            with open(history_json) as fh:\n",
    "                history_dict = json.load(fh)\n",
    "            # Use helper to generate plot and save\n",
    "            fig = Helper_Functions.plot_train_val_curve(history_object=type(\"H\", (), {\"history\": history_dict})(),\n",
    "                                                        training_target_variable=\"loss\")\n",
    "            fig.savefig(trial_dir + \"train_val_curve.jpg\")\n",
    "\n",
    "        # Save final model (weights only)\n",
    "        model.save(trial_dir + \"final_model.keras\")\n",
    "\n",
    "\n",
    "# ── Command-line helper ───────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Run an Intron-Exon experiment from a CSV spec.\")\n",
    "    parser.add_argument(\"csv\", help=\"Path to experiment CSV file\")\n",
    "    parser.add_argument(\"name\", help=\"Experiment folder name (under Models/)\")\n",
    "    parser.add_argument(\"--no-resume\", action=\"store_true\", help=\"Start from scratch, ignore existing checkpoints\")\n",
    "    parser.add_argument(\"--batch\", type=int, default=DEFAULT_BATCH_SIZE, help=\"Batch size\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    handler = ExperimentHandler(\n",
    "        csv_path=args.csv,\n",
    "        experiment_name=args.name,\n",
    "        resume=not args.no_resume,\n",
    "        batch_size=args.batch,\n",
    "    )\n",
    "    handler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  External call                                                        │\n",
    "│  best_hp, tuner = run_tuning(data_spec, project_name=\"exp01\")         │\n",
    "└────────────┬──────────────────────────────────────────────────────────┘\n",
    "             │\n",
    "             ▼\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  run_tuning()                                                         │\n",
    "│  ├─ _prepare_datasets()         →  Data_Functions.prep_dataset…       │\n",
    "│  │                                returns (train_ds, val_ds)          │\n",
    "│  ├─ DCNNHyperModel(data_spec)                                         │\n",
    "│  │    │                                                               │\n",
    "│  │    └─ build(hp)                                                    │\n",
    "│  │         ├─ hp.* search-space (lr, dilation_mult, attention …)      │\n",
    "│  │         ├─ Custom_Models.create_modular_dcnn_model()               │\n",
    "│  │         └─ model.compile( CustomBinaryFocalLoss , metrics )        │\n",
    "│  ├─ kt.Hyperband( … directory=Models/<proj>/tuner/ )                  │\n",
    "│  ├─ tuner.search(train_ds, val_ds, callbacks=[EarlyStopping])         │\n",
    "│  ├─ best_hp = tuner.get_best_hyperparameters(1)[0]                    │\n",
    "│  └─ _save_best(best_hp, project_name) → best_hparams.json             │\n",
    "└────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "   ▲ load_best_hparams(project_name)  ←────────────┘ (JSON helper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter_Tuning.py\n",
    "\"\"\"\n",
    "Keras-Tuner wrapper for the Intron-Exon project.\n",
    "\n",
    "Usage\n",
    "-----\n",
    ">>> from IEModules import Hyperparameter_Tuning as hpt\n",
    ">>> best_hp, tuner = hpt.run_tuning(\n",
    "...     data_spec=row_from_csv,\n",
    "...     project_name=\"exp01_hyperband\"\n",
    "... )\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, os, datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import (\n",
    "    Custom_Models,\n",
    "    Custom_Losses,\n",
    "    Data_Functions,\n",
    "    Custom_Metrics,\n",
    "    config\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. HyperModel definition\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class DCNNHyperModel(kt.HyperModel):\n",
    "    \"\"\"Wraps create_modular_dcnn_model with a tunable search-space.\"\"\"\n",
    "    def __init__(self, data_spec: Dict):\n",
    "        self.data_spec = data_spec          # row from your experiment CSV\n",
    "\n",
    "    # --- mandatory -----------------------------------------------------------\n",
    "    def build(self, hp: kt.HyperParameters) -> Model:\n",
    "        # Search-space examples — extend freely.\n",
    "        dilation_mult = hp.Float(\"dilation_mult\", 0.5, 2.0, step=0.25)\n",
    "        use_attention = hp.Boolean(\"attention\", default=True)\n",
    "        lr            = hp.Float(\"lr\", 1e-4, 3e-3, sampling=\"log\")\n",
    "\n",
    "        model = Custom_Models.create_modular_dcnn_model(\n",
    "            dilation_multiplier=dilation_mult,\n",
    "            use_local_attention=use_attention,\n",
    "            use_long_range_attention=use_attention,\n",
    "            use_final_attention=use_attention,\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=lr),\n",
    "            loss      = Custom_Losses.CustomBinaryFocalLoss(),\n",
    "            metrics   = [Custom_Metrics.CustomNoBackgroundF1Score(num_classes=5)],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # --- optional ------------------------------------------------------------\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        \"\"\"You can override to inject custom callbacks—or just omit.\"\"\"\n",
    "        return super().fit(hp, model, *args, **kwargs)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Helper: get train/val datasets exactly once per tuner process\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _prepare_datasets(data_spec: Dict, batch_size=32):\n",
    "    tfrecord_path = Path(config.DATA_DIR) / \"YOUR_DEFAULT.tfrecord.gz\"\n",
    "    # TODO - replace -- use data_spec to choose correct file\n",
    "    ds = Data_Functions.prep_dataset_from_tfrecord(\n",
    "        tfrecord_path,\n",
    "        batch_size=batch_size,\n",
    "        shuffled=True,\n",
    "        cut_background=not bool(data_spec[\"Background\"])\n",
    "    )\n",
    "    # simplistic 80-20 split\n",
    "    n = ds.cardinality().numpy()\n",
    "    train_ds = ds.take(int(n*0.8))\n",
    "    val_ds   = ds.skip(int(n*0.8))\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Public API\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def run_tuning(\n",
    "    data_spec: Dict,\n",
    "    project_name: str,\n",
    "    max_trials: int = 30,\n",
    "    overwrite: bool = False,\n",
    ") -> Tuple[kt.HyperParameters, kt.Tuner]:\n",
    "\n",
    "    # Materialise datasets *once* so every trial re-uses the same TF graph\n",
    "    train_ds, val_ds = _prepare_datasets(data_spec)\n",
    "\n",
    "    hypermodel = DCNNHyperModel(data_spec)\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel,\n",
    "        objective   = kt.Objective(\"val_no_background_f1\", direction=\"max\"),\n",
    "        max_epochs  = 15,\n",
    "        factor      = 3,\n",
    "        directory   = str(Path(config.MODEL_DIR) / project_name),\n",
    "        project_name= \"tuner\",\n",
    "        overwrite   = overwrite\n",
    "    )\n",
    "\n",
    "    stop_early = callbacks.EarlyStopping(\n",
    "        monitor=\"val_no_background_f1\",\n",
    "        mode=\"max\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        train_ds,\n",
    "        validation_data = val_ds,\n",
    "        callbacks       = [stop_early],\n",
    "    )\n",
    "\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    _save_best(best_hp, project_name)\n",
    "\n",
    "    return best_hp, tuner\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Convenience helpers\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _save_best(best_hp: kt.HyperParameters, project_name: str):\n",
    "    out = Path(config.MODEL_DIR) / project_name / \"best_hparams.json\"\n",
    "    with open(out, \"w\") as fh:\n",
    "        json.dump(best_hp.values, fh, indent=2)\n",
    "    print(f\"Best HP written to {out}\")\n",
    "\n",
    "def load_best_hparams(project_name: str) -> Dict:\n",
    "    path = Path(config.MODEL_DIR) / project_name / \"best_hparams.json\"\n",
    "    with open(path) as fh:\n",
    "        return json.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anywhere - CLI, notebook, or even inside ExperimentHandler\n",
    "from IEModules.Hyperparameter_Tuning import run_tuning, load_best_hparams\n",
    "\n",
    "best_hp, tuner = run_tuning(\n",
    "    data_spec   = row_from_csv_as_dict,\n",
    "    project_name= \"exp01_hyperband\"\n",
    ")\n",
    "print(\"Best LR  :\", best_hp['lr'])\n",
    "print(\"Dilation :\", best_hp['dilation_mult'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
