{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "Complete Binary, w/ background - sizes 1 and 10 and highly mixed 5\n",
    "Complete Binary, no background - sizes 1 and 10 and incremental 5\n",
    "Custom Smoothing, w/ background - sizes 1 and 10 - This is the baseline\n",
    "Custom Smoothing, no background - sizes 1 and 10\n",
    "\n",
    "\n",
    "Loss settings:\n",
    "Custom Smoothing, enabled - Loss A -This is the baseline\n",
    "Custom Smoothing, disabled - Loss A\n",
    "No Smoothing, disabled - Loss A or B \n",
    "Proper Smoothing, disabled - Loss B\n",
    "\n",
    "\n",
    "Model Types: \n",
    "Standard Dilation, No Attention - This is the baseline\n",
    "Standard Dilation, Attention\n",
    "Reduced Dilation, No Attention\n",
    "Reduced Dilation, Attention\n",
    "\n",
    "Todo: Need to make a reduced dilation version of the model - done, model takes dilation scaler\n",
    "Confirm that if no smoothing and early reward disabled, Loss A and Loss B are equivalent - confirmed, so long as parameters and thresholds are the same\n",
    "\n",
    "Checklist of datasets:\n",
    "\n",
    " Complete Binary, w/ background\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10\n",
    "\n",
    " Highly mixed 5\n",
    "\n",
    "\n",
    " Complete Binary, no background\n",
    "\n",
    " Size 1\n",
    "\n",
    " Size 10\n",
    "\n",
    " Incremental 5 - have a custom smoothed version w/ background that can be converted\n",
    "\n",
    " Custom Smoothing, w/ background (This is the baseline)\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Custom Smoothing, no background\n",
    "\n",
    " Size 1\n",
    "\n",
    " Size 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 21:51:14.932147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-12 21:51:15.028668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-12 21:51:15.056663: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-12 21:51:15.232283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-12 21:51:16.827781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IEModules as iem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered Howdy! into test_function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Howdy!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.test_function(\"Howdy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "_bytes_feature\n",
      "_float_feature\n",
      "_int64_feature\n",
      "build_chunk_data_for_indices\n",
      "build_dataset_from_tfrecords\n",
      "bytes_feature\n",
      "calculate_introns\n",
      "compute_chunk_indices\n",
      "convert_and_write_tfrecord\n",
      "convert_labels_to_binary\n",
      "float_feature_list\n",
      "int_feature_list\n",
      "label_sequence_local\n",
      "load_gtf_annotations\n",
      "main\n",
      "one_hot_encode_reference\n",
      "pad_encoded_seq\n",
      "pad_labels\n",
      "parse_chunk_example\n",
      "search_gtf_by_range\n",
      "serialize_chunk_example\n",
      "serialize_example_with_metadata_no_convert\n",
      "split_tfrecords\n",
      "stream_shuffled_records\n",
      "swap_columns_if_needed\n",
      "test_dataset_from_tfrecords\n",
      "test_function\n",
      "trim_chr_genome\n",
      "tvt_split_tfrecords\n",
      "write_shuffled_records_to_single_tfrecord\n",
      "write_tfrecord_in_shards_hybrid\n",
      "write_to_shard_with_threads\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "functions = inspect.getmembers(iem.Genetic_Data_Pipeline, inspect.isfunction)\n",
    "for name, func in functions:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Genetic_Data_Pipeline for correctness\n",
    "\n",
    "Check Custom_Models for correctness\n",
    "\n",
    "Need to make a shard shuffler module probably that goes from output of genetic_data_pipeline to fully shuffled test-train-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          1. build_initial_shards function generates 4 tfrecord shards at each shift fed to it.\n",
      "          I recommend giving a list of 1 window shift.  Currently paths are hardcoded.  \n",
      "          The first 4*n characters of the shards will be the shift numbers where n is \n",
      "          the number of shifts fed to the function.\n",
      "          \n",
      "          2. split_tfrecords takes an input and output directory and number of file splits\n",
      "          and splits each TFRecord in the input_directory (assumed to be gzipped TFRecords)\n",
      "          into 'num_splits' smaller TFRecords.\n",
      "          \n",
      "          3. write_shuffled_records_to_single_tfrecord builds a single big, shuffled, \n",
      "          gzip-compressed TFRecord file \n",
      "          \n",
      "          Args:\n",
      "            input_dir (str): Directory containing the source TFRecord files.\n",
      "            allowed_indices (list): List of allowed starting indices.\n",
      "            output_filepath (str): Full path to the output TFRecord file.\n",
      "            \n",
      "          4. tvt_split_records test, validate, train splits the big tfrecord into \n",
      "          test, validate, and train datasets\n",
      "          \n",
      "          5. convert_and_write_tfrecord writes a binary only version of the dataset fed to it\n",
      "          \n",
      "          6. Removing background happens when loading the data into the model\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "iem.Genetic_Data_Pipeline.dataset_pipeline_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist of datasets:\n",
    "\n",
    " Complete Binary, w/ background\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Highly mixed 5                                 Need to build highly mixed 5 and convert to binary\n",
    "\n",
    "\n",
    " Complete Binary, no background\n",
    "\n",
    " Size 1 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Size 10 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Incremental 5 - have a custom smoothed version w/ background that can be converted   conversion ongoing\n",
    "\n",
    " Custom Smoothing, w/ background (This is the baseline)\n",
    "\n",
    " Size 1 - COMPLETE\n",
    "\n",
    " Size 10 - COMPLETE\n",
    "\n",
    " Custom Smoothing, no background\n",
    "\n",
    " Size 1 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    " Size 10 - background removed when loading into model (Data_Functions.py)\n",
    "\n",
    "\n",
    "\n",
    " Size 10 has 2365261 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744095490.509284 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.699680 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.699799 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704425 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704488 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.704525 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.994540 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744095490.994740 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-08 00:58:10.994829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744095490.995069 1330704 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-08 00:58:10.995990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-08 09:35:11.668060: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/all_ten_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_ten_shuffled.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/fifths_incremental_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_fifths_incrememntal.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder created at: ../../Datasets/AugDataSets/AllTen/temp_shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744246104.885834  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.075605  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.075745  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078746  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078828  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.078872  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.373822  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744246105.373927  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-09 18:48:25.373940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744246105.374015  547085 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-09 18:48:25.374434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-09 19:01:38.755912: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records found in input dataset: 2365261\n",
      "Sampling 1182630 records (fraction = 0.5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 19:14:36.772563: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split all_ten_shuffled.tfrecord.gz into 45 shards.\n",
      "10000 records written so far...\n",
      "20000 records written so far...\n",
      "30000 records written so far...\n",
      "40000 records written so far...\n",
      "50000 records written so far...\n",
      "60000 records written so far...\n",
      "70000 records written so far...\n",
      "80000 records written so far...\n",
      "90000 records written so far...\n",
      "100000 records written so far...\n",
      "110000 records written so far...\n",
      "120000 records written so far...\n",
      "130000 records written so far...\n",
      "140000 records written so far...\n",
      "150000 records written so far...\n",
      "160000 records written so far...\n",
      "170000 records written so far...\n",
      "180000 records written so far...\n",
      "190000 records written so far...\n",
      "200000 records written so far...\n",
      "210000 records written so far...\n",
      "220000 records written so far...\n",
      "230000 records written so far...\n",
      "240000 records written so far...\n",
      "250000 records written so far...\n",
      "260000 records written so far...\n",
      "270000 records written so far...\n",
      "280000 records written so far...\n",
      "290000 records written so far...\n",
      "300000 records written so far...\n",
      "310000 records written so far...\n",
      "320000 records written so far...\n",
      "330000 records written so far...\n",
      "340000 records written so far...\n",
      "350000 records written so far...\n",
      "360000 records written so far...\n",
      "370000 records written so far...\n",
      "380000 records written so far...\n",
      "390000 records written so far...\n",
      "400000 records written so far...\n",
      "410000 records written so far...\n",
      "420000 records written so far...\n",
      "430000 records written so far...\n",
      "440000 records written so far...\n",
      "450000 records written so far...\n",
      "460000 records written so far...\n",
      "470000 records written so far...\n",
      "480000 records written so far...\n",
      "490000 records written so far...\n",
      "500000 records written so far...\n",
      "510000 records written so far...\n",
      "520000 records written so far...\n",
      "530000 records written so far...\n",
      "540000 records written so far...\n",
      "550000 records written so far...\n",
      "560000 records written so far...\n",
      "570000 records written so far...\n",
      "580000 records written so far...\n",
      "590000 records written so far...\n",
      "600000 records written so far...\n",
      "610000 records written so far...\n",
      "620000 records written so far...\n",
      "630000 records written so far...\n",
      "640000 records written so far...\n",
      "650000 records written so far...\n",
      "660000 records written so far...\n",
      "670000 records written so far...\n",
      "680000 records written so far...\n",
      "690000 records written so far...\n",
      "700000 records written so far...\n",
      "710000 records written so far...\n",
      "720000 records written so far...\n",
      "730000 records written so far...\n",
      "740000 records written so far...\n",
      "750000 records written so far...\n",
      "760000 records written so far...\n",
      "770000 records written so far...\n",
      "780000 records written so far...\n",
      "790000 records written so far...\n",
      "800000 records written so far...\n",
      "810000 records written so far...\n",
      "820000 records written so far...\n",
      "830000 records written so far...\n",
      "840000 records written so far...\n",
      "850000 records written so far...\n",
      "860000 records written so far...\n",
      "870000 records written so far...\n",
      "880000 records written so far...\n",
      "890000 records written so far...\n",
      "900000 records written so far...\n",
      "910000 records written so far...\n",
      "920000 records written so far...\n",
      "930000 records written so far...\n",
      "940000 records written so far...\n",
      "950000 records written so far...\n",
      "960000 records written so far...\n",
      "970000 records written so far...\n",
      "980000 records written so far...\n",
      "990000 records written so far...\n",
      "1000000 records written so far...\n",
      "1010000 records written so far...\n",
      "1020000 records written so far...\n",
      "1030000 records written so far...\n",
      "1040000 records written so far...\n",
      "1050000 records written so far...\n",
      "1060000 records written so far...\n",
      "1070000 records written so far...\n",
      "1080000 records written so far...\n",
      "1090000 records written so far...\n",
      "1100000 records written so far...\n",
      "1110000 records written so far...\n",
      "1120000 records written so far...\n",
      "1130000 records written so far...\n",
      "1140000 records written so far...\n",
      "1150000 records written so far...\n",
      "1160000 records written so far...\n",
      "1170000 records written so far...\n",
      "1180000 records written so far...\n",
      "Finished writing 1182630 records to ../../Datasets/AugDataSets/50_all_ten_shuffled.tfrecord.gz\n",
      "Temporary shards have been removed.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.transform_tfdataset(datasets_path+\"AugDataSets/AllTen\", datasets_path+\"AugDataSets\", 45, 0.5)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 04:28:15.601091: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# iem.Genetic_Data_Pipeline.convert_and_write_tfrecord(datasets_path+\"AugDataSets/50_all_ten_shuffled.tfrecord.gz\", datasets_path+\"AugDataSets/binary_5x_mixed.tfrecord.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset New Naming Scheme: \n",
    "Epoch Units: 01-10, Style: M for Mixed or I for Incremental, Smoothing: C for custom smoothing, B for Binary\n",
    "Epoch-Units_Style_Smoothing_list-of-shifts-if-incremental-divided-by-1000_IEData.tfrecord.gz\n",
    "ex: 02_I_B_0-2.5_IEData.tfrecord.gz\n",
    "\n",
    "Per line, this is input from trial csv:\n",
    "Trial,Smoothing,Background,Early Rewarding,Size,Style,Dilation,Attention\n",
    "\n",
    "Trial becomes folder where model data are saved\n",
    "\n",
    "Smoothing, Size, Style define the dataset\n",
    "\n",
    "Background is a choice handled in the data loader which returns a dataset generator pretending to be a dataset (Data_Functions.prep_dataset_from_tfrecord())\n",
    "\n",
    "Early rewarding: if true, need an option in the loss functions or something that checks epoch number.  Rewarding==True for more than 3 epoch units is overkill so the switch should flip after 3 epoch units\n",
    "\n",
    "Dilation and Attention are model hyperparameters\n",
    "\n",
    "What I need then:\n",
    "Experiment handler function (class?) that reads a line from the csv and trains for 10 epoch units.  Needs a working folder given to it (Experiment Name is probably the folder name) and a subfolder for each trial (Trial_01, etc).  Checkpoints get saved in there, as does the model history which gets saved to json every epoch unit but overwritten, as should the StatefulReduceLROnPlateau callback state that I have yet to implement.\n",
    "The experiment should track which trial and which epoch was most recently saved and be able to resume from that trial and checkpoint easily.  Maybe have it save a jpg of the train validate curve at the end of the trial using the function in Helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744516349.562100 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.758718 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.758835 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762785 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762879 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516349.762919 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516350.064039 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1744516350.064137 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-12 21:52:30.064148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744516350.064212 1396344 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-12 21:52:30.065583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2025-04-12 21:58:43.898655: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records found: 1182630\n",
      "Splitting into -> Train: 946104, Val: 118263, Test: 118263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 21:58:53.921334: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:6: Filling up shuffle buffer (this may take a while): 22613 of 25000\n",
      "2025-04-12 21:58:55.008046: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "2025-04-12 22:51:29.014481: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Split Counts -> Train: 946104, Val: 118263, Test: 118263\n",
      "05_M_B_IEData.tfrecord.gz split\n",
      "Total records found: 2365261\n",
      "Splitting into -> Train: 1892208, Val: 236526, Test: 236527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 23:04:33.559500: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:15: Filling up shuffle buffer (this may take a while): 20358 of 25000\n",
      "2025-04-12 23:04:35.240383: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Split Counts -> Train: 1892208, Val: 236526, Test: 236527\n",
      "10_M_C_IEData.tfrecord.gz split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:55:15.264840: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "data_directory = datasets_path+\"Experiment 01/\"\n",
    "save_directory = data_directory+\"TestValTrain/\"\n",
    "for root, dirs, files in os.walk(data_directory):\n",
    "    if 'TestValTrain' in dirs:\n",
    "        dirs.remove('TestValTrain')\n",
    "    for file in files:\n",
    "        file_path = data_directory+os.path.basename(file)\n",
    "        train_save_file_path = save_directory+\"Train_\"+os.path.basename(file)\n",
    "        val_save_file_path = save_directory+\"Val_\"+os.path.basename(file)\n",
    "        test_save_file_path = save_directory+\"Test_\"+os.path.basename(file)\n",
    "        iem.Genetic_Data_Pipeline.tvt_split_tfrecords(file_path, train_save_file_path, val_save_file_path,test_save_file_path)\n",
    "        print(os.path.basename(file)+\" split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# \"\"\"\n",
    "# Experiment Handler Module\n",
    "\n",
    "# This module defines an ExperimentHandler class that:\n",
    "#   • Reads a single trial’s configuration (with keys: Trial, Smoothing, Background,\n",
    "#     Early Rewarding, Size, Style, Dilation, Attention) from a CSV file.\n",
    "#   • Creates an experiment folder under a common experiment root for each trial.\n",
    "#   • Loads a dataset from a tfrecord using a constructed filename based on the configuration.\n",
    "#   • Builds a model via a provided model builder or a default placeholder.\n",
    "#   • Trains the model for 10 \"epoch units\" (one epoch per unit in this example).\n",
    "#   • Saves checkpoints, training history (using Helper_Functions.save_history_to_json),\n",
    "#     and a train/validation curve plot (using Helper_Functions.plot_train_val_curve) after each epoch unit.\n",
    "#   • Supports resuming progress from a saved state.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# import time\n",
    "# import sys\n",
    "# import os\n",
    "# import glob\n",
    "# import math\n",
    "# import threading\n",
    "# import concurrent.futures as cf\n",
    "# import random\n",
    "# import re\n",
    "# import json\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "# from keras import backend as K\n",
    "# import gc\n",
    "# import keras_tuner as kt\n",
    "# from pyfaidx import Fasta\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# datasets_path = \"../../Datasets/\"\n",
    "# models_path = \"../../Models/\"\n",
    "# import IEModules as iem\n",
    "\n",
    "\n",
    "# # Optionally import custom callbacks from Custom_Callbacks if available\n",
    "# # from Custom_Callbacks import StatefulReduceLROnPlateau  # placeholder for stateful LR scheduling\n",
    "\n",
    "# class ExperimentHandler:\n",
    "#     def __init__(self, trial_config, experiment_root=\"ExperimentResults\", model_builder=None):\n",
    "#         \"\"\"\n",
    "#         Initialize experiment for a single trial.\n",
    "        \n",
    "#         Parameters:\n",
    "#           trial_config (dict): Dictionary with keys:\n",
    "#              \"Trial\", \"Smoothing\", \"Background\", \"Early Rewarding\", \"Size\", \"Style\", \"Dilation\", \"Attention\"\n",
    "#           experiment_root (str): The root directory for experiment results.\n",
    "#           model_builder (callable, optional): Function that builds and compiles the model.\n",
    "#                Defaults to a simple placeholder model.\n",
    "#         \"\"\"\n",
    "#         self.trial_config = trial_config\n",
    "#         self.total_epoch_units = 10\n",
    "#         self.current_epoch_unit = 0\n",
    "#         self.model = None\n",
    "#         self.model_builder = model_builder if model_builder is not None else self.default_model_builder\n",
    "\n",
    "#         # Create a subfolder for the trial using the \"Trial\" field under the experiment root.\n",
    "#         self.trial_folder = os.path.join(experiment_root, self.trial_config[\"Trial\"])\n",
    "#         os.makedirs(self.trial_folder, exist_ok=True)\n",
    "#         self.state_file = os.path.join(self.trial_folder, \"state.json\")\n",
    "#         self.dataset = None\n",
    "\n",
    "#     def default_model_builder(self):\n",
    "#         \"\"\"\n",
    "#         Build a default placeholder model.\n",
    "#         Replace this with a call to your custom model builder (e.g., Custom_Models.create_dcnn_model).\n",
    "#         \"\"\"\n",
    "#         model = models.Sequential([\n",
    "#             layers.InputLayer(input_shape=(5000, 5)),\n",
    "#             layers.Conv1D(32, 3, activation='relu', padding='same'),\n",
    "#             layers.GlobalAveragePooling1D(),\n",
    "#             layers.Dense(5, activation='sigmoid')\n",
    "#         ])\n",
    "#         model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"f1_score\"])\n",
    "#         return model\n",
    "\n",
    "#     def load_dataset(self):\n",
    "#         \"\"\"\n",
    "#         Constructs the tfrecord filename based on trial configuration and loads the dataset.\n",
    "#         Modify the logic below to reflect your specific naming convention.\n",
    "#         \"\"\"\n",
    "#         # For example: \"Epoch-Units_Style_Smoothing_list-of-shifts-if-incremental-divided-by-1000_IEData.tfrecord.gz\"\n",
    "#         epoch_unit_placeholder = \"XX\"  # Placeholder: replace or update with epoch unit if needed.\n",
    "#         style = self.trial_config[\"Style\"]  # e.g., \"M\" or \"I\"\n",
    "#         smoothing = self.trial_config[\"Smoothing\"]  # e.g., \"C\" (custom) or \"B\" (binary)\n",
    "#         # For incremental style, include shifts (placeholder here as \"0-2.5\")\n",
    "#         shifts = \"0-2.5\" if style.upper() == \"I\" else \"\"\n",
    "#         file_name = f\"{epoch_unit_placeholder}_{style}_{smoothing}_{shifts}_IEData.tfrecord.gz\"\n",
    "#         # Adjust the base path for your tfrecords:\n",
    "#         tfrecord_path = os.path.join(\"path_to_tfrecords\", file_name)\n",
    "        \n",
    "#         # Determine if the Background should be removed (interpreting string to boolean).\n",
    "#         cut_background = str(self.trial_config[\"Background\"]).lower() in [\"true\", \"1\"]\n",
    "#         # Use the \"Size\" field for the batch size.\n",
    "#         batch_size = int(self.trial_config[\"Size\"])\n",
    "#         self.dataset = iem.Data_Functions.prep_dataset_from_tfrecord(\n",
    "#             tfrecord_path,\n",
    "#             batch_size=batch_size,\n",
    "#             shuffled=True,\n",
    "#             cut_background=cut_background\n",
    "#         )\n",
    "    \n",
    "#     def resume_progress(self):\n",
    "#         \"\"\"\n",
    "#         Resumes from a previously saved state if it exists.\n",
    "#         Loads the current epoch unit and last saved checkpoint.\n",
    "#         \"\"\"\n",
    "#         if os.path.exists(self.state_file):\n",
    "#             with open(self.state_file, \"r\") as f:\n",
    "#                 state = json.load(f)\n",
    "#             self.current_epoch_unit = state.get(\"current_epoch_unit\", 0)\n",
    "#             last_checkpoint = state.get(\"last_checkpoint\", None)\n",
    "#             if last_checkpoint and os.path.exists(last_checkpoint):\n",
    "#                 self.model = models.load_model(last_checkpoint, compile=False)\n",
    "#             print(f\"Resumed trial {self.trial_config['Trial']} at epoch unit {self.current_epoch_unit}\")\n",
    "#         else:\n",
    "#             print(f\"Starting new trial {self.trial_config['Trial']}\")\n",
    "    \n",
    "#     def save_state(self, checkpoint_path):\n",
    "#         \"\"\"\n",
    "#         Saves the current experiment state to allow resuming later.\n",
    "#         \"\"\"\n",
    "#         state = {\n",
    "#             \"current_epoch_unit\": self.current_epoch_unit,\n",
    "#             \"last_checkpoint\": checkpoint_path\n",
    "#         }\n",
    "#         with open(self.state_file, \"w\") as f:\n",
    "#             json.dump(state, f)\n",
    "#         print(f\"Saved state for trial {self.trial_config['Trial']} to {self.state_file}\")\n",
    "\n",
    "#     def train_epoch_unit(self):\n",
    "#         \"\"\"\n",
    "#         Trains the model for one epoch unit.\n",
    "#         Implements early rewarding logic (active only for the first 3 epoch units),\n",
    "#         saves a checkpoint, training history, and a plot of the train/validation curve.\n",
    "#         \"\"\"\n",
    "#         checkpoint_path = os.path.join(self.trial_folder, f\"checkpoint_epochunit_{self.current_epoch_unit+1}.h5\")\n",
    "#         cp_callback = callbacks.ModelCheckpoint(\n",
    "#             filepath=checkpoint_path,\n",
    "#             save_weights_only=False,\n",
    "#             save_freq=\"epoch\",\n",
    "#             verbose=1\n",
    "#         )\n",
    "#         callbacks_list = [cp_callback]\n",
    "        \n",
    "#         # Early Rewarding: Only effective if set in the config and for the first 3 epoch units.\n",
    "#         early_reward = str(self.trial_config[\"Early Rewarding\"]).lower() in [\"true\", \"1\"]\n",
    "#         if early_reward and self.current_epoch_unit >= 3:\n",
    "#             early_reward = False\n",
    "#         print(f\"Trial {self.trial_config['Trial']}: Early Rewarding set to {early_reward} at epoch unit {self.current_epoch_unit+1}\")\n",
    "        \n",
    "#         # Train for one epoch unit. (Here, one epoch = one epoch unit.)\n",
    "#         history = self.model.fit(\n",
    "#             self.dataset,\n",
    "#             epochs=1,\n",
    "#             callbacks=callbacks_list,\n",
    "#             validation_data=self.dataset,  # Adjust for a dedicated validation dataset if available.\n",
    "#             verbose=1\n",
    "#         )\n",
    "        \n",
    "#         # Save the training history to JSON\n",
    "#         iem.Helper_Functions.save_history_to_json(history, metadata=f\"epochunit_{self.current_epoch_unit+1}\", save_path=self.trial_folder)\n",
    "        \n",
    "#         # Plot and save the training and validation curve\n",
    "#         fig = iem.Helper_Functions.plot_train_val_curve(history, training_target_variable=\"loss\")  # Modify target variable as needed.\n",
    "#         plot_path = os.path.join(self.trial_folder, f\"train_val_curve_epochunit_{self.current_epoch_unit+1}.jpg\")\n",
    "#         fig.savefig(plot_path)\n",
    "#         plt.close(fig)\n",
    "        \n",
    "#         self.current_epoch_unit += 1\n",
    "#         self.save_state(checkpoint_path)\n",
    "    \n",
    "#     def run_trial(self):\n",
    "#         \"\"\"\n",
    "#         Runs the trial by loading the dataset, initializing (or resuming) the model,\n",
    "#         and training for 10 epoch units.\n",
    "#         \"\"\"\n",
    "#         self.load_dataset()\n",
    "#         if self.model is None:\n",
    "#             self.model = self.model_builder()\n",
    "#         self.resume_progress()\n",
    "#         while self.current_epoch_unit < self.total_epoch_units:\n",
    "#             print(f\"Trial {self.trial_config['Trial']}: Starting epoch unit {self.current_epoch_unit+1}/{self.total_epoch_units}\")\n",
    "#             self.train_epoch_unit()\n",
    "#         print(f\"Trial {self.trial_config['Trial']} completed.\")\n",
    "\n",
    "# def run_trials_from_csv(csv_path):\n",
    "#     \"\"\"\n",
    "#     Reads the CSV file containing trial configurations and runs the experiment for each trial.\n",
    "    \n",
    "#     CSV file is expected to have the following columns:\n",
    "#       Trial, Smoothing, Background, Early Rewarding, Size, Style, Dilation, Attention\n",
    "#     \"\"\"\n",
    "#     df = pd.read_csv(csv_path)\n",
    "    \n",
    "#     # Define a root directory to hold all trial folders.\n",
    "#     experiment_root = \"ExperimentResults\"\n",
    "#     os.makedirs(experiment_root, exist_ok=True)\n",
    "    \n",
    "#     # Loop over each row (trial) and run the experiment.\n",
    "#     for idx, row in df.iterrows():\n",
    "#         trial_config = row.to_dict()\n",
    "#         print(f\"Starting trial: {trial_config.get('Trial', 'Unknown')}\")\n",
    "#         trial_handler = ExperimentHandler(trial_config, experiment_root=experiment_root)\n",
    "#         trial_handler.run_trial()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Update this with the path to your CSV file containing trial configurations.\n",
    "#     csv_path = \"trial_config.csv\"\n",
    "#     run_trials_from_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  User/CLI                                                             │\n",
    "│  $ python experiment_framework.py spec.csv EXP_NAME                  │\n",
    "└────────────┬──────────────────────────────────────────────────────────┘\n",
    "             │\n",
    "             ▼\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  ExperimentHandler(csv_path, experiment_name, …)                      │\n",
    "│  ├─ reads CSV into DataFrame                                          │\n",
    "│  └─ creates   <MODEL_DIR>/<EXP_NAME>/                                 │\n",
    "└────────────┬──────────────────────────────────────────────────────────┘\n",
    "             │\n",
    "             ▼\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  run()                                                                │\n",
    "│  loop rows ▸ trial_row                                                │\n",
    "│     │                                                                 │\n",
    "│     ├─ _prepare_trial_folder()    →  …/Trial_XX/ (+ checkpoints/)     │\n",
    "│     ├─ discover latest checkpoint ─┐                                  │\n",
    "│     │                             └─▶  models.load_model()   │\n",
    "│     └─ if none → _build_model()   →  create_modular_dcnn_model()      │\n",
    "│                                                                       │\n",
    "│     loop epoch-unit 0-9                                               │\n",
    "│        ├─ _load_datasets()        →  Data_Functions.prep_dataset…     │\n",
    "│        ├─ _make_callbacks()       →  {ModelCheckpoint, LR, TB…}       │\n",
    "│        └─ model.fit(epochs=1)     →  train 1 *epoch-unit*             │\n",
    "│               └─ Helper_Functions.save_history_to_json() (overwrite)  │\n",
    "│                                                                       │\n",
    "│     after 10 units                                                    │\n",
    "│        └─ _finalise_trial()                                           │\n",
    "│            ├─ plot_train_val_curve() → Trial_XX/train_val_curve.jpg   │\n",
    "│            └─ model.save(\"final_model.keras\")                         │\n",
    "└────────────────────────────────────────────────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_framework.py\n",
    "\"\"\"\n",
    "High-level experiment orchestration for the Intron-Exon project.\n",
    "\n",
    "* 1 CSV == 1 experiment folder ⤏ each row is a *trial* that runs **10 epoch-units**.\n",
    "* Handles automatic folder creation, checkpointing, history/curve saving, and resume logic.\n",
    "* Designed to work with the existing IEModules package (Data_Functions, Custom_Models, etc.).\n",
    "\n",
    "The implementation purposefully leaves a handful of 🔲 *fill-in-the-blank* areas where deeper\n",
    "project knowledge is needed.  Search for the keyword `TODO` to complete the details.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# ── In-house modules ───────────────────────────────────────────────────────────\n",
    "from IEModules import (\n",
    "    Data_Functions,\n",
    "    Custom_Models,\n",
    "    Custom_Callbacks,\n",
    "    Custom_Losses,\n",
    "    Helper_Functions,\n",
    "    Custom_Metrics\n",
    ")\n",
    "from IEModules.config import (\n",
    "    DATA_DIR, \n",
    "    MODEL_DIR, \n",
    "    LOG_DIR, \n",
    "    EPOCH_UNIT_SIZE, \n",
    "    EPOCH_UNITS_PER_TRIAL, \n",
    "    DEFAULT_BATCH_SIZE, \n",
    "    STEPS_PER_EPOCH_UNIT, \n",
    "    experiment_data_folder,\n",
    "    experiment_folder,\n",
    "    LR_STATE_SAVE_SUBDIR,\n",
    "    LR_STATE_SAVE_FILENAME,\n",
    "    DOMINANT_CLASS_INDEX,\n",
    "    DOMINANT_CORRECT_MULTIPLIER,\n",
    "    DOMINANT_INCORRECT_MULTIPLIER,\n",
    "    OTHER_TP_MULTIPLIER,\n",
    "    OTHER_FN_MULTIPLIER,\n",
    "    OTHER_FP_MULTIPLIER,\n",
    "    OTHER_TN_MULTIPLIER,\n",
    "    THRESHOLD,\n",
    "    FOCAL_GAMMA,\n",
    "    FOCAL_ALPHA,\n",
    "    CORRECT_SMOOTHING_MULTIPLIER,\n",
    "    INCORRECT_SMOOTHING_MULTIPLIER,\n",
    "    DEFAULT_SMOOTHING_AS_CORRECT,\n",
    "    LABEL_SMOOTHING,\n",
    "    SWAP_EPOCH,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Dataset naming helpers -------------------------------------------------------\n",
    "\n",
    "def build_dataset_filepath(\n",
    "    split: str,              # 'train' | 'val' | 'test'\n",
    "    units: int,              # 1-10  (will be zero-padded to 02)\n",
    "    style_flag: str,         # 'M' or 'I'\n",
    "    smoothing_flag: str,     # 'C' or 'B'\n",
    "    base_data_dir: str | Path = experiment_data_folder # Modify in config\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return the *full* path to the TFRecord whose filename starts with\n",
    "    '{split}_{units:02d}_{style_flag}_{smoothing_flag}' (case-insensitive).\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError - no matching file in base_data_dir\n",
    "        RuntimeError      - more than one match (ambiguous)\n",
    "    \"\"\"\n",
    "    split   = split.lower()\n",
    "    prefix  = f\"{split}_{units:02d}_{style_flag.upper()}_{smoothing_flag.upper()}\"\n",
    "    pattern = f\"{prefix}*IEData.tfrecord.gz\"        # wildcard swallows shift part\n",
    "    matches = glob.glob(str(Path(base_data_dir) / pattern), recursive=False)\n",
    "\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No TFRecord found for prefix '{prefix}' in {base_data_dir}\")\n",
    "    if len(matches) > 1:\n",
    "        raise RuntimeError(f\"Ambiguous prefix '{prefix}': {matches}\")\n",
    "\n",
    "    return matches[0]   # the unique match\n",
    "\n",
    "\n",
    "\n",
    "# ── Utility functions ─────────────────────────────────────────────────────────\n",
    "\n",
    "def latest_checkpoint(ckpt_dir: Path) -> Optional[Path]:\n",
    "    \"\"\"Return the most recent .keras checkpoint inside *ckpt_dir* (None if absent).\"\"\"\n",
    "    checkpoints = sorted(ckpt_dir.glob(\"*.keras\"))\n",
    "    return checkpoints[-1] if checkpoints else None\n",
    "\n",
    "\n",
    "def parse_epoch_from_ckpt(ckpt_path: Path) -> int:\n",
    "    \"\"\"Extract integer epoch-unit from a filename like 'epoch-05-val_metric-0.1234.keras'.\"\"\"\n",
    "    m = re.search(r\"epoch[-_](\\d+)\", ckpt_path.name)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "\n",
    "# ── Main class ────────────────────────────────────────────────────────────────\n",
    "\n",
    "class ExperimentHandler:\n",
    "    \"\"\"Drive the end-to-end training process described by a CSV file.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Path | str, # The csv with trial parameters\n",
    "        experiment_name: str,\n",
    "        model_dir: Path | str = MODEL_DIR,\n",
    "        data_folder: Path | str = experiment_data_folder,  \n",
    "        resume: bool = True,\n",
    "        batch_size: int = DEFAULT_BATCH_SIZE,\n",
    "    ) -> None:\n",
    "        self.csv_path      = Path(csv_path)\n",
    "        self.experiment    = experiment_name\n",
    "        self.batch_size    = batch_size\n",
    "        self.resume_flag   = resume\n",
    "        self.experiment_dir = Path(model_dir) / experiment_name\n",
    "        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.trials_df = pd.read_csv(self.csv_path)\n",
    "        # Normalise column names just in case\n",
    "        self.trials_df.columns = [c.strip() for c in self.trials_df.columns]\n",
    "        self.base_data_dir = data_folder\n",
    "\n",
    "    # ── Public API ──────────────────────────────────────────────────────────\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"Loop over trials and epoch-units, honouring resume logic.\"\"\"\n",
    "        for row_idx, trial_row in self.trials_df.iterrows():\n",
    "            trial_id  = int(trial_row[\"Trial\"])  # e.g. 1 ➜ folder Trial_01\n",
    "            trial_dir = self._prepare_trial_folder(trial_id)\n",
    "\n",
    "            # ── Determine starting point ---------------------------------------------------\n",
    "            ckpt_dir = trial_dir / \"checkpoints\"\n",
    "            ckpt_dir.mkdir(exist_ok=True)\n",
    "            start_unit = 0\n",
    "            model = None\n",
    "            if self.resume_flag:\n",
    "                latest_ckpt = latest_checkpoint(ckpt_dir)\n",
    "                if latest_ckpt is not None:\n",
    "                    start_unit = parse_epoch_from_ckpt(latest_ckpt) + 1\n",
    "                    model = models.load_model(latest_ckpt, compile=False)\n",
    "\n",
    "            # ── Build model from scratch if needed ----------------------------------------\n",
    "            if model is None:\n",
    "                model = self._build_model(trial_row)\n",
    "\n",
    "            # ── Training loop over *epoch units* -----------------------------------------\n",
    "            for unit in range(start_unit, EPOCH_UNITS_PER_TRIAL):\n",
    "                print(f\"\\nTrial {trial_id:02d}  •  Epoch-Unit {unit+1}/{EPOCH_UNITS_PER_TRIAL}\")\n",
    "\n",
    "                train_ds, val_ds = self._load_datasets(trial_row)\n",
    "                \n",
    "                '''#################### GOTTA UPDATE THIS FOR CONFIG ##############'''\n",
    "                callbacks = self._make_callbacks(trial_row, unit)\n",
    "                lr_state_path = experiment_folder / LR_STATE_SAVE_SUBDIR / LR_STATE_SAVE_FILENAME\n",
    "                if Path(lr_state_path).exists:\n",
    "                    Custom_Callbacks.reduce_lr_cb.load_state_from_file(lr_state_path)\n",
    "\n",
    "                \n",
    "                history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=EPOCH_UNITS_PER_TRIAL,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH_UNIT,\n",
    "                    callbacks=callbacks,\n",
    "                )\n",
    "\n",
    "                # Overwrite history JSON each unit (requirement)\n",
    "                Helper_Functions.save_history_to_json(\n",
    "                    history,\n",
    "                    metadata=\"history\",  # single file always named history_*.json\n",
    "                    save_path=str(trial_dir),\n",
    "                )\n",
    "\n",
    "            # ── End-of-trial artifacts ----------------------------------------------------\n",
    "            self._finalise_trial(model, trial_dir)\n",
    "\n",
    "        print(\"\\n✅ All trials complete!\")\n",
    "\n",
    "    # ── Internals ───────────────────────────────────────────────────────────\n",
    "\n",
    "    def _prepare_trial_folder(self, trial_num: int) -> Path:\n",
    "        trial_dir = self.experiment_dir / f\"Trial_{trial_num:02d}\"\n",
    "        trial_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (trial_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
    "        return trial_dir\n",
    "\n",
    "    \n",
    "    def _dataset_path_from_row(self, row: pd.Series, split: str, base_data_dir: str) -> str:\n",
    "        \"\"\"\n",
    "        Given a CSV row and a split ('train', 'val', 'test'),\n",
    "        return the full path to the TFRecord.\n",
    "        \"\"\"\n",
    "        units_flag     = int(row[\"Size\"])                       # 1-10\n",
    "        style_flag     = str(row[\"Style\"]).upper()[0]           # 'M' | 'I'\n",
    "        smoothing_flag = (\n",
    "            \"C\" if str(row[\"Smoothing\"]).upper().startswith(\"C\") else \"B\"\n",
    "        )\n",
    "\n",
    "        # shift descriptor intentionally ignored\n",
    "        return build_dataset_filepath(\n",
    "            split          = split,\n",
    "            units          = units_flag,\n",
    "            style_flag     = style_flag,\n",
    "            smoothing_flag = smoothing_flag,\n",
    "            base_data_dir  = base_data_dir\n",
    "        )\n",
    "\n",
    "    def _load_datasets(self, row: pd.Series, test: bool = False):\n",
    "        \"\"\"\n",
    "        Instantiate *separate* train / validation datasets based on the CSV row.\n",
    "        Also works as a train-test-set loader, set test = True when using _load_datasets\n",
    "        test dataset pretends to be the val set when test = True\n",
    "        \"\"\"\n",
    "        train_path = self._dataset_path_from_row(row, split=\"train\")\n",
    "        if test:\n",
    "            val_path = self._dataset_path_from_row(row, split=\"test\")\n",
    "        else:\n",
    "            val_path = self._dataset_path_from_row(row, split=\"val\")\n",
    "\n",
    "        train_ds = Data_Functions.prep_dataset_from_tfrecord(\n",
    "            train_path,\n",
    "            batch_size   = self.batch_size,\n",
    "            shuffled     = True,                       # shuffle only training\n",
    "            cut_background = not bool(row[\"Background\"]),\n",
    "        )\n",
    "        train_ds = train_ds.repeat()\n",
    "\n",
    "        val_ds = Data_Functions.prep_dataset_from_tfrecord(\n",
    "            val_path,\n",
    "            batch_size   = self.batch_size,\n",
    "            shuffled     = False,                      # never shuffle validation\n",
    "            cut_background = not bool(row[\"Background\"]),\n",
    "        )\n",
    "        return train_ds, val_ds\n",
    "\n",
    "        ########################################################################################\n",
    "        ########################################################################################\n",
    "    def _build_model(self, row: pd.Series) -> Model:\n",
    "        \"\"\"Create and compile a new model according to hyper-params in *row*.\"\"\"\n",
    "        # 🔲 TODO - Make compatible with config and save a copy of config\n",
    "        dilation_mult = float(row.get(\"Dilation\", 1.0))\n",
    "        use_attention = bool(row.get(\"Attention\", False))\n",
    "\n",
    "        model = Custom_Models.create_modular_dcnn_model(\n",
    "            dilation_multiplier=dilation_mult,\n",
    "            use_local_attention=use_attention,\n",
    "            use_long_range_attention=use_attention,\n",
    "            use_final_attention=use_attention,\n",
    "        )\n",
    "\n",
    "        loss_fn = self._select_loss(row)\n",
    "        metrics = Custom_Metrics.METRICS\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(),\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "    def _select_loss(self, row: pd.Series):\n",
    "        \"\"\"\n",
    "        Choose and instantiate the loss for this trial, based on:\n",
    "        • early_rewarding → wrap in Switching*Loss\n",
    "        • smoothing       → “Custom” → focal, else → cross‐entropy\n",
    "        • background      → background_removed = not background\n",
    "        Expects columns:\n",
    "        \"Early Rewarding\" (bool), \"Smoothing\" (str), \"Background\" (bool)\n",
    "        \"\"\"\n",
    "        # 1) read your trial flags\n",
    "        early_rewarding = bool(row[\"Early Rewarding\"])\n",
    "        smoothing      = str(row[\"Smoothing\"]).lower()   # e.g. \"custom\" or \"proper\" or \"none\"\n",
    "        background     = bool(row[\"Background\"])\n",
    "        background_removed = not background\n",
    "\n",
    "        # 2) build the shared kwargs\n",
    "        loss_kwargs = {\n",
    "            \"dominant_class_index\":               DOMINANT_CLASS_INDEX,\n",
    "            \"dominant_correct_multiplier\":        DOMINANT_CORRECT_MULTIPLIER,\n",
    "            \"dominant_incorrect_multiplier\":      DOMINANT_INCORRECT_MULTIPLIER,\n",
    "            \"other_class_true_positive_multiplier\":   OTHER_TP_MULTIPLIER,\n",
    "            \"other_class_false_negative_multiplier\":  OTHER_FN_MULTIPLIER,\n",
    "            \"other_class_false_positive_multiplier\":  OTHER_FP_MULTIPLIER,\n",
    "            \"other_class_true_negative_multiplier\":   OTHER_TN_MULTIPLIER,\n",
    "            \"background_removed\":                 background_removed,\n",
    "            \"threshold\":                          THRESHOLD,\n",
    "            \"focal_gamma\":                        FOCAL_GAMMA,\n",
    "            \"focal_alpha\":                        FOCAL_ALPHA,\n",
    "        }\n",
    "\n",
    "        # 3) pick focal vs cross-entropy and add their hyper-params\n",
    "        if smoothing == \"custom\":\n",
    "            BaseLoss = Custom_Losses.CustomBinaryFocalLoss\n",
    "            loss_kwargs.update({\n",
    "                \"smoothing_multiplier\": INCORRECT_SMOOTHING_MULTIPLIER,\n",
    "                \"smoothing_as_correct\":  DEFAULT_SMOOTHING_AS_CORRECT,\n",
    "            })\n",
    "        else:\n",
    "            BaseLoss = Custom_Losses. AllBinaryFocalLoss\n",
    "            if smoothing == \"proper\":\n",
    "                loss_kwargs.update({\n",
    "                    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "                })\n",
    "            else:\n",
    "                loss_kwargs.update({\n",
    "                    \"label_smoothing\": 0,\n",
    "                })\n",
    "\n",
    "        # 4) if early_rewarding, wrap in the switching‐loss class\n",
    "        if early_rewarding:\n",
    "            loss_kwargs[\"swap_epoch\"] = SWAP_EPOCH\n",
    "            if BaseLoss is Custom_Losses.CustomBinaryFocalLoss:\n",
    "                LossClass = Custom_Losses.SwitchingFocalLoss\n",
    "            else:\n",
    "                LossClass = Custom_Losses.SwitchingBinaryCrossentropyLoss\n",
    "        else:\n",
    "            LossClass = BaseLoss\n",
    "\n",
    "        # 5) return the instantiated loss\n",
    "        return LossClass(**loss_kwargs)\n",
    "\n",
    "\n",
    "    def _make_callbacks(\n",
    "        self,\n",
    "        row: pd.Series,\n",
    "        epoch_unit_idx: int,\n",
    "    ) -> List[callbacks.Callback]:\n",
    "        \"\"\"Create per-epoch-unit callback list.\"\"\"\n",
    "        cbs_from_config = Custom_Callbacks.CALLBACKS\n",
    "\n",
    "        tensorboard_dir = LOG_DIR + f\"/{self.experiment}/trial_{row['Trial']:02d}/unit_{epoch_unit_idx:02d}\"\n",
    "        tb_cb = callbacks.TensorBoard(log_dir=tensorboard_dir, histogram_freq=1)\n",
    "\n",
    "        cbs_from_config.append(tb_cb)\n",
    "        return cbs_from_config\n",
    "\n",
    "        ####################################################################################\n",
    "        ####################################################################################\n",
    "        # Breadcrumbs I aint dealing with tonight: switchable losses config: does it need **shared_kwargs \n",
    "        # Class EpochUpdater needs a pointer to a loss object so it maybe needs to get added in the training loop\n",
    "    def _finalise_trial(self, model: Model, trial_dir: Path) -> None:\n",
    "        \"\"\"Plot train/val curves & save a compact *production* checkpoint.\"\"\"\n",
    "        # 🔲 TODO - capture the accumulated History over 10 units (requires persistence).\n",
    "        history_json = trial_dir + \"history_training_history.json\"\n",
    "        if history_json.exists():\n",
    "            with open(history_json) as fh:\n",
    "                history_dict = json.load(fh)\n",
    "            # Use helper to generate plot and save\n",
    "            fig = Helper_Functions.plot_train_val_curve(history_object=type(\"H\", (), {\"history\": history_dict})(),\n",
    "                                                        training_target_variable=\"loss\")\n",
    "            fig.savefig(trial_dir + \"train_val_curve.jpg\")\n",
    "\n",
    "        # Save final model (weights only)\n",
    "        model.save(trial_dir + \"final_model.keras\")\n",
    "\n",
    "\n",
    "# ── Command-line helper ───────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Run an Intron-Exon experiment from a CSV spec.\")\n",
    "    parser.add_argument(\"csv\", help=\"Path to experiment CSV file\")\n",
    "    parser.add_argument(\"name\", help=\"Experiment folder name (under Models/)\")\n",
    "    parser.add_argument(\"--no-resume\", action=\"store_true\", help=\"Start from scratch, ignore existing checkpoints\")\n",
    "    parser.add_argument(\"--batch\", type=int, default=DEFAULT_BATCH_SIZE, help=\"Batch size\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    handler = ExperimentHandler(\n",
    "        csv_path=args.csv,\n",
    "        experiment_name=args.name,\n",
    "        resume=not args.no_resume,\n",
    "        batch_size=args.batch,\n",
    "    )\n",
    "    handler.run()\n",
    "    \n",
    "    # Move logs to experiment folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  External call                                                        │\n",
    "│  best_hp, tuner = run_tuning(data_spec, project_name=\"exp01\")         │\n",
    "└────────────┬──────────────────────────────────────────────────────────┘\n",
    "             │\n",
    "             ▼\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│  run_tuning()                                                         │\n",
    "│  ├─ _prepare_datasets()         →  Data_Functions.prep_dataset…       │\n",
    "│  │                                returns (train_ds, val_ds)          │\n",
    "│  ├─ DCNNHyperModel(data_spec)                                         │\n",
    "│  │    │                                                               │\n",
    "│  │    └─ build(hp)                                                    │\n",
    "│  │         ├─ hp.* search-space (lr, dilation_mult, attention …)      │\n",
    "│  │         ├─ Custom_Models.create_modular_dcnn_model()               │\n",
    "│  │         └─ model.compile( Custom_Losses.CustomBinaryFocalLoss , metrics )        │\n",
    "│  ├─ kt.Hyperband( … directory=Models/<proj>/tuner/ )                  │\n",
    "│  ├─ tuner.search(train_ds, val_ds, callbacks=[EarlyStopping])         │\n",
    "│  ├─ best_hp = tuner.get_best_hyperparameters(1)[0]                    │\n",
    "│  └─ _save_best(best_hp, project_name) → best_hparams.json             │\n",
    "└────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "   ▲ load_best_hparams(project_name)  ←────────────┘ (JSON helper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter_Tuning.py\n",
    "\"\"\"\n",
    "Keras-Tuner wrapper for the Intron-Exon project.\n",
    "\n",
    "Usage\n",
    "-----\n",
    ">>> from IEModules import Hyperparameter_Tuning as hpt\n",
    ">>> best_hp, tuner = hpt.run_tuning(\n",
    "...     data_spec=row_from_csv,\n",
    "...     project_name=\"exp01_hyperband\"\n",
    "... )\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, os, datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import (\n",
    "    Custom_Models,\n",
    "    Custom_Losses,\n",
    "    Data_Functions,\n",
    "    Custom_Metrics,\n",
    "    config\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. HyperModel definition\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class DCNNHyperModel(kt.HyperModel):\n",
    "    \"\"\"Wraps create_modular_dcnn_model with a tunable search-space.\"\"\"\n",
    "    def __init__(self, data_spec: Dict):\n",
    "        self.data_spec = data_spec          # row from your experiment CSV\n",
    "\n",
    "    # --- mandatory -----------------------------------------------------------\n",
    "    def build(self, hp: kt.HyperParameters) -> Model:\n",
    "        # Search-space examples — extend freely.\n",
    "        dilation_mult = hp.Float(\"dilation_mult\", 0.5, 2.0, step=0.25)\n",
    "        use_attention = hp.Boolean(\"attention\", default=True)\n",
    "        lr            = hp.Float(\"lr\", 1e-4, 3e-3, sampling=\"log\")\n",
    "\n",
    "        model = Custom_Models.create_modular_dcnn_model(\n",
    "            dilation_multiplier=dilation_mult,\n",
    "            use_local_attention=use_attention,\n",
    "            use_long_range_attention=use_attention,\n",
    "            use_final_attention=use_attention,\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=lr),\n",
    "            loss      = Custom_Losses.Custom_Losses.CustomBinaryFocalLoss(),\n",
    "            metrics   = [Custom_Metrics.CustomNoBackgroundF1Score(num_classes=5)],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # --- optional ------------------------------------------------------------\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        \"\"\"You can override to inject custom callbacks—or just omit.\"\"\"\n",
    "        return super().fit(hp, model, *args, **kwargs)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Helper: get train/val datasets exactly once per tuner process\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _prepare_datasets(data_spec: Dict, batch_size=32):\n",
    "    tfrecord_path = Path(config.DATA_DIR) / \"YOUR_DEFAULT.tfrecord.gz\"\n",
    "    # TODO - replace -- use data_spec to choose correct file\n",
    "    ds = Data_Functions.prep_dataset_from_tfrecord(\n",
    "        tfrecord_path,\n",
    "        batch_size=batch_size,\n",
    "        shuffled=True,\n",
    "        cut_background=not bool(data_spec[\"Background\"])\n",
    "    )\n",
    "    # simplistic 80-20 split\n",
    "    n = ds.cardinality().numpy()\n",
    "    train_ds = ds.take(int(n*0.8))\n",
    "    val_ds   = ds.skip(int(n*0.8))\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Public API\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def run_tuning(\n",
    "    data_spec: Dict,\n",
    "    project_name: str,\n",
    "    max_trials: int = 30,\n",
    "    overwrite: bool = False,\n",
    ") -> Tuple[kt.HyperParameters, kt.Tuner]:\n",
    "\n",
    "    # Materialise datasets *once* so every trial re-uses the same TF graph\n",
    "    train_ds, val_ds = _prepare_datasets(data_spec)\n",
    "\n",
    "    hypermodel = DCNNHyperModel(data_spec)\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel,\n",
    "        objective   = kt.Objective(\"val_no_background_f1\", direction=\"max\"),\n",
    "        max_epochs  = 15,\n",
    "        factor      = 3,\n",
    "        directory   = str(Path(config.MODEL_DIR) / project_name),\n",
    "        project_name= \"tuner\",\n",
    "        overwrite   = overwrite\n",
    "    )\n",
    "\n",
    "    stop_early = callbacks.EarlyStopping(\n",
    "        monitor=\"val_no_background_f1\",\n",
    "        mode=\"max\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        train_ds,\n",
    "        validation_data = val_ds,\n",
    "        callbacks       = [stop_early],\n",
    "    )\n",
    "\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    _save_best(best_hp, project_name)\n",
    "\n",
    "    return best_hp, tuner\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Convenience helpers\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _save_best(best_hp: kt.HyperParameters, project_name: str):\n",
    "    out = Path(config.MODEL_DIR) / project_name / \"best_hparams.json\"\n",
    "    with open(out, \"w\") as fh:\n",
    "        json.dump(best_hp.values, fh, indent=2)\n",
    "    print(f\"Best HP written to {out}\")\n",
    "\n",
    "def load_best_hparams(project_name: str) -> Dict:\n",
    "    path = Path(config.MODEL_DIR) / project_name / \"best_hparams.json\"\n",
    "    with open(path) as fh:\n",
    "        return json.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anywhere - CLI, notebook, or even inside ExperimentHandler\n",
    "from IEModules.Hyperparameter_Tuning import run_tuning, load_best_hparams\n",
    "\n",
    "best_hp, tuner = run_tuning(\n",
    "    data_spec   = row_from_csv_as_dict,\n",
    "    project_name= \"exp01_hyperband\"\n",
    ")\n",
    "print(\"Best LR  :\", best_hp['lr'])\n",
    "print(\"Dilation :\", best_hp['dilation_mult'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 02:49:53.904750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-29 02:49:54.003581: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-29 02:49:54.032874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-29 02:49:54.214451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-29 02:49:55.597796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Custom_Callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks, optimizers, models, backend, utils\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# ── In-house modules ───────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIEModules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     51\u001b[0m     Data_Functions,\n\u001b[1;32m     52\u001b[0m     Custom_Models,\n\u001b[1;32m     53\u001b[0m     Custom_Callbacks,\n\u001b[1;32m     54\u001b[0m     Custom_Losses,\n\u001b[1;32m     55\u001b[0m     Helper_Functions,\n\u001b[1;32m     56\u001b[0m     Custom_Metrics,\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIEModules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     59\u001b[0m     MODEL_DIR,\n\u001b[1;32m     60\u001b[0m     LOG_DIR,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     LR_STATE_SAVE_FILENAME,\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIEModules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m cfg  \u001b[38;5;66;03m# Generic access to loss hyper-params\u001b[39;00m\n",
      "File \u001b[0;32m~/Deep Learning Projects/Intron Exon/IEModules/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_tuner\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkt\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyfaidx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fasta\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mCustom_Callbacks\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mCustom_Losses\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mCustom_Metrics\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Custom_Callbacks'"
     ]
    }
   ],
   "source": [
    "# experiment_framework.py\n",
    "\"\"\"\n",
    "High-level experiment orchestration for the Intron-Exon project.\n",
    "\n",
    "Each CSV row represents a *trial* that is trained for a fixed number of\n",
    "**epoch-units** (``config.EPOCH_UNITS_PER_TRIAL``).  One epoch-unit corresponds\n",
    "exactly to ``config.STEPS_PER_EPOCH_UNIT`` gradient-descent steps on the\n",
    "training dataset.\n",
    "\n",
    "Responsibilities\n",
    "----------------\n",
    "*     Experiment folder setup and per-trial sub-folders\n",
    "*     Incremental/resumable training with checkpoint discovery\n",
    "*     Loading TFRecord datasets based on naming conventions\n",
    "*     Selecting the appropriate model architecture, loss, metrics and callbacks\n",
    "*     Aggregating Keras ``History`` objects across epoch-units and persisting\n",
    "      them crash-safely as JSON\n",
    "*     Re-plotting a train/val loss curve and saving a final model artefact at\n",
    "      the end of every trial\n",
    "\n",
    "Revision history\n",
    "----------------\n",
    "🔄 **2025-04-29 rewrite** —⁠ consolidated fixes discussed with ChatGPT:\n",
    "    • Added missing imports, Path-safe ops, and metrics instantiation\n",
    "    • Numeric checkpoint ordering\n",
    "    • One-file running history with crash-safety\n",
    "    • Correct ``epochs``/``initial_epoch`` per unit\n",
    "    • Avoid mutating global callback list; TB/Updater added per unit\n",
    "    • LR-scheduler state kept *per-trial*\n",
    "    • ``_dataset_path_from_row`` uses ``self.base_data_dir``\n",
    "    • Garbage-collect & clear Keras session between trials\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import tempfile\n",
    "import shutil\n",
    "import gc\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import callbacks, optimizers, models, backend, utils\n",
    "\n",
    "# ── In-house modules ───────────────────────────────────────────────────────────\n",
    "from IEModules import (\n",
    "    Data_Functions,\n",
    "    Custom_Models,\n",
    "    Custom_Callbacks,\n",
    "    Custom_Losses,\n",
    "    Helper_Functions,\n",
    "    Custom_Metrics,\n",
    ")\n",
    "from IEModules.config import (\n",
    "    MODEL_DIR,\n",
    "    LOG_DIR,\n",
    "    EPOCH_UNITS_PER_TRIAL,\n",
    "    DEFAULT_BATCH_SIZE,\n",
    "    STEPS_PER_EPOCH_UNIT,\n",
    "    experiment_data_folder,\n",
    "    LR_STATE_SAVE_SUBDIR,\n",
    "    LR_STATE_SAVE_FILENAME,\n",
    ")\n",
    "from IEModules import config as cfg  # Generic access to loss hyper-params\n",
    "\n",
    "# ── Dataset naming helpers ─────────────────────────────────────────────────────\n",
    "\n",
    "def build_dataset_filepath(\n",
    "    split: str,\n",
    "    units: int,\n",
    "    style_flag: str,\n",
    "    smoothing_flag: str,\n",
    "    base_data_dir: Path = experiment_data_folder,\n",
    ") -> Path:\n",
    "    \"\"\"Resolve the unique TFRecord file that matches a naming convention.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    split : str\n",
    "        One of ``{\"train\", \"val\", \"test\"}``.\n",
    "    units : int\n",
    "        Size flag taken from the CSV (number of epoch-units included when the\n",
    "        TFRecord was generated).\n",
    "    style_flag : str\n",
    "        Either ``\"M\"`` (mixed) or ``\"I\"`` (incremental).  Only the first\n",
    "        character is used.\n",
    "    smoothing_flag : str\n",
    "        Either ``\"C\"`` (custom smoothed labels) or ``\"B\"`` (binary labels).\n",
    "    base_data_dir : pathlib.Path, optional\n",
    "        Directory that contains the dataset files.  Defaults to the folder\n",
    "        chosen in *config.experiment_data_folder*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        Fully-qualified path to the single matching TFRecord file.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If no file with the expected prefix exists.\n",
    "    RuntimeError\n",
    "        If more than one file matches the prefix (ambiguous).\n",
    "    \"\"\"\n",
    "    split = split.lower()\n",
    "    prefix = f\"{split}_{units:02d}_{style_flag.upper()}_{smoothing_flag.upper()}\"\n",
    "    pattern = f\"{prefix}*IEData.tfrecord.gz\"\n",
    "    matches = sorted(base_data_dir.glob(pattern))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No TFRecord starting with '{prefix}' in {base_data_dir}\")\n",
    "    if len(matches) > 1:\n",
    "        raise RuntimeError(f\"Ambiguous prefix '{prefix}': {matches}\")\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "# ── Utility helpers ────────────────────────────────────────────────────────────\n",
    "\n",
    "def latest_checkpoint(ckpt_dir: Path) -> Optional[Path]:\n",
    "    \"\"\"Return the checkpoint with the highest *numeric* epoch identifier.\n",
    "\n",
    "    Keras' ``ModelCheckpoint`` callback embeds the epoch number into the file\n",
    "    name (e.g. ``epoch-012-val_*.keras``).  This helper extracts the number and\n",
    "    returns the latest file so that training can resume from there.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ckpt_dir : pathlib.Path\n",
    "        Directory that contains ``*.keras`` model files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path | None\n",
    "        Path to the checkpoint or ``None`` if the directory is empty.\n",
    "    \"\"\"\n",
    "    epochs: Dict[int, Path] = {}\n",
    "    for p in ckpt_dir.glob(\"*.keras\"):\n",
    "        m = re.search(r\"epoch[-_](\\d+)\", p.name)\n",
    "        if m:\n",
    "            epochs[int(m.group(1))] = p\n",
    "    return epochs[max(epochs)] if epochs else None\n",
    "\n",
    "\n",
    "def _load_running_history(path: Path) -> Dict[str, List[float]]:\n",
    "    \"\"\"Load a *running* history JSON file if it exists else return an empty dict.\"\"\"\n",
    "    if path.exists():\n",
    "        with open(path) as fh:\n",
    "            return json.load(fh)\n",
    "    return defaultdict(list)\n",
    "\n",
    "\n",
    "def _extend_history(store: Dict[str, List], fragment: Dict[str, List]):\n",
    "    \"\"\"Append values from *fragment* into the list-of-lists *store* in-place.\"\"\"\n",
    "    for k, v in fragment.items():\n",
    "        store.setdefault(k, []).extend(v)\n",
    "\n",
    "\n",
    "def _atomic_dump(obj: Dict, path: Path):\n",
    "    \"\"\"Write *obj* as JSON to *path* atomically (write-then-move).\"\"\"\n",
    "    tmp = tempfile.NamedTemporaryFile(\"w\", delete=False)\n",
    "    try:\n",
    "        json.dump(obj, tmp)\n",
    "        tmp.close()\n",
    "        shutil.move(tmp.name, path)\n",
    "    finally:\n",
    "        if Path(tmp.name).exists():\n",
    "            Path(tmp.name).unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "# ── Main handler ───────────────────────────────────────────────────────────────\n",
    "\n",
    "class ExperimentHandler:\n",
    "    \"\"\"Orchestrate a *multi-trial* experiment defined by a CSV specification.\n",
    "\n",
    "    Each trial is trained for ``config.EPOCH_UNITS_PER_TRIAL`` epoch-units.  The\n",
    "    handler creates a dedicated sub-folder per trial, handles resumption logic\n",
    "    (checkpoint + ReduceLROnPlateau state + aggregated history) and finally\n",
    "    stores a *final* model plus a JPEG learning-curve.\n",
    "    \"\"\"\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Path | str,\n",
    "        experiment_name: str,\n",
    "        model_dir: Path | str = MODEL_DIR,\n",
    "        data_folder: Path | str = experiment_data_folder,\n",
    "        resume: bool = True,\n",
    "        batch_size: int = DEFAULT_BATCH_SIZE,\n",
    "    ) -> None:\n",
    "        \"\"\"Construct a new *ExperimentHandler*.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        csv_path : str | pathlib.Path\n",
    "            Path to the CSV file whose rows specify the hyper-parameters for each\n",
    "            trial (columns must match the expected headers).\n",
    "        experiment_name : str\n",
    "            Name of the experiment folder to be created inside ``MODEL_DIR``.\n",
    "        model_dir : str | pathlib.Path, optional\n",
    "            Root directory that will hold the *experiment* folder (default:\n",
    "            ``config.MODEL_DIR``).\n",
    "        data_folder : str | pathlib.Path, optional\n",
    "            Directory that contains pre-built TFRecord datasets (default:\n",
    "            ``config.experiment_data_folder``).\n",
    "        resume : bool, optional\n",
    "            If ``True`` (default) the handler will look for an existing\n",
    "            checkpoint & aggregated history and continue training; otherwise it\n",
    "            always starts fresh.\n",
    "        batch_size : int, optional\n",
    "            Mini-batch size fed to ``tf.data`` pipelines (default:\n",
    "            ``config.DEFAULT_BATCH_SIZE``).\n",
    "        \"\"\"\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.experiment = experiment_name\n",
    "        self.batch_size = batch_size\n",
    "        self.resume_flag = resume\n",
    "        self.experiment_dir = Path(model_dir) / experiment_name\n",
    "        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.trials_df = pd.read_csv(self.csv_path)\n",
    "        self.trials_df.columns = [c.strip() for c in self.trials_df.columns]\n",
    "        self.base_data_dir = Path(data_folder) / Path(\"TestValTrain\")\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    def run(self):\n",
    "        \"\"\"Iterate over CSV rows and execute all trials sequentially.\"\"\"\n",
    "        for _, row in self.trials_df.iterrows():\n",
    "            trial_id = int(row[\"Trial\"])\n",
    "            trial_dir = self._prepare_trial_folder(trial_id)\n",
    "            history_path = trial_dir / \"history_full.json\"\n",
    "            running_hist = _load_running_history(history_path)\n",
    "            start_unit = len(next(iter(running_hist.values()), []))\n",
    "\n",
    "            # ── Resume / build model ────────────────────────────────\n",
    "            ckpt_dir = trial_dir / \"checkpoints\"\n",
    "            model = None\n",
    "            if self.resume_flag and (ckpt := latest_checkpoint(ckpt_dir)):\n",
    "                custom_objs = utils.get_custom_objects()\n",
    "                model = models.load_model(ckpt, compile=False, custom_objects=custom_objs)\n",
    "            if model is None:\n",
    "                backend.clear_session(); gc.collect()\n",
    "                model = self._build_model(row)\n",
    "            else:                                   # resume\n",
    "                loss_fn = self._select_loss(row)\n",
    "                metrics = [\n",
    "                    m(num_classes=5) if \"num_classes\" in m.__init__.__code__.co_varnames else m()\n",
    "                    for m in Custom_Metrics.METRICS\n",
    "                ]                    \n",
    "                model.compile(optimizer=optimizers.Adam(),\n",
    "                            loss=loss_fn,\n",
    "                            metrics=metrics)\n",
    "\n",
    "            # ── Loop over epoch-units ───────────────────────────────\n",
    "            for unit in range(start_unit, EPOCH_UNITS_PER_TRIAL):\n",
    "                print(f\"\\nTrial {trial_id:02d} • Epoch-Unit {unit+1}/{EPOCH_UNITS_PER_TRIAL}\")\n",
    "\n",
    "                train_ds, val_ds = self._load_datasets(row)\n",
    "                cbs = self._make_callbacks(row, unit, trial_dir, model)\n",
    "\n",
    "                history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=unit + 1,\n",
    "                    initial_epoch=unit,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH_UNIT,\n",
    "                    callbacks=cbs,\n",
    "                )\n",
    "\n",
    "                _extend_history(running_hist, history.history)\n",
    "                _atomic_dump(running_hist, history_path)\n",
    "\n",
    "            # ── End-of-trial clean-up ───────────────────────────────\n",
    "            Helper_Functions.plot_train_val_curve(\n",
    "                history_object=type(\"H\", (), {\"history\": running_hist})(),\n",
    "                training_target_variable=\"loss\",\n",
    "            ).savefig(trial_dir / \"train_val_curve.jpg\")\n",
    "            model.save(trial_dir / \"final_model.keras\")\n",
    "\n",
    "        print(\"\\n✅ All trials complete!\")\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    def _prepare_trial_folder(self, num: int) -> Path:\n",
    "        \"\"\"Ensure the directory structure for a given trial exists.\n",
    "\n",
    "        Creates ``Trial_XX/checkpoints`` (nested under the experiment folder)\n",
    "        and returns the trial directory path.\n",
    "        \"\"\"\n",
    "        tdir = self.experiment_dir / f\"Trial_{num:02d}\"\n",
    "        (tdir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "        return tdir\n",
    "\n",
    "    # ---------------- Dataset helpers ----------------------------------\n",
    "    def _dataset_path_from_row(self, row: pd.Series, split: str) -> Path:\n",
    "        \"\"\"Resolve the TFRecord file for *split* according to CSV flags.\"\"\"\n",
    "        units_flag = int(row[\"Size\"])\n",
    "        style_flag = str(row[\"Style\"]).upper()[0]\n",
    "        smoothing_flag = \"C\" if str(row[\"Smoothing\"]).upper().startswith(\"C\") else \"B\"\n",
    "        return build_dataset_filepath(\n",
    "            split,\n",
    "            units_flag,\n",
    "            style_flag,\n",
    "            smoothing_flag,\n",
    "            base_data_dir=self.base_data_dir,\n",
    "        )\n",
    "\n",
    "    def _load_datasets(self, row: pd.Series, test: bool = False):\n",
    "        \"\"\"Build ``tf.data`` datasets for training/validation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row : pandas.Series\n",
    "            The current CSV row describing the trial.\n",
    "        test : bool, optional\n",
    "            If ``True`` the *validation* dataset is taken from the ``test``\n",
    "            TFRecord instead of ``val`` (useful for final evaluation).\n",
    "        \"\"\"\n",
    "        train_path = self._dataset_path_from_row(row, \"train\")\n",
    "        val_path = self._dataset_path_from_row(row, \"test\" if test else \"val\")\n",
    "        train_ds = Data_Functions.prep_dataset_from_tfrecord(\n",
    "            train_path,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffled=True,\n",
    "            cut_background=not bool(row[\"Background\"]),\n",
    "        ).repeat()\n",
    "        val_ds = Data_Functions.prep_dataset_from_tfrecord(\n",
    "            val_path,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffled=False,\n",
    "            cut_background=not bool(row[\"Background\"]),\n",
    "        )\n",
    "        return train_ds, val_ds\n",
    "\n",
    "    # ---------------- Model/loss building ------------------------------\n",
    "    def _build_model(self, row: pd.Series):\n",
    "        \"\"\"Instantiate and compile a model based on CSV hyper-parameters.\"\"\"\n",
    "        dilation_mult = float(row.get(\"Dilation\", 1.0))\n",
    "        use_attention = bool(row.get(\"Attention\", False))\n",
    "        model = Custom_Models.create_modular_dcnn_model(\n",
    "            dilation_multiplier=dilation_mult,\n",
    "            use_local_attention=use_attention,\n",
    "            use_long_range_attention=use_attention,\n",
    "            use_final_attention=use_attention,\n",
    "        )\n",
    "        loss_fn = self._select_loss(row)\n",
    "        metrics = [\n",
    "            m(num_classes=5) if \"num_classes\" in m.__init__.__code__.co_varnames else m()\n",
    "            for m in Custom_Metrics.METRICS\n",
    "        ]\n",
    "        model.compile(optimizer=optimizers.Adam(), loss=loss_fn, metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    def _select_loss(self, row: pd.Series):\n",
    "        \"\"\"Return an appropriately parameterised loss object for the trial.\"\"\"\n",
    "        early_rewarding = bool(row[\"Early Rewarding\"])\n",
    "        smoothing = str(row[\"Smoothing\"]).lower()\n",
    "        background_removed = not bool(row[\"Background\"])\n",
    "\n",
    "        kwargs = dict(\n",
    "            dominant_class_index=cfg.DOMINANT_CLASS_INDEX,\n",
    "            dominant_correct_multiplier=cfg.DOMINANT_CORRECT_MULTIPLIER,\n",
    "            dominant_incorrect_multiplier=cfg.DOMINANT_INCORRECT_MULTIPLIER,\n",
    "            other_class_true_positive_multiplier=cfg.OTHER_TP_MULTIPLIER,\n",
    "            other_class_false_negative_multiplier=cfg.OTHER_FN_MULTIPLIER,\n",
    "            other_class_false_positive_multiplier=cfg.OTHER_FP_MULTIPLIER,\n",
    "            other_class_true_negative_multiplier=cfg.OTHER_TN_MULTIPLIER,\n",
    "            background_removed=background_removed,\n",
    "            threshold=cfg.THRESHOLD,\n",
    "            focal_gamma=cfg.FOCAL_GAMMA,\n",
    "            focal_alpha=cfg.FOCAL_ALPHA,\n",
    "        )\n",
    "\n",
    "        if smoothing == \"custom\":\n",
    "            BaseLoss = Custom_Losses.CustomBinaryFocalLoss\n",
    "            kwargs.update(\n",
    "                smoothing_multiplier=cfg.INCORRECT_SMOOTHING_MULTIPLIER,\n",
    "                smoothing_as_correct=cfg.DEFAULT_SMOOTHING_AS_CORRECT,\n",
    "            )\n",
    "        else:\n",
    "            BaseLoss = Custom_Losses. AllBinaryFocalLoss\n",
    "            kwargs.update(label_smoothing=cfg.LABEL_SMOOTHING if smoothing == \"proper\" else 0)\n",
    "\n",
    "        if early_rewarding:\n",
    "            kwargs[\"swap_epoch\"] = cfg.SWAP_EPOCH\n",
    "            LossClass = (\n",
    "                Custom_Losses.SwitchingFocalLoss\n",
    "                if BaseLoss is Custom_Losses.CustomBinaryFocalLoss\n",
    "                else Custom_Losses.SwitchingBinaryCrossentropyLoss\n",
    "            )\n",
    "        else:\n",
    "            LossClass = BaseLoss\n",
    "        return LossClass(**kwargs)\n",
    "\n",
    "    # ---------------- Callbacks ----------------------------------------\n",
    "    def _make_callbacks(\n",
    "        self,\n",
    "        row: pd.Series,\n",
    "        unit_idx: int,\n",
    "        trial_dir: Path,\n",
    "        model,\n",
    "    ) -> List[callbacks.Callback]:\n",
    "        \"\"\"Assemble a fresh list of callbacks for the *current* epoch-unit.\"\"\"\n",
    "        # Start with a shallow copy so that the global list is never mutated.\n",
    "        cbs = [copy.deepcopy(cb) for cb in Custom_Callbacks.CALLBACKS]\n",
    "\n",
    "        # --- TensorBoard ------------------------------------------------\n",
    "        tb_dir = LOG_DIR / self.experiment / f\"trial_{row['Trial']:02d}\" / f\"unit_{unit_idx:02d}\"\n",
    "        cbs.append(callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=1))\n",
    "\n",
    "        # --- Epoch updater for switchable loss --------------------------\n",
    "        if hasattr(model.loss, \"epoch_var\"):\n",
    "            cbs.append(Custom_Losses.EpochUpdater(model.loss))\n",
    "\n",
    "        # --- LR-scheduler state path (unique per trial) -----------------\n",
    "        lr_state_path = trial_dir / LR_STATE_SAVE_SUBDIR / LR_STATE_SAVE_FILENAME\n",
    "        for cb in cbs:\n",
    "            if isinstance(cb, callbacks.ModelCheckpoint):\n",
    "                cb.filepath = str(trial_dir / cfg.CHECKPOINT_SUBDIR / cfg.CHECKPOINT_FILENAME)\n",
    "            if isinstance(cb, Custom_Callbacks.StatefulReduceLROnPlateau):\n",
    "                cb.state_save_filepath = lr_state_path\n",
    "                if lr_state_path.exists() and self.resume_flag and unit_idx == 0:\n",
    "                    cb.load_state_from_file(lr_state_path)\n",
    "        return cbs\n",
    "\n",
    "\n",
    "# ── CLI helper ────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Run an Intron-Exon experiment from a CSV spec.\")\n",
    "    parser.add_argument(\"csv\", help=\"Path to experiment CSV file\")\n",
    "    parser.add_argument(\"name\", help=\"Experiment folder name (under Models/)\")\n",
    "    parser.add_argument(\"--no-resume\", action=\"store_true\", help=\"Ignore existing checkpoints/history\")\n",
    "    parser.add_argument(\"--batch\", type=int, default=DEFAULT_BATCH_SIZE, help=\"Batch size\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    handler = ExperimentHandler(\n",
    "        csv_path=args.csv,\n",
    "        experiment_name=args.name,\n",
    "        resume=not args.no_resume,\n",
    "        batch_size=args.batch,\n",
    "    )\n",
    "    handler.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │ concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv1d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_34          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_35          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">92,320</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv1d_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_36          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">230,560</span> │ dropout_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv1d_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_37          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">276,672</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_38          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">331,968</span> │ dropout_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_39          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,912</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">331,968</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_40          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">331,968</span> │ dropout_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,824</span> │ concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_41          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,056</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_33          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,368</span> │ concatenate_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_42          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_43          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dropout_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m10\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_49 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m704\u001b[0m │ concatenate_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_50 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ conv1d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_34          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_51 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_35          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_12 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_53 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │     \u001b[38;5;34m92,320\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │        \u001b[38;5;34m640\u001b[0m │ conv1d_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_36          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_54 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │    \u001b[38;5;34m230,560\u001b[0m │ dropout_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │        \u001b[38;5;34m640\u001b[0m │ conv1d_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_37          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_52 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │     \u001b[38;5;34m10,400\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_13 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_56 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m276,672\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_38          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_57 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m331,968\u001b[0m │ dropout_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_39          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_55 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │     \u001b[38;5;34m30,912\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_14 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_59 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m331,968\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_40          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_60 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m331,968\u001b[0m │ dropout_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_48 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m5,824\u001b[0m │ concatenate_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_41          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_58 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │     \u001b[38;5;34m37,056\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_33          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_15 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m330\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ concatenate_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_61 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m42,368\u001b[0m │ concatenate_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_42          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_62 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m16,512\u001b[0m │ dropout_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_43          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_63 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │        \u001b[38;5;34m645\u001b[0m │ dropout_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,819,877</span> (6.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,819,877\u001b[0m (6.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,816,805</span> (6.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,816,805\u001b[0m (6.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> (12.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,072\u001b[0m (12.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 5000, 5), found shape=(2, 5000, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ train_on_batch succeeded: loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, metrics=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_vals\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m x_dummy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5000\u001b[39m, \u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     49\u001b[0m y_dummy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5000\u001b[39m, \u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 50\u001b[0m loss, \u001b[38;5;241m*\u001b[39mmetric_vals \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dummy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ train_on_batch succeeded: loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, metrics=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_vals\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Deep Learning Projects/Intron Exon/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:549\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 549\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/Deep Learning Projects/Intron Exon/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Deep Learning Projects/Intron Exon/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:121\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_step_on_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    125\u001b[0m     outputs,\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    127\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Deep Learning Projects/Intron Exon/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:108\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Deep Learning Projects/Intron Exon/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:51\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_has_training_arg:\n\u001b[0;32m---> 51\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n",
      "File \u001b[0;32m~/Deep Learning Projects/Intron Exon/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Deep Learning Projects/Intron Exon/.venv/lib/python3.10/site-packages/keras/src/layers/input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 5000, 5), found shape=(2, 5000, 4)"
     ]
    }
   ],
   "source": [
    "# test_model_compile.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "\n",
    "# 👉 adjust this import path if your packages live elsewhere\n",
    "from IEModules.Custom_Models import create_modular_dcnn_model\n",
    "from IEModules.Custom_Losses import (\n",
    "    CustomBinaryFocalLoss,\n",
    "    AllBinaryFocalLoss,\n",
    "    SwitchingFocalLoss,\n",
    "    SwitchingBinaryCrossentropyLoss,\n",
    ")\n",
    "from IEModules.Custom_Metrics import METRICS\n",
    "\n",
    "def instantiate_metrics():\n",
    "    \"\"\"Deep-copy or re-instantiate your metric objects to avoid shared state.\"\"\"\n",
    "    out = [copy.deepcopy(m) for m in METRICS]\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    # 1) Build the model\n",
    "    model = create_modular_dcnn_model(\n",
    "        input_dim=5,\n",
    "        sequence_length=5000,\n",
    "        num_classes=5,\n",
    "        use_local_attention=False,    # toggle on/off to test\n",
    "        use_long_range_attention=False,\n",
    "        use_final_attention=False,\n",
    "        dilation_multiplier=1.0,\n",
    "    )\n",
    "\n",
    "    # 2) Pick a loss (test both focal & crossentropy variants)\n",
    "    loss_fn =  CustomBinaryFocalLoss()  # or CustomBinaryFocalLoss()\n",
    "\n",
    "    # 3) Compile\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(),\n",
    "        loss=loss_fn,\n",
    "        metrics=instantiate_metrics(),\n",
    "    )\n",
    "\n",
    "    # 4) See the summary\n",
    "    model.summary()\n",
    "\n",
    "    # 5) Do a single train-on-batch with random data\n",
    "    x_dummy = np.random.random((2, 5000, 4)).astype(np.float32)\n",
    "    y_dummy = np.random.random((2, 5000, 4)).astype(np.float32)\n",
    "    loss, *metric_vals = model.train_on_batch(x_dummy, y_dummy)\n",
    "    print(f\"\\n✅ train_on_batch succeeded: loss={loss:.4f}, metrics={metric_vals}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 20:45:17.681885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-29 20:45:17.744500: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-29 20:45:17.764353: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-29 20:45:17.807802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-29 20:45:18.738792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745981120.726796  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1745981120.790844  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1745981120.790978  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1745981120.794912  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1745981120.794993  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1745981120.795037  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1745981121.189634  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1745981121.189734  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-29 20:45:21.189746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1745981121.189824  227861 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-29 20:45:21.189854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "from IEModules.config import DATA_DIR, LOG_DIR, MODEL_DIR, MODULE_DIR, NOTEBOOK_DIR\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "def tile_to_batch(z):\n",
    "    pe, x = z\n",
    "    return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class LocalMaskLayer(layers.Layer):\n",
    "    def __init__(self, radius=3, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            radius (int): The allowed distance for positions (default 3).\n",
    "        \"\"\"\n",
    "        super(LocalMaskLayer, self).__init__(**kwargs)\n",
    "        self.radius = radius\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        i = tf.range(seq_len)[:, None]\n",
    "        j = tf.range(seq_len)[None, :]\n",
    "        mask = tf.cast(tf.abs(i - j) <= self.radius, tf.float32)\n",
    "        mask = tf.expand_dims(mask, 0)  # shape: (1, seq_len, seq_len)\n",
    "        mask = tf.tile(mask, [tf.shape(inputs)[0], 1, 1])  # shape: (batch, seq_len, seq_len)\n",
    "        return mask\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(LocalMaskLayer, self).get_config()\n",
    "        config.update({'radius': self.radius})\n",
    "        return config\n",
    "\n",
    "\n",
    "# Helper fn to adjust dilation rates.\n",
    "@utils.register_keras_serializable()\n",
    "def adjust_dilation_rate(rate, dilation_multiplier):\n",
    "    # Multiply by dilation_multiplier, round to nearest int, and make sure it is at least 1.\n",
    "    new_rate = int(round(rate * dilation_multiplier))\n",
    "    return new_rate if new_rate >= 1 else 1\n",
    "\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "def create_modular_dcnn_model(\n",
    "    input_dim=5,\n",
    "    sequence_length=5000,\n",
    "    num_classes=5,\n",
    "    use_local_attention=True,\n",
    "    use_long_range_attention=True,\n",
    "    use_final_attention=True,\n",
    "    dilation_multiplier=1.0  # New parameter to scale the dilation rates\n",
    "):\n",
    "    \n",
    "\n",
    "    inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "    # Condensed positional encoding block\n",
    "    positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "    \n",
    "    concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "    \n",
    "    # Option 1: Local Masked Attention after positional encoding\n",
    "    if use_local_attention:\n",
    "        local_mask = LocalMaskLayer()(concat_input)\n",
    "        local_attn = layers.Attention(use_scale=True)([concat_input, concat_input], mask=[local_mask, local_mask])\n",
    "        # # Residual connection\n",
    "        concat_input = layers.Concatenate(axis=-1)([concat_input, local_attn])\n",
    "    \n",
    "    # Hyperparameters for dropout\n",
    "    early_dropout = 0.1\n",
    "    middle_dropout = 0\n",
    "    late_dropout = 0.2\n",
    "\n",
    "    cnn = layers.Conv1D(filters=64, kernel_size=9, activation='relu', padding='same')(concat_input)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Dropout(early_dropout)(cnn)\n",
    "    \n",
    "    # Dilated convolution block\n",
    "    skip = concat_input\n",
    "    skip = layers.Conv1D(filters=64, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=adjust_dilation_rate(1, dilation_multiplier),  # originally 1\n",
    "        activation='relu', padding='same')(skip)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(early_dropout)(dcnn)\n",
    "    low_dcnn = dcnn\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=64, kernel_size=9, dilation_rate=adjust_dilation_rate(2, dilation_multiplier),  # originally 2\n",
    "        activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(early_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=adjust_dilation_rate(4, dilation_multiplier),  # originally 4\n",
    "        activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=9, dilation_rate=adjust_dilation_rate(8, dilation_multiplier),  # originally 8\n",
    "        activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=adjust_dilation_rate(16, dilation_multiplier),  # originally 16\n",
    "        activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=adjust_dilation_rate(32, dilation_multiplier),  # originally 32\n",
    "        activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=adjust_dilation_rate(64, dilation_multiplier),  # originally 64\n",
    "        activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=9, dilation_rate=adjust_dilation_rate(128, dilation_multiplier),  # originally 128\n",
    "        activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(middle_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    # Concatenate inputs from different paths\n",
    "    second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn, low_dcnn])\n",
    "    \n",
    "    # Option 2: Long-range Attention after pooling the final convolution outputs\n",
    "    if use_long_range_attention:\n",
    "        pool_size = 10  # You can adjust the pooling factor as needed.\n",
    "        pooled = layers.MaxPooling1D(pool_size=pool_size, padding='same')(second_concat)\n",
    "        long_attn = layers.MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(pooled, pooled)\n",
    "        long_attn_upsampled = layers.UpSampling1D(size=pool_size)(long_attn)\n",
    "        second_concat = layers.Concatenate(axis=-1)([second_concat, long_attn_upsampled])\n",
    "    \n",
    "    # Option 3: Final Attention to capture which outputs are most important\n",
    "    if use_final_attention:\n",
    "        final_attn = layers.MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(second_concat, second_concat)\n",
    "        second_concat = layers.Concatenate()([second_concat, final_attn])\n",
    "    \n",
    "    # Instead of flattening, use Conv1D (kernel_size=1) as dense layers.\n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(late_dropout)(dense)\n",
    "    \n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(late_dropout)(dense)\n",
    "    \n",
    "    # Final classification layer (applied at every time step)\n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ local_mask_layer    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LocalMaskLayer</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ local_mask_layer… │\n",
       "│                     │                   │            │ local_mask_layer… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">92,320</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">230,560</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">276,672</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">331,968</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,912</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">331,968</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">331,968</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">11,584</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,056</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">174,804</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ up_sampling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">680</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ up_sampling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">680</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">349,224</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1360</span>)             │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">174,208</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m10\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ local_mask_layer    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mLocalMaskLayer\u001b[0m)    │ \u001b[38;5;34m5000\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m10\u001b[0m)  │          \u001b[38;5;34m1\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ local_mask_layer… │\n",
       "│                     │                   │            │ local_mask_layer… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m20\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m1,344\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │     \u001b[38;5;34m92,320\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │        \u001b[38;5;34m640\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │    \u001b[38;5;34m230,560\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │        \u001b[38;5;34m640\u001b[0m │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │     \u001b[38;5;34m10,400\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m276,672\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m331,968\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │     \u001b[38;5;34m30,912\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m331,968\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m331,968\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m11,584\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │     \u001b[38;5;34m37,056\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m340\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m340\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m340\u001b[0m)  │    \u001b[38;5;34m174,804\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ up_sampling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m340\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mUpSampling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m680\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ up_sampling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m680\u001b[0m) │    \u001b[38;5;34m349,224\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m1360\u001b[0m)             │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m174,208\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m16,512\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_15 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │        \u001b[38;5;34m645\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,482,146</span> (9.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,482,146\u001b[0m (9.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,479,074</span> (9.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,479,074\u001b[0m (9.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> (12.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,072\u001b[0m (12.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_modular_dcnn_model()\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
