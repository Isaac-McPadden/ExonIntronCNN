{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the file that was brought over to AWS Sagemaker but is also kind of outdated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonZeroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, average='weighted', threshold=0.5, name='non_zero_f1', **kwargs):\n",
    "        \"\"\"\n",
    "        Custom F1 score metric that only considers non-dominant classes (ignoring index 0).\n",
    "        \n",
    "        This version is designed for multi-encoded labels where:\n",
    "          - The dominant class (index 0) is represented as a hard label [1, 0, 0, ...]\n",
    "          - For non-dominant classes (indices 1 to num_classes-1), only an exact label of 1 is considered positive.\n",
    "            (Any partial credit/smoothed values below 1 are treated as 0.)\n",
    "          - Predictions are thresholded (default threshold = 0.5) to decide 1 vs. 0.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Total number of classes.\n",
    "            average (str): 'weighted' (default) to weight by support or 'macro' for a simple average.\n",
    "            threshold (float): Threshold on y_pred to decide a positive (default 0.5).\n",
    "            name (str): Name of the metric.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(CustomNonZeroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "        if average not in ['weighted', 'macro']:\n",
    "            raise ValueError(\"average must be 'weighted' or 'macro'\")\n",
    "        self.average = average\n",
    "\n",
    "        # Create state variables to accumulate counts for each class.\n",
    "        # We use a vector of length num_classes but we will update only indices 1...num_classes-1.\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros', dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the metric state.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Tensor of shape (batch_size, num_classes). These are multi-encoded labels.\n",
    "                    For non-dominant classes, a label is considered positive only if it is exactly 1.\n",
    "            y_pred: Tensor of shape (batch_size, num_classes) with predictions (e.g. probabilities).\n",
    "            sample_weight: Optional sample weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Flatten all dimensions except the last one (which should be num_classes).\n",
    "        y_true = tf.reshape(y_true, [-1, self.num_classes])\n",
    "        y_pred = tf.reshape(y_pred, [-1, self.num_classes])\n",
    "        \n",
    "        # We want to ignore the dominant class (index 0) and work on classes 1...num_classes-1.\n",
    "        # Assume y_true and y_pred are both of shape (batch_size, num_classes).\n",
    "        y_true_non_dominant = y_true[:, 1:]\n",
    "        y_pred_non_dominant = y_pred[:, 1:]\n",
    "        \n",
    "        # For ground truth: treat a class as positive only if its value is exactly 1.\n",
    "        one_value = tf.cast(1.0, dtype=y_true_non_dominant.dtype)\n",
    "        y_true_bin = tf.cast(tf.equal(y_true_non_dominant, one_value), tf.int32)\n",
    "        # For predictions: apply thresholding.\n",
    "        y_pred_bin = tf.cast(y_pred_non_dominant >= self.threshold, tf.int32)\n",
    "        \n",
    "        # (Optionally) apply sample weighting.\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.int32)\n",
    "            sample_weight = tf.reshape(sample_weight, (-1, 1))\n",
    "            y_true_bin = y_true_bin * sample_weight\n",
    "            y_pred_bin = y_pred_bin * sample_weight\n",
    "        \n",
    "        # Compute per-class true positives, false positives, and false negatives for non-dominant classes.\n",
    "        tp = tf.reduce_sum(tf.cast(y_true_bin * y_pred_bin, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true_bin) * y_pred_bin, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true_bin * (1 - y_pred_bin), tf.float32), axis=0)\n",
    "        \n",
    "        # Our state variables have length num_classes. We want to update only indices 1... with our computed values.\n",
    "        zeros = tf.zeros([1], dtype=tf.float32)\n",
    "        tp_update = tf.concat([zeros, tp], axis=0)\n",
    "        fp_update = tf.concat([zeros, fp], axis=0)\n",
    "        fn_update = tf.concat([zeros, fn], axis=0)\n",
    "        \n",
    "        self.true_positives.assign_add(tp_update)\n",
    "        self.false_positives.assign_add(fp_update)\n",
    "        self.false_negatives.assign_add(fn_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Computes the F1 score over the non-dominant classes (indices 1...num_classes-1).\n",
    "        \"\"\"\n",
    "        # Select non-dominant classes only.\n",
    "        tp = self.true_positives[1:]\n",
    "        fp = self.false_positives[1:]\n",
    "        fn = self.false_negatives[1:]\n",
    "        \n",
    "        precision = tf.math.divide_no_nan(tp, tp + fp)\n",
    "        recall = tf.math.divide_no_nan(tp, tp + fn)\n",
    "        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)\n",
    "        \n",
    "        if self.average == 'weighted':\n",
    "            support = tp + fn\n",
    "            weighted_f1 = tf.reduce_sum(f1 * support) / (tf.reduce_sum(support) + K.epsilon())\n",
    "            return weighted_f1\n",
    "        else:  # macro\n",
    "            return tf.reduce_mean(f1)\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state variables.\n",
    "        \"\"\"\n",
    "        for v in self.variables:\n",
    "            v.assign(tf.zeros_like(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_binary_crossentropy_loss(\n",
    "#     dominant_class_index=0,\n",
    "#     dominant_correct_multiplier=0.1,    # reward factor when the dominant class is correct\n",
    "#     dominant_incorrect_multiplier=2.0,    # penalty factor when the dominant class is predicted incorrectly\n",
    "#     other_class_multiplier=1.0,           # multiplier for non-dominant classes when y_true == 1\n",
    "#     smoothing_multiplier=1.0              # multiplier for non-dominant classes when y_true is a smoothed value (0 < y_true < 1)\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Returns a custom binary crossentropy loss function that treats the dominant class specially,\n",
    "#     and applies different multipliers for non-dominant classes based on their true label values.\n",
    "    \n",
    "#     For the dominant class (specified by dominant_class_index):\n",
    "#       - If y_true == 1, the loss is scaled by dominant_correct_multiplier.\n",
    "#       - Otherwise, it is scaled by dominant_incorrect_multiplier.\n",
    "    \n",
    "#     For non-dominant classes:\n",
    "#       - If y_true == 1, the loss is scaled by other_class_multiplier.\n",
    "#       - If 0 < y_true < 1 (e.g. label-smoothed values, typically in (0, 0.5]), the loss is scaled by smoothing_multiplier.\n",
    "#       - If y_true == 0, no additional scaling is applied.\n",
    "      \n",
    "#     Parameters:\n",
    "#       dominant_class_index (int): Index of the dominant class in the output vector.\n",
    "#       dominant_correct_multiplier (float): Multiplier for the loss when the dominant class is correctly predicted.\n",
    "#       dominant_incorrect_multiplier (float): Multiplier for the loss when the dominant class is incorrectly predicted.\n",
    "#       other_class_multiplier (float): Multiplier for non-dominant classes when the true label is 1.\n",
    "#       smoothing_multiplier (float): Multiplier for non-dominant classes when the true label is a smoothed value (0 < y_true < 1).\n",
    "      \n",
    "#     Returns:\n",
    "#       A callable loss function usable with model.compile(loss=...).\n",
    "#     \"\"\"\n",
    "#     def loss(y_true, y_pred):\n",
    "#         # Prevent issues with log(0)\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        \n",
    " \n",
    "#         # Compute standard element-wise binary crossentropy.\n",
    "#         base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "#                        (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "#         # Determine the number of classes.\n",
    "#         num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "#         # Create a one-hot mask for the dominant class.\n",
    "#         dominant_mask = tf.one_hot(dominant_class_index, depth=num_classes, dtype=y_true.dtype)\n",
    "#         # The complement selects all non-dominant columns.\n",
    "#         non_dominant_mask = 1 - dominant_mask\n",
    "        \n",
    "#         # --- Dominant Class Weighting ---\n",
    "#         # For the dominant class: if y_true == 1 use dominant_correct_multiplier; otherwise use dominant_incorrect_multiplier.\n",
    "#         dominant_true = y_true[:, dominant_class_index]  # Shape: (batch_size,)\n",
    "#         dominant_weight = tf.where(tf.equal(dominant_true, 1.0),\n",
    "#                                    dominant_correct_multiplier,\n",
    "#                                    dominant_incorrect_multiplier)  # Shape: (batch_size,)\n",
    "#         dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # Shape: (batch_size, 1)\n",
    "        \n",
    "#         # --- Non-Dominant Class Weighting ---\n",
    "#         # For non-dominant classes, apply:\n",
    "#         #   - other_class_multiplier if y_true == 1\n",
    "#         #   - smoothing_multiplier if 0 < y_true < 1 (i.e. a smoothed value)\n",
    "#         #   - otherwise (y_true == 0) leave as 1.\n",
    "#         non_dominant_weight = tf.where(\n",
    "#             tf.equal(y_true, 1.0),\n",
    "#             other_class_multiplier,\n",
    "#             tf.where(tf.greater(y_true, 0.0),\n",
    "#                      smoothing_multiplier,\n",
    "#                      1.0)\n",
    "#         )\n",
    "        \n",
    "#         # Combine the weights for each class.\n",
    "#         weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "#         # Compute and return the weighted loss.\n",
    "#         weighted_loss = base_loss * weights\n",
    "#         return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "\n",
    "\n",
    "# def custom_binary_crossentropy_loss(\n",
    "#     dominant_class_index=0,\n",
    "#     dominant_correct_multiplier=0.1,    # Reward factor when the dominant class is correct\n",
    "#     dominant_incorrect_multiplier=2.0,    # Penalty factor when the dominant class is predicted incorrectly\n",
    "#     other_class_multiplier=1.0,           # Multiplier for non-dominant classes when y_true == 1\n",
    "#     smoothing_multiplier=1.0              # Multiplier for non-dominant classes when y_true is a smoothed value (0 < y_true < 1)\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Returns a custom binary crossentropy loss function that treats the dominant class specially,\n",
    "#     and applies different multipliers for non-dominant classes based on their true label values.\n",
    "\n",
    "#     For the dominant class (specified by dominant_class_index):\n",
    "#       - If y_true == 1, the loss is scaled by dominant_correct_multiplier.\n",
    "#       - Otherwise, it is scaled by dominant_incorrect_multiplier.\n",
    "\n",
    "#     For non-dominant classes:\n",
    "#       - If y_true == 1, the loss is scaled by other_class_multiplier.\n",
    "#       - If 0 < y_true < 1 (e.g. label-smoothed values), the loss is scaled by smoothing_multiplier.\n",
    "#       - If y_true == 0, no additional scaling is applied.\n",
    "\n",
    "#     This version also reshapes the inputs so that it can handle batches with extra dimensions.\n",
    "\n",
    "#     Parameters:\n",
    "#       dominant_class_index (int): Index of the dominant class in the output vector.\n",
    "#       dominant_correct_multiplier (float): Multiplier for the loss when the dominant class is correctly predicted.\n",
    "#       dominant_incorrect_multiplier (float): Multiplier for the loss when the dominant class is incorrectly predicted.\n",
    "#       other_class_multiplier (float): Multiplier for non-dominant classes when the true label is 1.\n",
    "#       smoothing_multiplier (float): Multiplier for non-dominant classes when the true label is a smoothed value (0 < y_true < 1).\n",
    "\n",
    "#     Returns:\n",
    "#       A callable loss function usable with model.compile(loss=...).\n",
    "#     \"\"\"\n",
    "#     def loss(y_true, y_pred):\n",
    "#         # Prevent log(0) issues.\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "#         # Flatten all dimensions except the last one (which should represent the classes).\n",
    "#         y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "#         y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "#         # Compute the standard element-wise binary crossentropy.\n",
    "#         base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "#                        (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "#         # Determine the number of classes.\n",
    "#         num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "#         # Create a one-hot mask for the dominant class.\n",
    "#         # This yields a tensor of shape (num_classes,).\n",
    "#         dominant_mask = tf.one_hot(dominant_class_index, depth=num_classes, dtype=y_true.dtype)\n",
    "#         non_dominant_mask = 1 - dominant_mask\n",
    "        \n",
    "#         # --- Dominant Class Weighting ---\n",
    "#         # For the dominant class: if y_true == 1, use dominant_correct_multiplier; otherwise, use dominant_incorrect_multiplier.\n",
    "#         dominant_true = y_true[:, dominant_class_index]  # Shape: (N,)\n",
    "#         dominant_weight = tf.where(tf.equal(dominant_true, 1.0),\n",
    "#                                    dominant_correct_multiplier,\n",
    "#                                    dominant_incorrect_multiplier)  # Shape: (N,)\n",
    "#         dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # Shape: (N, 1)\n",
    "        \n",
    "#         # --- Non-Dominant Class Weighting ---\n",
    "#         # For non-dominant classes:\n",
    "#         #   - If y_true == 1, use other_class_multiplier.\n",
    "#         #   - If 0 < y_true < 1 (i.e. a smoothed value), use smoothing_multiplier.\n",
    "#         #   - Otherwise (y_true == 0) use 1.\n",
    "#         non_dominant_weight = tf.where(\n",
    "#             tf.equal(y_true, 1.0),\n",
    "#             other_class_multiplier,\n",
    "#             tf.where(tf.greater(y_true, 0.0),\n",
    "#                      smoothing_multiplier,\n",
    "#                      1.0)\n",
    "#         )\n",
    "        \n",
    "#         # Reshape the masks so they broadcast properly with the batch.\n",
    "#         dominant_mask = tf.reshape(dominant_mask, [1, num_classes])\n",
    "#         non_dominant_mask = tf.reshape(non_dominant_mask, [1, num_classes])\n",
    "        \n",
    "#         # Combine the weights: for each sample and each class,\n",
    "#         # the weight is dominant_weight for the dominant class and non_dominant_weight for the others.\n",
    "#         weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "#         # Compute the weighted loss.\n",
    "#         weighted_loss = base_loss * weights\n",
    "#         return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "\n",
    "@utils.register_keras_serializable()\n",
    "class CustomBinaryCrossentropyLoss(losses.Loss):\n",
    "    def __init__(self,\n",
    "                 dominant_class_index=0,\n",
    "                 # Multiplier n values (0 < n < 1) reward the loss function, n = 1 makes no effect, n > 1 scales up the punishment. n > 5 is likely to cause instability\n",
    "                 # Shouldn't set to 0 or lower because 0 kills any signal for the gradient to use and negative numbers get weird with logs \n",
    "                 dominant_correct_multiplier=0.99,    # \"Reward\" factor when the dominant class is correct\n",
    "                 dominant_incorrect_multiplier=2.5,     # Penalty factor when the dominant class is predicted incorrectly\n",
    "                 # Expanded non-dominant multipliers for hard labels:\n",
    "                 other_class_true_positive_multiplier=0.05,   # \"Reward\" when y_true==1 and prediction is positive (Strong reward for a bulls-eye)\n",
    "                 other_class_false_negative_multiplier=3.0,    # Punish when y_true==1 but prediction is negative (Punish a miss on a rare opportunity to find a target)\n",
    "                 other_class_false_positive_multiplier=1.0,    # Punish when y_true==0 but prediction is positive (Keeping neutral for now, no shame in guessing)\n",
    "                 other_class_true_negative_multiplier=0.99,     # Neutral when y_true==0 and prediction is negative (Small reward for an easy correct guess)\n",
    "                 # For smoothed labels (0 < y_true < 1)\n",
    "                 smoothing_multiplier=0.5,              # Scales the effect of the smoothed label. Pair decimals with smoothing_as_correct = True, n > 1 for False (Current setting rewards getting close to the target)\n",
    "                 smoothing_as_correct=True,             # If True, treat a high prediction as rewarded; if False, as punished\n",
    "                 threshold=0.5,                         # Threshold to decide if a prediction is “positive”\n",
    "                 name=\"custom_binary_crossentropy_loss\",\n",
    "                 reduction=\"sum_over_batch_size\"):\n",
    "        super().__init__(name=name)\n",
    "        self.dominant_class_index = dominant_class_index\n",
    "        self.dominant_correct_multiplier = dominant_correct_multiplier\n",
    "        self.dominant_incorrect_multiplier = dominant_incorrect_multiplier\n",
    "\n",
    "        self.other_class_true_positive_multiplier = other_class_true_positive_multiplier\n",
    "        self.other_class_false_negative_multiplier = other_class_false_negative_multiplier\n",
    "        self.other_class_false_positive_multiplier = other_class_false_positive_multiplier\n",
    "        self.other_class_true_negative_multiplier = other_class_true_negative_multiplier\n",
    "\n",
    "        self.smoothing_multiplier = smoothing_multiplier\n",
    "        self.smoothing_as_correct = smoothing_as_correct\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Prevent log(0) issues.\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Reshape to (batch_size, num_classes)\n",
    "        y_true = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "        \n",
    "        # Compute standard binary crossentropy (elementwise).\n",
    "        base_loss = - (y_true * tf.math.log(y_pred) +\n",
    "                       (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "        # Get number of classes.\n",
    "        num_classes = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Create masks for the dominant class vs. non-dominant.\n",
    "        dominant_mask = tf.one_hot(self.dominant_class_index, depth=num_classes, dtype=tf.float32)\n",
    "        non_dominant_mask = tf.cast(tf.constant(1.0, tf.float32) - dominant_mask, tf.float32)\n",
    "        \n",
    "        # --- Dominant Class Weighting ---\n",
    "        # For the dominant class, use one multiplier if the true label is 1,\n",
    "        # and another if it is 0.\n",
    "        dominant_true = y_true[:, self.dominant_class_index]  # shape: (batch_size,)\n",
    "        dominant_weight = tf.where(\n",
    "            tf.equal(dominant_true, tf.constant(1.0, dtype=y_true.dtype)),\n",
    "            tf.constant(self.dominant_correct_multiplier, dtype=y_true.dtype),\n",
    "            tf.constant(self.dominant_incorrect_multiplier, dtype=y_true.dtype)\n",
    "        )\n",
    "        dominant_weight = tf.expand_dims(dominant_weight, axis=1)  # shape: (batch_size, 1)\n",
    "        \n",
    "        # --- Non-Dominant Class Weighting ---\n",
    "        # For non-dominant classes, we now treat hard labels (exactly 0 or 1) separately\n",
    "        # from smoothed labels (0 < y_true < 1).\n",
    "        \n",
    "        # Determine conditions for hard labels.\n",
    "        is_hard_positive = tf.equal(y_true, tf.constant(1.0, dtype=y_true.dtype))\n",
    "        is_hard_negative = tf.equal(y_true, tf.constant(0.0, dtype=y_true.dtype))\n",
    "        is_hard = tf.logical_or(is_hard_positive, is_hard_negative)\n",
    "        \n",
    "        # Determine prediction condition: is the prediction \"positive\"?\n",
    "        pred_positive = tf.greater_equal(y_pred, tf.constant(self.threshold, dtype=y_true.dtype))\n",
    "        \n",
    "        # For hard labels:\n",
    "        # If y_true is 1:\n",
    "        #    - If prediction is positive: true positive multiplier.\n",
    "        #    - Else: false negative multiplier.\n",
    "        # If y_true is 0:\n",
    "        #    - If prediction is positive: false positive multiplier.\n",
    "        #    - Else: true negative multiplier.\n",
    "        hard_weight = tf.where(\n",
    "            tf.equal(y_true, tf.constant(1.0, dtype=y_true.dtype)),\n",
    "            tf.where(\n",
    "                pred_positive,\n",
    "                tf.constant(self.other_class_true_positive_multiplier, dtype=y_true.dtype),\n",
    "                tf.constant(self.other_class_false_negative_multiplier, dtype=y_true.dtype)\n",
    "            ),\n",
    "            tf.where(\n",
    "                tf.equal(y_true, tf.constant(0.0, dtype=y_true.dtype)),\n",
    "                tf.where(\n",
    "                    pred_positive,\n",
    "                    tf.constant(self.other_class_false_positive_multiplier, dtype=y_true.dtype),\n",
    "                    tf.constant(self.other_class_true_negative_multiplier, dtype=y_true.dtype)\n",
    "                ),\n",
    "                tf.constant(1.0, dtype=y_true.dtype)  # fallback, should not occur for a hard label.\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # For smoothed labels: (values strictly between 0 and 1)\n",
    "        is_smoothed = tf.logical_and(\n",
    "            tf.greater(y_true, tf.constant(0.0, dtype=y_true.dtype)),\n",
    "            tf.less(y_true, tf.constant(1.0, dtype=y_true.dtype))\n",
    "        )\n",
    "        # Here, we modulate the weight using the label’s value and a multiplier.\n",
    "        # We let the toggle smoothing_as_correct decide which way to go.\n",
    "        if self.smoothing_as_correct:\n",
    "            smoothed_weight = tf.where(\n",
    "                pred_positive,\n",
    "                1.0 - y_true * self.smoothing_multiplier,  # reward by lowering the loss\n",
    "                1.0 + y_true * self.smoothing_multiplier   # punish by increasing the loss\n",
    "            )\n",
    "        else:\n",
    "            smoothed_weight = tf.where(\n",
    "                pred_positive,\n",
    "                1.0 + y_true * self.smoothing_multiplier,  # punish\n",
    "                1.0 - y_true * self.smoothing_multiplier   # reward\n",
    "            )\n",
    "        # Combine the hard and smoothed weights for non-dominant classes.\n",
    "        non_dominant_weight = tf.where(\n",
    "            is_hard,\n",
    "            hard_weight,\n",
    "            tf.where(\n",
    "                is_smoothed,\n",
    "                smoothed_weight,\n",
    "                tf.constant(1.0, dtype=y_true.dtype)  # fallback\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Reshape the masks so they broadcast properly.\n",
    "        dominant_mask = tf.reshape(dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "        non_dominant_mask = tf.reshape(non_dominant_mask, tf.stack([tf.constant(1, dtype=tf.int32), num_classes]))\n",
    "        \n",
    "        # Combine weights: dominant classes get their weight; non-dominant get their own.\n",
    "        # dominant_weight has shape (batch_size, 1) and will broadcast.\n",
    "        weights = dominant_mask * dominant_weight + non_dominant_mask * non_dominant_weight\n",
    "        \n",
    "        # Compute weighted loss.\n",
    "        weighted_loss = base_loss * weights\n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'dominant_class_index': self.dominant_class_index,\n",
    "            'dominant_correct_multiplier': self.dominant_correct_multiplier,\n",
    "            'dominant_incorrect_multiplier': self.dominant_incorrect_multiplier,\n",
    "            'other_class_true_positive_multiplier': self.other_class_true_positive_multiplier,\n",
    "            'other_class_false_negative_multiplier': self.other_class_false_negative_multiplier,\n",
    "            'other_class_false_positive_multiplier': self.other_class_false_positive_multiplier,\n",
    "            'other_class_true_negative_multiplier': self.other_class_true_negative_multiplier,\n",
    "            'smoothing_multiplier': self.smoothing_multiplier,\n",
    "            'smoothing_as_correct': self.smoothing_as_correct,\n",
    "            'threshold': self.threshold\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,528</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">102,560</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">128,160</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,640</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">153,792</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">184,512</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,912</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,368</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ batch_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m10\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m6,528\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m82,048\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m1,408\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │    \u001b[38;5;34m102,560\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │        \u001b[38;5;34m640\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │    \u001b[38;5;34m128,160\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │        \u001b[38;5;34m640\u001b[0m │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │     \u001b[38;5;34m20,640\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m160\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m153,792\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m184,512\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m3,968\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │        \u001b[38;5;34m768\u001b[0m │ conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │     \u001b[38;5;34m30,912\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m330\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m42,368\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m16,512\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │        \u001b[38;5;34m645\u001b[0m │ batch_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">779,429</span> (2.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m779,429\u001b[0m (2.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">776,741</span> (2.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m776,741\u001b[0m (2.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,688</span> (10.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,688\u001b[0m (10.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_dcnn_model(\n",
    "    input_dim=5,\n",
    "    sequence_length=5000,\n",
    "    num_classes=5\n",
    "):\n",
    "    inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "    # Condensed positional encoding block.  See cnn for description\n",
    "    positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    def tile_to_batch(z):\n",
    "        pe, x = z\n",
    "        return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "    pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "\n",
    "    concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "\n",
    "    cnn = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(concat_input)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Dropout(0.3)(cnn)\n",
    "    # We use six layers with increasing dilation rates to capture a wider receptive field.\n",
    "    # Dilating convolutional blocks with dropout (pooling is bad because exact sequence matters)\n",
    "    skip = concat_input\n",
    "    skip = layers.Conv1D(filters=128, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=128, kernel_size=5, dilation_rate=1, activation='relu', padding='same')(concat_input)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.3)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.3)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=160, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=5, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.3)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=160, kernel_size=5, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.3)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=192, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=5, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.3)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=192, kernel_size=5, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(0.3)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn])\n",
    "\n",
    "    # Instead of flattening, use Conv1D with kernel_size=1 as dense layers:\n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(0.3)(dense)\n",
    "    \n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "\n",
    "    # Final classification layer applied at every time step:\n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "dcnn_model = create_dcnn_model(5, 5000, 5)\n",
    "dcnn_model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss=CustomBinaryCrossentropyLoss(),\n",
    "                  metrics=[CustomNonZeroF1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
    "dcnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuner Search 0 Params:  \n",
    "\n",
    "explore_filters_1 = hp.Choice('explore_filters_1', values=[64, 128, 192], default=128)\n",
    "\n",
    "explore_filters_2 = hp.Choice('explore_filters_2', values=[80, 160, 240], default=160)\n",
    "\n",
    "explore_filters_3 = hp.Choice('explore_filters_3', values=[96, 192, 288], default=192)\n",
    "\n",
    "explore_kernel_size = hp.Choice('explore_kernel_size', values = [3, 5, 6, 7, 9], default=5)\n",
    "\n",
    "explore_dropout = hp.Float('explore_dropout', min_value=0.1, max_value=0.5, step=0.1, default=0.3)\n",
    "\n",
    "learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    \n",
    "dominant_correct_multiplier = hp.Float('dominant_correct_multiplier', 0, 0.1, step=0.01)\n",
    "\n",
    "dominant_incorrect_multiplier = hp.Float('dominant_incorrect_multiplier', 1.0, 5.0, step=0.5)\n",
    "\n",
    "other_class_multiplier = hp.Float('other_class_multiplier', 0.5, 3.0, step=0.5)\n",
    "\n",
    "smoothing_multiplier = hp.Float('smoothing_multiplier', 0.5, 1.5, step=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore_filters_1 = hp.Choice('explore_filters_1', values=[128], default=128)\n",
    "\n",
    "explore_filters_2 = hp.Choice('explore_filters_2', values=[160, 240], default=160)\n",
    "\n",
    "explore_filters_3 = hp.Choice('explore_filters_3', values=[192, 288], default=192)\n",
    "\n",
    "explore_kernel_size = hp.Choice('explore_kernel_size', values = [3, 5, 6, 7, 9], default=5)\n",
    "\n",
    "explore_dropout = hp.Float('explore_dropout', 0.2, 0.3, step=0.1, default=0.3)\n",
    "\n",
    "learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, sampling='LOG', default=1e-3)  \n",
    "\n",
    "dominant_correct_multiplier = hp.Float('dominant_correct_multiplier', 0.06, 0.08, step=0.01, default=0.07)\n",
    "\n",
    "dominant_incorrect_multiplier = hp.Float('dominant_incorrect_multiplier', 2.0, 3.5, step=0.5, default=2.0)\n",
    "\n",
    "other_class_multiplier = hp.Float('other_class_multiplier', 0.5, 2.0, step=0.5, default=2.0)\n",
    "\n",
    "smoothing_multiplier = hp.Float('smoothing_multiplier', 0.5, 0.7, step=0.2, default=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_dcnn_model(hp):\n",
    "    \n",
    "    input_dim=5\n",
    "    sequence_length=5000\n",
    "    num_classes=5\n",
    "    inputs = Input(shape=(sequence_length, input_dim))\n",
    "    \n",
    "    # Condensed positional encoding block.  See cnn for description\n",
    "    positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_classes)(positions)\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    def tile_to_batch(z):\n",
    "        pe, x = z\n",
    "        return tf.tile(pe, [tf.shape(x)[0], 1, 1])\n",
    "    pos_encoding = layers.Lambda(tile_to_batch)([pos_encoding, inputs])\n",
    "\n",
    "    concat_input = layers.Concatenate(axis=-1)([inputs, pos_encoding])\n",
    "    \n",
    "    explore_filters_1 = hp.Choice('explore_filters_1', values=[64, 128, 192], default=128)\n",
    "    explore_filters_2 = hp.Choice('explore_filters_2', values=[80, 160, 240], default=160)\n",
    "    explore_filters_3 = hp.Choice('explore_filters_3', values=[96, 192, 288], default=192)\n",
    "    explore_kernel_size = hp.Choice('explore_kernel_size', values = [3, 5, 6, 7, 9], default=5)\n",
    "    explore_dropout = hp.Float('explore_dropout', min_value=0.1, max_value=0.5, step=0.1, default=0.3)\n",
    "\n",
    "    cnn = layers.Conv1D(filters=explore_filters_1, kernel_size=explore_kernel_size, activation='relu', padding='same')(concat_input)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Dropout(explore_dropout)(cnn)\n",
    "    # We use six layers with increasing dilation rates to capture a wider receptive field.\n",
    "    # Dilating convolutional blocks with dropout (pooling is bad because exact sequence matters)\n",
    "    skip = concat_input\n",
    "    skip = layers.Conv1D(filters=explore_filters_1, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=explore_filters_1, kernel_size=explore_kernel_size, dilation_rate=1, activation='relu', padding='same')(concat_input)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=explore_filters_1, kernel_size=explore_kernel_size, dilation_rate=2, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=explore_filters_2, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=explore_filters_2, kernel_size=explore_kernel_size, dilation_rate=4, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=explore_filters_2, kernel_size=explore_kernel_size, dilation_rate=8, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    skip = dcnn\n",
    "    skip = layers.Conv1D(filters=explore_filters_3, kernel_size=1, padding='same')(skip)\n",
    "    dcnn = layers.Conv1D(filters=explore_filters_3, kernel_size=explore_kernel_size, dilation_rate=16, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    \n",
    "    dcnn = layers.Conv1D(filters=explore_filters_3, kernel_size=explore_kernel_size, dilation_rate=32, activation='relu', padding='same')(dcnn)\n",
    "    dcnn = layers.BatchNormalization()(dcnn)\n",
    "    dcnn = layers.Dropout(explore_dropout)(dcnn)\n",
    "    dcnn = layers.Add()([dcnn, skip])\n",
    "    \n",
    "    second_concat = layers.Concatenate(axis=-1)([concat_input, cnn, dcnn])\n",
    "\n",
    "    # Instead of flattening, use Conv1D with kernel_size=1 as dense layers:\n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(second_concat)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "    dense = layers.Dropout(explore_dropout)(dense)\n",
    "    \n",
    "    dense = layers.Conv1D(128, kernel_size=1, activation='relu')(dense)\n",
    "    dense = layers.BatchNormalization()(dense)\n",
    "\n",
    "    # Final classification layer applied at every time step:\n",
    "    outputs = layers.Conv1D(num_classes, kernel_size=1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    \n",
    "    dominant_correct_multiplier = hp.Float('dominant_correct_multiplier', 0.95, 1, step=0.01)\n",
    "    dominant_incorrect_multiplier = hp.Float('dominant_incorrect_multiplier', 1.0, 5.0, step=0.5)\n",
    "    other_class_true_positive_multiplier = hp.Float('other_class_true_positive_multiplier', 0.025, 0.15, step=0.025)\n",
    "    other_class_false_negative_multiplier = hp.Float('other_class_false_negative_multiplier', 1.0, 5.0, step=0.5)\n",
    "    other_class_false_positive_multiplier = hp.Float('other_class_false_positive_multiplier', 1.0, 2.5, step=0.5)\n",
    "    other_class_true_negative_multiplier = hp.Float('other_class_true_negative_multiplier', 1, 2.5, step=0.5)\n",
    "    smoothing_multiplier = hp.Float('smoothing_multiplier', 0.2, 0.8, step=0.1)\n",
    "\n",
    "    loss_fn = CustomBinaryCrossentropyLoss(\n",
    "        dominant_class_index=0,\n",
    "        dominant_correct_multiplier=dominant_correct_multiplier,\n",
    "        dominant_incorrect_multiplier=dominant_incorrect_multiplier,\n",
    "        other_class_true_positive_multiplier=other_class_true_positive_multiplier,\n",
    "        other_class_false_negative_multiplier=other_class_false_negative_multiplier,\n",
    "        other_class_false_positive_multiplier=other_class_false_positive_multiplier,\n",
    "        other_class_true_negative_multiplier=other_class_true_negative_multiplier,\n",
    "        smoothing_multiplier=smoothing_multiplier,\n",
    "        smoothing_as_correct=True\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=loss_fn,\n",
    "                  metrics=[CustomNonZeroF1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
    "    return model\n",
    "\n",
    "\n",
    "# dcnn_model = create_dcnn_model(5, 5000, 5)\n",
    "# dcnn_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# dcnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train: 200985, Val: 23645, Test: 11824   ~39 Percent of the chr genome.  ~2-3 percent of the genome is exons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_exact_records(dataset: tf.data.Dataset, total_records, num_to_drop, seed=None):\n",
    "    '''\n",
    "    Function to drop n records from data before constructing parsed dataset.  \n",
    "    Mostly for bug checking.\n",
    "    '''\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    drop_indices = set(np.random.choice(total_records, num_to_drop, replace=False))\n",
    "    dataset = dataset.enumerate()\n",
    "    dataset = dataset.filter(lambda i, x: ~tf.reduce_any(tf.equal(i, list(drop_indices))))\n",
    "    dataset = dataset.map(lambda i, x: x)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def parse_chunk_example(serialized_example):\n",
    "    \"\"\"\n",
    "    Parses a single serialized tf.train.Example back into tensors.\n",
    "    Used in testing datasets and in piping tfrecords to DL Algorithms\n",
    "    \"\"\"\n",
    "    feature_spec = {\n",
    "        'X':          tf.io.VarLenFeature(tf.float32),\n",
    "        'y':          tf.io.VarLenFeature(tf.float32),\n",
    "        'record_id':  tf.io.FixedLenFeature([], tf.string),\n",
    "        'cstart':     tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'cend':       tf.io.FixedLenFeature([1], tf.int64),\n",
    "        'strand':     tf.io.FixedLenFeature([], tf.string),\n",
    "        'chunk_size': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    }\n",
    "    \n",
    "    parsed = tf.io.parse_single_example(serialized_example, feature_spec)\n",
    "    \n",
    "    # chunk_size is shape [1]\n",
    "    chunk_size = parsed['chunk_size'][0]\n",
    "    \n",
    "    # Convert sparse to dense\n",
    "    X_flat = tf.sparse.to_dense(parsed['X'])\n",
    "    y_flat = tf.sparse.to_dense(parsed['y'])\n",
    "\n",
    "    # Reshape X to [chunk_size, 5]\n",
    "    X_reshaped = tf.reshape(X_flat, [chunk_size, 5])\n",
    "    # Reshape y to [chunk_size], probably redundant\n",
    "    y_reshaped = tf.reshape(y_flat, [chunk_size, 5])\n",
    "    \n",
    "    record_id = parsed['record_id']\n",
    "    cstart    = parsed['cstart'][0]\n",
    "    cend      = parsed['cend'][0]\n",
    "    strand    = parsed['strand']\n",
    "    \n",
    "    return X_reshaped, y_reshaped, record_id, cstart, cend, strand\n",
    "\n",
    "\n",
    "def prepare_for_model(X, y, record_id, cstart, cend, strand):\n",
    "    '''\n",
    "    Helper function that extracts and reshapes parsed data for feeding to DL Models\n",
    "    '''\n",
    "    # Expand last dimension of y from (batch_size, 5000) to (batch_size, 5000, 1)\n",
    "    # y = tf.expand_dims(y, axis=-1) turns out this line is not needed\n",
    "    # Return only (X, y). Discard the extra columns for training knowing that \n",
    "    # they still exist in the TestValTrain originals if we need them\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def prep_dataset_from_tfrecord(\n",
    "    tfrecord_path,\n",
    "    batch_size=28,\n",
    "    compression_type='GZIP',\n",
    "    shuffled = False,\n",
    "    shuffle_buffer=25000,\n",
    "    total_records=None,\n",
    "    num_to_drop=None,\n",
    "    seed=None\n",
    "):\n",
    "    '''\n",
    "    Imports tfrecord and shuffles it then parses it for use in fitting a model\n",
    "    '''\n",
    "    # Loads in records in a round robin fashion for slightly increased mixing\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path, compression_type=compression_type, num_parallel_reads = tf.data.AUTOTUNE)\n",
    "    \n",
    "    if num_to_drop:\n",
    "        dataset = drop_exact_records(dataset, total_records=total_records, num_to_drop=num_to_drop, seed=seed)\n",
    "    \n",
    "    if shuffled == True:\n",
    "        # Shuffle at the record level\n",
    "        dataset = dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
    "        \n",
    "    \n",
    "    dataset = dataset.map(parse_chunk_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(prepare_for_model, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # dataset = dataset.map(lambda x, y: (x, tf.cast(y, tf.int32))) # found out tensorflow wants int32 in y # Note: Not anymore due to change in label format\n",
    "\n",
    "    # Rebatch parsed and prefetch for efficient reading\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimit(callbacks.Callback):\n",
    "    def __init__(self, max_time_seconds):\n",
    "        super().__init__()\n",
    "        self.max_time_seconds = max_time_seconds\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    # def on_batch_end(self, batch, logs=None):\n",
    "    #     if time.time() - self.start_time > self.max_time_seconds:\n",
    "    #         self.model.stop_training = True\n",
    "    \n",
    "    # def on_train_batch_end(self, batch, logs=None):  # ✅ Runs more frequently than `on_batch_end`\n",
    "    #     elapsed_time = time.time() - self.start_time\n",
    "    #     if elapsed_time > self.max_time_seconds:\n",
    "    #         print(f\"\\n⏳ Time limit of {self.max_time_seconds} sec reached. Stopping training!\")\n",
    "    #         self.model.stop_training = True  # 🔥 Stops training mid-batch\n",
    "    \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if elapsed_time > self.max_time_seconds:\n",
    "            print(f\"\\n⏳ Time limit of {self.max_time_seconds} sec reached. Stopping training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):  # New method added\n",
    "        if time.time() - self.start_time > self.max_time_seconds:\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class DebugCallback(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\n🚀 Starting Epoch {epoch+1}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if batch % 1000 == 0:\n",
    "            print(f\"🔄 Processing Batch {batch}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % 1000 == 0:\n",
    "            print(f\"✅ Finished Batch {batch}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\n🏁 Epoch {epoch+1} Completed!\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "class CleanupCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Example: force garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # If you need more extensive cleanup, you can add it here.\n",
    "        # e.g., close files, flush logs, free external resources, etc.\n",
    "        print(f\"Cleanup done at the end of epoch {epoch+1}\")\n",
    "        \n",
    "os.makedirs(datasets_path + 'checkpoints', exist_ok=True)\n",
    "\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    filepath=datasets_path + 'checkpoints/epoch-{epoch:02d}-val_loss-{val_loss:.4f}.keras',\n",
    "    monitor='val_loss',          # what metric to name file on\n",
    "    save_best_only=False,        # save model every epoch\n",
    "    save_weights_only=False,     # save full model (architecture + weights)\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "early_stopping_cb = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 20m 19s]\n",
      "val_loss: 0.0633837878704071\n",
      "\n",
      "Best val_loss So Far: 0.05201800912618637\n",
      "Total elapsed time: 02h 49m 00s\n",
      "Results summary\n",
      "Results in DCNN_Tuner_4/dcnn_hyperparam_opt\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0005 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 64\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 9\n",
      "explore_dropout: 0.4\n",
      "learning_rate: 0.0005343042689938801\n",
      "dominant_correct_multiplier: 0.95\n",
      "dominant_incorrect_multiplier: 2.0\n",
      "other_class_true_positive_multiplier: 0.125\n",
      "other_class_false_negative_multiplier: 5.0\n",
      "other_class_false_positive_multiplier: 2.0\n",
      "other_class_true_negative_multiplier: 1.0\n",
      "smoothing_multiplier: 0.30000000000000004\n",
      "tuner/epochs: 8\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0004\n",
      "Score: 0.05201800912618637\n",
      "\n",
      "Trial 0004 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 64\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 9\n",
      "explore_dropout: 0.4\n",
      "learning_rate: 0.0005343042689938801\n",
      "dominant_correct_multiplier: 0.95\n",
      "dominant_incorrect_multiplier: 2.0\n",
      "other_class_true_positive_multiplier: 0.125\n",
      "other_class_false_negative_multiplier: 5.0\n",
      "other_class_false_positive_multiplier: 2.0\n",
      "other_class_true_negative_multiplier: 1.0\n",
      "smoothing_multiplier: 0.30000000000000004\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.056114036589860916\n",
      "\n",
      "Trial 0006 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 64\n",
      "explore_filters_2: 80\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 9\n",
      "explore_dropout: 0.1\n",
      "learning_rate: 0.0007251386418532748\n",
      "dominant_correct_multiplier: 0.95\n",
      "dominant_incorrect_multiplier: 4.0\n",
      "other_class_true_positive_multiplier: 0.05\n",
      "other_class_false_negative_multiplier: 3.5\n",
      "other_class_false_positive_multiplier: 1.5\n",
      "other_class_true_negative_multiplier: 1.5\n",
      "smoothing_multiplier: 0.8\n",
      "tuner/epochs: 8\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0001\n",
      "Score: 0.05832061916589737\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 64\n",
      "explore_filters_2: 80\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 9\n",
      "explore_dropout: 0.1\n",
      "learning_rate: 0.0007251386418532748\n",
      "dominant_correct_multiplier: 0.95\n",
      "dominant_incorrect_multiplier: 4.0\n",
      "other_class_true_positive_multiplier: 0.05\n",
      "other_class_false_negative_multiplier: 3.5\n",
      "other_class_false_positive_multiplier: 1.5\n",
      "other_class_true_negative_multiplier: 1.5\n",
      "smoothing_multiplier: 0.8\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.06151838228106499\n",
      "\n",
      "Trial 0009 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 80\n",
      "explore_filters_3: 96\n",
      "explore_kernel_size: 7\n",
      "explore_dropout: 0.4\n",
      "learning_rate: 0.0002598597527871886\n",
      "dominant_correct_multiplier: 0.99\n",
      "dominant_incorrect_multiplier: 1.5\n",
      "other_class_true_positive_multiplier: 0.05\n",
      "other_class_false_negative_multiplier: 2.0\n",
      "other_class_false_positive_multiplier: 2.0\n",
      "other_class_true_negative_multiplier: 2.5\n",
      "smoothing_multiplier: 0.6000000000000001\n",
      "tuner/epochs: 8\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.0633837878704071\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 96\n",
      "explore_kernel_size: 6\n",
      "explore_dropout: 0.5\n",
      "learning_rate: 0.0033117989675966585\n",
      "dominant_correct_multiplier: 0.96\n",
      "dominant_incorrect_multiplier: 5.0\n",
      "other_class_true_positive_multiplier: 0.125\n",
      "other_class_false_negative_multiplier: 4.0\n",
      "other_class_false_positive_multiplier: 1.0\n",
      "other_class_true_negative_multiplier: 2.5\n",
      "smoothing_multiplier: 0.30000000000000004\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.06495395302772522\n",
      "\n",
      "Trial 0008 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 64\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 5\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.00013529562061620576\n",
      "dominant_correct_multiplier: 0.95\n",
      "dominant_incorrect_multiplier: 4.5\n",
      "other_class_true_positive_multiplier: 0.05\n",
      "other_class_false_negative_multiplier: 3.5\n",
      "other_class_false_positive_multiplier: 2.0\n",
      "other_class_true_negative_multiplier: 2.5\n",
      "smoothing_multiplier: 0.30000000000000004\n",
      "tuner/epochs: 8\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.06691107153892517\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 5\n",
      "explore_dropout: 0.4\n",
      "learning_rate: 0.0012024531988653548\n",
      "dominant_correct_multiplier: 0.98\n",
      "dominant_incorrect_multiplier: 2.0\n",
      "other_class_true_positive_multiplier: 0.125\n",
      "other_class_false_negative_multiplier: 3.5\n",
      "other_class_false_positive_multiplier: 2.0\n",
      "other_class_true_negative_multiplier: 2.5\n",
      "smoothing_multiplier: 0.7\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.07027754187583923\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 64\n",
      "explore_filters_2: 80\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 9\n",
      "explore_dropout: 0.5\n",
      "learning_rate: 0.0001285970125975765\n",
      "dominant_correct_multiplier: 0.97\n",
      "dominant_incorrect_multiplier: 4.5\n",
      "other_class_true_positive_multiplier: 0.07500000000000001\n",
      "other_class_false_negative_multiplier: 2.0\n",
      "other_class_false_positive_multiplier: 2.0\n",
      "other_class_true_negative_multiplier: 2.5\n",
      "smoothing_multiplier: 0.30000000000000004\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.07989492267370224\n",
      "\n",
      "Trial 0007 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 64\n",
      "explore_filters_2: 80\n",
      "explore_filters_3: 96\n",
      "explore_kernel_size: 3\n",
      "explore_dropout: 0.5\n",
      "learning_rate: 0.0007860854030984577\n",
      "dominant_correct_multiplier: 1.0\n",
      "dominant_incorrect_multiplier: 4.0\n",
      "other_class_true_positive_multiplier: 0.1\n",
      "other_class_false_negative_multiplier: 3.0\n",
      "other_class_false_positive_multiplier: 2.5\n",
      "other_class_true_negative_multiplier: 2.0\n",
      "smoothing_multiplier: 0.8\n",
      "tuner/epochs: 8\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.08439307659864426\n",
      "Best Hyperparameters:\n",
      "{'explore_filters_1': 64, 'explore_filters_2': 160, 'explore_filters_3': 192, 'explore_kernel_size': 9, 'explore_dropout': 0.4, 'learning_rate': 0.0005343042689938801, 'dominant_correct_multiplier': 0.95, 'dominant_incorrect_multiplier': 2.0, 'other_class_true_positive_multiplier': 0.125, 'other_class_false_negative_multiplier': 5.0, 'other_class_false_positive_multiplier': 2.0, 'other_class_true_negative_multiplier': 1.0, 'smoothing_multiplier': 0.30000000000000004, 'tuner/epochs': 8, 'tuner/initial_epoch': 3, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0004'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(False)\n",
    "# Train: 200985, Val: 23645, Test: 11824    \n",
    "max_time_seconds = 3600/3  # 1 hour is 3600 seconds\n",
    "batch_size = 28\n",
    "epochs = 8  # Set high enough to allow stopping by time\n",
    "steps_per_epoch = 1500\n",
    "\n",
    "print('Compiling train dataset')\n",
    "train_dataset = prep_dataset_from_tfrecord(datasets_path + \"TestValTrain/train.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=True,\n",
    "                                shuffle_buffer=10000,\n",
    "                                total_records=200985,\n",
    "                                num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "                                )\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "print('Compiling val dataset')\n",
    "val_dataset = prep_dataset_from_tfrecord(datasets_path + \"TestValTrain/val.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=False,\n",
    "                                shuffle_buffer=5000,\n",
    "                                total_records=23645,\n",
    "                                num_to_drop=13, # Batch size 28 leaves remainder of 13 records\n",
    "                                seed=42 # Seed for dropping the same 13 records every time\n",
    "                                )\n",
    "\n",
    "# test_dataset = prep_dataset_from_tfrecord(datasets_path + \"TestValTrain/test.tfrecord.gz\",\n",
    "#                                 batch_size=batch_size, \n",
    "#                                 compression_type='GZIP', \n",
    "#                                 shuffled=False,\n",
    "#                                 shuffle_buffer=5000,\n",
    "#                                 total_records=11824,\n",
    "#                                 num_to_drop=8, # Batch size 28 leaves remainder of 13 records\n",
    "#                                 seed=42 # Seed for dropping the same 8 records every time\n",
    "#                                 )\n",
    "tuner = kt.Hyperband(\n",
    "tune_dcnn_model,\n",
    "objective='val_loss',\n",
    "max_epochs=8,\n",
    "factor=3,\n",
    "directory='DCNN_Tuner_4',\n",
    "project_name='dcnn_hyperparam_opt',\n",
    "overwrite=False\n",
    ")\n",
    "\n",
    "stop_early = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "time_limit_callback = TimeLimit(max_time_seconds=max_time_seconds)\n",
    "\n",
    "tuner.search(train_dataset, epochs=8, steps_per_epoch = steps_per_epoch, validation_data=val_dataset, callbacks=[stop_early, time_limit_callback])\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model and hyperparameters:\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from DCNN_Tuner_2/dcnn_hyperparam_opt/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "tune_dcnn_model,\n",
    "objective='val_loss',\n",
    "max_epochs=20,\n",
    "factor=3,\n",
    "directory='DCNN_Tuner_2',\n",
    "project_name='dcnn_hyperparam_opt',\n",
    "overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters so far: {'explore_filters_1': 128, 'explore_filters_2': 240, 'explore_filters_3': 192, 'explore_kernel_size': 7, 'explore_dropout': 0.2, 'learning_rate': 0.0006232206265540669, 'dominant_correct_multiplier': 0.06, 'dominant_incorrect_multiplier': 2.5, 'other_class_multiplier': 1.5, 'smoothing_multiplier': 0.5, 'tuner/epochs': 7, 'tuner/initial_epoch': 3, 'tuner/bracket': 2, 'tuner/round': 1, 'tuner/trial_id': '0000'}\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters so far:\", best_hps.values)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 10\n",
      "explore_filters_1 (Choice)\n",
      "{'default': 128, 'conditions': [], 'values': [64, 128, 192], 'ordered': True}\n",
      "explore_filters_2 (Choice)\n",
      "{'default': 160, 'conditions': [], 'values': [80, 160, 240], 'ordered': True}\n",
      "explore_filters_3 (Choice)\n",
      "{'default': 192, 'conditions': [], 'values': [96, 192, 288], 'ordered': True}\n",
      "explore_kernel_size (Choice)\n",
      "{'default': 5, 'conditions': [], 'values': [3, 5, 6, 7, 9], 'ordered': True}\n",
      "explore_dropout (Float)\n",
      "{'default': 0.3, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "dominant_correct_multiplier (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.1, 'step': 0.01, 'sampling': 'linear'}\n",
      "dominant_incorrect_multiplier (Float)\n",
      "{'default': 1.0, 'conditions': [], 'min_value': 1.0, 'max_value': 5.0, 'step': 0.5, 'sampling': 'linear'}\n",
      "other_class_multiplier (Float)\n",
      "{'default': 0.5, 'conditions': [], 'min_value': 0.5, 'max_value': 3.0, 'step': 0.5, 'sampling': 'linear'}\n",
      "smoothing_multiplier (Float)\n",
      "{'default': 0.5, 'conditions': [], 'min_value': 0.5, 'max_value': 1.5, 'step': 0.2, 'sampling': 'linear'}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "search_space = tuner.search_space_summary()\n",
    "print(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in DCNN_Tuner_2/dcnn_hyperparam_opt\n",
      "Showing 17 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0015 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 7\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0006232206265540669\n",
      "dominant_correct_multiplier: 0.06\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 1.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0000\n",
      "Score: 0.025108424946665764\n",
      "\n",
      "Trial 0012 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 9\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0006860211454828152\n",
      "dominant_correct_multiplier: 0.06999999999999999\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 2.0\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0009\n",
      "Score: 0.025149695575237274\n",
      "\n",
      "Trial 0014 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 7\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.00046143082283313386\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 1.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0010\n",
      "Score: 0.02543702907860279\n",
      "\n",
      "Trial 0013 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 7\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0009533207293341387\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 3.0\n",
      "other_class_multiplier: 2.0\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0003\n",
      "Score: 0.025567123666405678\n",
      "\n",
      "Trial 0009 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 9\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0006860211454828152\n",
      "dominant_correct_multiplier: 0.06999999999999999\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 2.0\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.02574654296040535\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 7\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0009533207293341387\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 3.0\n",
      "other_class_multiplier: 2.0\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.026216724887490273\n",
      "\n",
      "Trial 0010 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 7\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.00046143082283313386\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 1.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.026313703507184982\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 7\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0006232206265540669\n",
      "dominant_correct_multiplier: 0.06\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 1.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.026493828743696213\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 6\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0003097459302945507\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 3.0\n",
      "other_class_multiplier: 1.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.026798035949468613\n",
      "\n",
      "Trial 0006 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 6\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0003718338102349428\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 2.0\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.026821404695510864\n",
      "\n",
      "Trial 0008 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 5\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.00032989099516752955\n",
      "dominant_correct_multiplier: 0.06\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 1.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.02788635902106762\n",
      "\n",
      "Trial 0004 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 6\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.000157823209001387\n",
      "dominant_correct_multiplier: 0.06999999999999999\n",
      "dominant_incorrect_multiplier: 2.0\n",
      "other_class_multiplier: 2.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.02794300578534603\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 6\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.00013769866215964098\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 3.0\n",
      "other_class_multiplier: 2.0\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.02876814268529415\n",
      "\n",
      "Trial 0011 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 9\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.00011029996196383996\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 2.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.02892313525080681\n",
      "\n",
      "Trial 0007 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 288\n",
      "explore_kernel_size: 3\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0003858617807411013\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 2.0\n",
      "other_class_multiplier: 2.0\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.0314442440867424\n",
      "\n",
      "Trial 0005 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 160\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 3\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0001499412374440074\n",
      "dominant_correct_multiplier: 0.08\n",
      "dominant_incorrect_multiplier: 3.5\n",
      "other_class_multiplier: 2.0\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.03490055352449417\n",
      "\n",
      "Trial 0016 summary\n",
      "Hyperparameters:\n",
      "explore_filters_1: 128\n",
      "explore_filters_2: 240\n",
      "explore_filters_3: 192\n",
      "explore_kernel_size: 7\n",
      "explore_dropout: 0.2\n",
      "learning_rate: 0.0006232206265540669\n",
      "dominant_correct_multiplier: 0.06\n",
      "dominant_incorrect_multiplier: 2.5\n",
      "other_class_multiplier: 1.5\n",
      "smoothing_multiplier: 0.5\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0015\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "results = tuner.results_summary(num_trials=17)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 Complete [00h 20m 35s]\n",
      "val_loss: 0.030874434858560562\n",
      "\n",
      "Best val_loss So Far: 0.024632375687360764\n",
      "Total elapsed time: 08h 36m 40s\n",
      "\n",
      "Search: Running Trial #22\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "128               |128               |explore_filters_1\n",
      "160               |160               |explore_filters_2\n",
      "288               |288               |explore_filters_3\n",
      "6                 |9                 |explore_kernel_size\n",
      "0.2               |0.2               |explore_dropout\n",
      "0.00029791        |0.00068602        |learning_rate\n",
      "0.08              |0.07              |dominant_correct_multiplier\n",
      "3.5               |2.5               |dominant_incorrect_multiplier\n",
      "2                 |2                 |other_class_multiplier\n",
      "0.5               |0.5               |smoothing_multiplier\n",
      "7                 |20                |tuner/epochs\n",
      "0                 |7                 |tuner/initial_epoch\n",
      "1                 |2                 |tuner/bracket\n",
      "0                 |2                 |tuner/round\n",
      "\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_early\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCleanupCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTimeLimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_time_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_time_seconds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras_tuner/src/tuners/hyperband.py:427\u001b[0m, in \u001b[0;36mHyperband.run_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuner/epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    426\u001b[0m     fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuner/initial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(train_dataset, epochs=20, steps_per_epoch = steps_per_epoch, validation_data=val_dataset, callbacks=[stop_early, CleanupCallback(), TimeLimit(max_time_seconds=max_time_seconds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.run_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling train dataset\n",
      "X shape: (28, 5000, 5)\n",
      "y shape: (28, 5000, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 22:11:57.955635: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "max_time_seconds = 60  # 1 hour is 3600 seconds\n",
    "batch_size = 28\n",
    "epochs = 100  # Set high enough to allow stopping by time\n",
    "steps_per_epoch = 7178\n",
    "\n",
    "print('Compiling train dataset')\n",
    "train_dataset = prep_dataset_from_tfrecord(datasets_path + \"TestValTrain/train.tfrecord.gz\",\n",
    "                                batch_size=batch_size, \n",
    "                                compression_type='GZIP', \n",
    "                                shuffled=True,\n",
    "                                shuffle_buffer=10000,\n",
    "                                total_records=200985,\n",
    "                                num_to_drop=1 # Batch size 28 leaves remainder of 1 record\n",
    "                                )\n",
    "\n",
    "for X_batch, y_batch in train_dataset.take(1):\n",
    "    print(\"X shape:\", X_batch.shape)\n",
    "    print(\"y shape:\", y_batch.shape)\n",
    "    # print(\"record_id:\", record_id_batch)\n",
    "    # print(\"cstart:\", cstart_batch)\n",
    "    # print(\"cend:\", cend_batch)\n",
    "    # print(\"strand:\", strand_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ The best free AWS training can get you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
