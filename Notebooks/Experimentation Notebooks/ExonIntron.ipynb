{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, metrics, losses, callbacks, optimizers, models, utils\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import keras_tuner as kt\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "datasets_path = \"../../Datasets/\"\n",
    "models_path = \"../../Models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to adjust the fasta to match my purposes.  The first cell below is the only current and useful one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1\n",
      "chr2\n",
      "chr3\n",
      "chr4\n",
      "chr5\n",
      "chr6\n",
      "chr7\n",
      "chr8\n",
      "chr9\n",
      "chr10\n",
      "chr11\n",
      "chr12\n",
      "chr13\n",
      "chr14\n",
      "chr15\n",
      "chr16\n",
      "chr17\n",
      "chr18\n",
      "chr19\n",
      "chr20\n",
      "chr21\n",
      "chr22\n",
      "chrX\n",
      "chrY\n"
     ]
    }
   ],
   "source": [
    "valid_chroms = set([f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"])\n",
    "\n",
    "input_fasta = \"chr_genome.fa\"  # Removed chrM because there aren't introns on it and I didn't want to confuse the training data\n",
    "output_fasta = \"trim_chr_genome.fa\"\n",
    "\n",
    "keep = False\n",
    "'''\n",
    "Removes non-chromosome entries in the human genome data\n",
    "If line is a record key (starswith(\">\")), it compares first 'word' in the record key to valid_chroms\n",
    "If comparison is true, it keeps writing lines until it finds another record key line to make the comparison\n",
    "'''\n",
    "with open(input_fasta, \"r\") as fin, open(output_fasta, \"w\") as fout:\n",
    "    for line in fin:\n",
    "        if line.startswith(\">\"):\n",
    "            chrom_name = line.strip()[1:].split()[0]\n",
    "            keep = (chrom_name in valid_chroms)\n",
    "\n",
    "            if keep:\n",
    "                fout.write(line)\n",
    "                print(chrom_name)\n",
    "        else:\n",
    "            if keep:\n",
    "                fout.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrX\n",
      "chrY\n",
      "chrM\n"
     ]
    }
   ],
   "source": [
    "valid_chroms = set([\"chrX\", \"chrY\", \"chrM\"])\n",
    "\n",
    "input_fasta = \"genome.fa\"\n",
    "output_fasta = \"test_genome.fa\"\n",
    "\n",
    "keep = False\n",
    "'''\n",
    "Makes a smaller test set of chromosomes\n",
    "If line is a record key (starswith(\">\")), it compares first 'word' in the record key to valid_chroms\n",
    "If comparison is true, it keeps writing lines until it finds another record key line to make the comparison\n",
    "'''\n",
    "with open(input_fasta, \"r\") as fin, open(output_fasta, \"w\") as fout:\n",
    "    for line in fin:\n",
    "        if line.startswith(\">\"):\n",
    "            chrom_name = line.strip()[1:].split()[0]\n",
    "            keep = (chrom_name in valid_chroms)\n",
    "\n",
    "            if keep:\n",
    "                fout.write(line)\n",
    "                print(chrom_name)\n",
    "        else:\n",
    "            if keep:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_chroms = set([f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\", \"chrM\"])\n",
    "\n",
    "input_gtf = \"annotations.gtf\"\n",
    "output_gtf = \"chr_annotations.gtf\"\n",
    "\n",
    "'''\n",
    "Removes annotations for non-chromosome entries in the human genome data\n",
    "If a line is a comment, it keeps it\n",
    "Otherwise, it checks if the line is in record format (tab separated) and checks field 0 for chromosome name of that record\n",
    "If record is a member of valid_chroms, it writes the line\n",
    "'''\n",
    "with open(input_gtf, \"r\") as f_in, open(output_gtf, \"w\") as f_out:\n",
    "    for line in f_in:\n",
    "        if line.startswith(\"#\"):\n",
    "            f_out.write(line)\n",
    "            continue\n",
    "\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        if len(fields) > 0:\n",
    "            chrom = fields[0]\n",
    "            if chrom in valid_chroms:\n",
    "                f_out.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_chroms = set([\"chrX\", \"chrY\", \"chrM\"])\n",
    "\n",
    "input_gtf = \"annotations.gtf\"\n",
    "output_gtf = \"test_annotations.gtf\"\n",
    "\n",
    "'''\n",
    "Makes a smaller test set of annotations\n",
    "If a line is a comment, it keeps it\n",
    "Otherwise, it checks if the line is in record format (tab separated) and checks field 0 for chromosome name of that record\n",
    "If record is a member of valid_chroms, it writes the line\n",
    "'''\n",
    "with open(input_gtf, \"r\") as f_in, open(output_gtf, \"w\") as f_out:\n",
    "    for line in f_in:\n",
    "        if line.startswith(\"#\"):\n",
    "            f_out.write(line)\n",
    "            continue\n",
    "\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        if len(fields) > 0:\n",
    "            chrom = fields[0]\n",
    "            if chrom in valid_chroms:\n",
    "                f_out.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_gtf_annotations(gtf_file):\n",
    "    \"\"\"\n",
    "    Loads GTF into a pandas DataFrame and converts start and end to zero-based indexing because python.\n",
    "    \"\"\"\n",
    "    gtf_data = pd.read_csv(\n",
    "        gtf_file, sep='\\t', comment='#', header=None,\n",
    "        names=['seqname', 'source', 'feature', 'start', 'end', \n",
    "               'score', 'strand', 'frame', 'attribute']\n",
    "    )\n",
    "    # Convert to zero-based indexing at start.  Note that end is not here because 1 indexing is end-inclusive and 0 indexing is end-exclusive\n",
    "    gtf_data['start'] = gtf_data['start'] - 1\n",
    "    return gtf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfaidx import Fasta\n",
    "\n",
    "def compute_chunk_indices(fasta_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Creates a list of (record_id, start, end) for each chunk in the FASTA.\n",
    "    \"\"\"\n",
    "    fa = Fasta(fasta_file)  # for indexed random access\n",
    "    chunk_indices = []\n",
    "    for record_id in fa.keys():\n",
    "        seq_len = len(fa[record_id])\n",
    "        for start in range(0, seq_len, chunk_size):\n",
    "            end = min(start + chunk_size, seq_len)\n",
    "            chunk_indices.append((record_id, start, end))\n",
    "    return chunk_indices\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters in the FASTA file:\n",
      "A\n",
      "C\n",
      "G\n",
      "N\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Reads DNA to confirm there are no special characters that require masking besides N.  Skips header lines\n",
    "'''\n",
    "\n",
    "unique_chars = set()\n",
    "\n",
    "with open(\"chr_genome.fa\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            continue\n",
    "        \n",
    "        for char in line:\n",
    "            unique_chars.add(char)\n",
    "\n",
    "\n",
    "print(\"Unique characters in the FASTA file:\")\n",
    "for char in sorted(unique_chars):\n",
    "    print(char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to the above is this:\n",
    "\n",
    "Unique characters in the FASTA file:\n",
    "\n",
    "A\n",
    "\n",
    "C\n",
    "\n",
    "G\n",
    "\n",
    "N\n",
    "\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exon-intron overlaps: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** WARNING: File /tmp/pybedtools.gphbugsb.tmp has inconsistent naming convention for record:\n",
      "MU273365.1\t5969\t8223\texon\t.\t+\n",
      "\n",
      "***** WARNING: File /tmp/pybedtools.gphbugsb.tmp has inconsistent naming convention for record:\n",
      "MU273365.1\t5969\t8223\texon\t.\t+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['BEDTOOLS_PATH'] = '/usr/bin/bedtools'\n",
    "import pybedtools\n",
    "\n",
    "# Load your GTF into a DataFrame\n",
    "gtf = pd.read_csv(\n",
    "    \"annotations.gtf\", sep='\\t', comment='#', header=None,\n",
    "    names=['seqname','source','feature','start','end','score','strand','frame','attribute']\n",
    ")\n",
    "\n",
    "# If your GTF is truly 1-based, convert to 0-based\n",
    "# (uncomment if needed)\n",
    "gtf['start'] = gtf['start'] - 1\n",
    "\n",
    "# Subset exons & introns\n",
    "exons_df = gtf[gtf['feature'] == 'exon'].copy()\n",
    "introns_df = gtf[gtf['feature'] == 'intron'].copy()\n",
    "\n",
    "# pybedtools expects at least 3 columns: chrom, start, end\n",
    "# If you want to keep strand, you can include that as well\n",
    "exons_bed = pybedtools.BedTool.from_dataframe(\n",
    "    exons_df[['seqname', 'start', 'end', 'feature', 'score', 'strand']]\n",
    ")\n",
    "introns_bed = pybedtools.BedTool.from_dataframe(\n",
    "    introns_df[['seqname', 'start', 'end', 'feature', 'score', 'strand']]\n",
    ")\n",
    "\n",
    "# Intersect\n",
    "overlaps = exons_bed.intersect(introns_bed, wa=True, wb=True)\n",
    "\n",
    "# 'overlaps' is a BedTool object. You can convert to a DataFrame:\n",
    "overlaps_df = overlaps.to_dataframe(\n",
    "    names=[\n",
    "        'exon_chrom','exon_start','exon_end','exon_feature','exon_score','exon_strand',\n",
    "        'intron_chrom','intron_start','intron_end','intron_feature','intron_score','intron_strand'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of exon-intron overlaps:\", len(overlaps_df))\n",
    "print(overlaps_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlaps between exons: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** WARNING: File /tmp/pybedtools.ute8oiog.tmp has inconsistent naming convention for record:\n",
      "MU273365.1\tHAVANA\texon\t5969\t8223\t.\t+\t.\t\"gene_id \"\"ENSG00000291343.1\"\"; transcript_id \"\"ENST00000707199.1\"\"; gene_type \"\"processed_pseudogene\"\"; gene_name \"\"BEND3P2\"\"; transcript_type \"\"processed_pseudogene\"\"; transcript_name \"\"BEND3P2-202\"\"; exon_number 1; exon_id \"\"ENSE00004001511.1\"\"; level 2; hgnc_id \"\"HGNC:45015\"\"; ont \"\"PGO:0000004\"\"; tag \"\"basic\"\"; tag \"\"Ensembl_canonical\"\";\"\n",
      "\n",
      "Error: Invalid record in file /tmp/pybedtools.ute8oiog.tmp. Record is \n",
      "KI270861.1\tHAVANA\texon\t0\t5793\t.\t-\t.\t\"gene_id \"\"ENSG00000278550.4\"\"; transcript_id \"\"ENST00000634102.1\"\"; gene_type \"\"protein_coding\"\"; gene_name \"\"SLC43A2\"\"; transcript_type \"\"protein_coding\"\"; transcript_name \"\"SLC43A2-224\"\"; exon_number 14; exon_id \"\"ENSE00003783572.1\"\"; level 2; protein_id \"\"ENSP00000488355.1\"\"; transcript_support_level \"\"1\"\"; hgnc_id \"\"HGNC:23087\"\"; tag \"\"basic\"\"; havana_gene \"\"OTTHUMG00000191160.1\"\"; havana_transcript \"\"OTTHUMT00000486890.1\"\";\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pybedtools\n",
    "\n",
    "# Load GTF into a DataFrame\n",
    "gtf = pd.read_csv(\n",
    "    \"annotations.gtf\", sep='\\t', comment='#', header=None,\n",
    "    names=[\"seqname\",\"source\",\"feature\",\"start\",\"end\",\"score\",\"strand\",\"frame\",\"attribute\"]\n",
    ")\n",
    "\n",
    "# Filter for genes only\n",
    "genes_df = gtf[gtf[\"feature\"] == \"gene\"].copy()\n",
    "\n",
    "# If needed: convert to 0-based. (Uncomment if your GTF is standard 1-based.)\n",
    "# genes_df[\"start\"] = genes_df[\"start\"] - 1\n",
    "\n",
    "# Now we have a table of genes, each with [seqname, start, end, strand, etc.].\n",
    "# We'll pick columns for a BED-like structure:\n",
    "genes_df[\"score\"] = 0  # If you don't have a score, just set 0\n",
    "\n",
    "# columns = [chrom, start, end, name, score, strand]\n",
    "genes_bedtool = pybedtools.BedTool.from_dataframe(\n",
    "    genes_df[[\"seqname\", \"start\", \"end\", \"feature\", \"score\", \"strand\"]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** WARNING: File /tmp/pybedtools.qdasi872.tmp has inconsistent naming convention for record:\n",
      "MU273365.1\t5969\t8223\tgene\t0\t+\n",
      "\n",
      "***** WARNING: File /tmp/pybedtools.qdasi872.tmp has inconsistent naming convention for record:\n",
      "MU273365.1\t5969\t8223\tgene\t0\t+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import pybedtools\n",
    "\n",
    "# Load GTF into a DataFrame\n",
    "gtf = pd.read_csv(\n",
    "    \"annotations.gtf\", sep='\\t', comment='#', header=None,\n",
    "    names=[\"seqname\",\"source\",\"feature\",\"start\",\"end\",\"score\",\"strand\",\"frame\",\"attribute\"]\n",
    ")\n",
    "\n",
    "# Filter for genes only\n",
    "genes_df = gtf[gtf[\"feature\"] == \"gene\"].copy()\n",
    "\n",
    "# If needed: convert to 0-based. (Uncomment if your GTF is standard 1-based.)\n",
    "genes_df[\"start\"] = genes_df[\"start\"] - 1\n",
    "\n",
    "# Now we have a table of genes, each with [seqname, start, end, strand, etc.].\n",
    "# We'll pick columns for a BED-like structure:\n",
    "genes_df[\"score\"] = 0  # If you don't have a score, just set 0\n",
    "\n",
    "# columns = [chrom, start, end, name, score, strand]\n",
    "genes_bedtool = pybedtools.BedTool.from_dataframe(\n",
    "    genes_df[[\"seqname\", \"start\", \"end\", \"feature\", \"score\", \"strand\"]]\n",
    ")\n",
    "\n",
    "overlaps = genes_bedtool.intersect(genes_bedtool, wa=True, wb=True)\n",
    "\n",
    "overlaps_df = overlaps.to_dataframe(\n",
    "    names=[\n",
    "        \"chromA\",\"startA\",\"endA\",\"nameA\",\"scoreA\",\"strandA\",\n",
    "        \"chromB\",\"startB\",\"endB\",\"nameB\",\"scoreB\",\"strandB\"\n",
    "    ]\n",
    ")\n",
    "print(len(overlaps_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77926\n"
     ]
    }
   ],
   "source": [
    "overlaps_df.head()\n",
    "filtered_overlap = overlaps_df[\n",
    "    ~(\n",
    "        (overlaps_df[\"startA\"] == overlaps_df[\"startB\"]) &\n",
    "        (overlaps_df[\"endA\"]   == overlaps_df[\"endB\"])\n",
    "    )\n",
    "]\n",
    "print(len(filtered_overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromA</th>\n",
       "      <th>startA</th>\n",
       "      <th>endA</th>\n",
       "      <th>nameA</th>\n",
       "      <th>scoreA</th>\n",
       "      <th>strandA</th>\n",
       "      <th>chromB</th>\n",
       "      <th>startB</th>\n",
       "      <th>endB</th>\n",
       "      <th>nameB</th>\n",
       "      <th>scoreB</th>\n",
       "      <th>strandB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>11868</td>\n",
       "      <td>14409</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>12009</td>\n",
       "      <td>13670</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>12009</td>\n",
       "      <td>13670</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>11868</td>\n",
       "      <td>14409</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>14695</td>\n",
       "      <td>24886</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1</td>\n",
       "      <td>17368</td>\n",
       "      <td>17436</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chr1</td>\n",
       "      <td>17368</td>\n",
       "      <td>17436</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1</td>\n",
       "      <td>14695</td>\n",
       "      <td>24886</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chr1</td>\n",
       "      <td>29553</td>\n",
       "      <td>31109</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>30365</td>\n",
       "      <td>30503</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chromA  startA   endA nameA  scoreA strandA chromB  startB   endB nameB  \\\n",
       "1   chr1   11868  14409  gene       0       +   chr1   12009  13670  gene   \n",
       "2   chr1   12009  13670  gene       0       +   chr1   11868  14409  gene   \n",
       "4   chr1   14695  24886  gene       0       -   chr1   17368  17436  gene   \n",
       "7   chr1   17368  17436  gene       0       -   chr1   14695  24886  gene   \n",
       "9   chr1   29553  31109  gene       0       +   chr1   30365  30503  gene   \n",
       "\n",
       "   scoreB strandB  \n",
       "1       0       +  \n",
       "2       0       +  \n",
       "4       0       -  \n",
       "7       0       -  \n",
       "9       0       +  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_overlap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37430/1209143289.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_overlap[\"pair_key\"] = filtered_overlap.apply(canonical_pair, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromA</th>\n",
       "      <th>startA</th>\n",
       "      <th>endA</th>\n",
       "      <th>nameA</th>\n",
       "      <th>scoreA</th>\n",
       "      <th>strandA</th>\n",
       "      <th>chromB</th>\n",
       "      <th>startB</th>\n",
       "      <th>endB</th>\n",
       "      <th>nameB</th>\n",
       "      <th>scoreB</th>\n",
       "      <th>strandB</th>\n",
       "      <th>pair_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>11868</td>\n",
       "      <td>14409</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>12009</td>\n",
       "      <td>13670</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>((chr1, 11868, 14409, +), (chr1, 12009, 13670,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>14695</td>\n",
       "      <td>24886</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1</td>\n",
       "      <td>17368</td>\n",
       "      <td>17436</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>((chr1, 14695, 24886, -), (chr1, 17368, 17436,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chr1</td>\n",
       "      <td>29553</td>\n",
       "      <td>31109</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>30365</td>\n",
       "      <td>30503</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>((chr1, 29553, 31109, +), (chr1, 30365, 30503,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>chr1</td>\n",
       "      <td>57597</td>\n",
       "      <td>64116</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>62948</td>\n",
       "      <td>63887</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>((chr1, 57597, 64116, +), (chr1, 62948, 63887,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chr1</td>\n",
       "      <td>89294</td>\n",
       "      <td>133566</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1</td>\n",
       "      <td>89550</td>\n",
       "      <td>91105</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>((chr1, 89294, 133566, -), (chr1, 89550, 91105...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chromA  startA    endA nameA  scoreA strandA chromB  startB   endB nameB  \\\n",
       "1    chr1   11868   14409  gene       0       +   chr1   12009  13670  gene   \n",
       "4    chr1   14695   24886  gene       0       -   chr1   17368  17436  gene   \n",
       "9    chr1   29553   31109  gene       0       +   chr1   30365  30503  gene   \n",
       "15   chr1   57597   64116  gene       0       +   chr1   62948  63887  gene   \n",
       "19   chr1   89294  133566  gene       0       -   chr1   89550  91105  gene   \n",
       "\n",
       "    scoreB strandB                                           pair_key  \n",
       "1        0       +  ((chr1, 11868, 14409, +), (chr1, 12009, 13670,...  \n",
       "4        0       -  ((chr1, 14695, 24886, -), (chr1, 17368, 17436,...  \n",
       "9        0       +  ((chr1, 29553, 31109, +), (chr1, 30365, 30503,...  \n",
       "15       0       +  ((chr1, 57597, 64116, +), (chr1, 62948, 63887,...  \n",
       "19       0       -  ((chr1, 89294, 133566, -), (chr1, 89550, 91105...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def canonical_pair(row):\n",
    "    # Build tuples for A and B\n",
    "    A = (row[\"chromA\"], row[\"startA\"], row[\"endA\"], row[\"strandA\"])\n",
    "    B = (row[\"chromB\"], row[\"startB\"], row[\"endB\"], row[\"strandB\"])\n",
    "    # Sort them so the 'lesser' always comes first\n",
    "    # (Python compares tuples lexicographically)\n",
    "    pair = tuple(sorted([A, B]))\n",
    "    return pair\n",
    "\n",
    "# Apply and store the result in a new column\n",
    "filtered_overlap[\"pair_key\"] = filtered_overlap.apply(canonical_pair, axis=1)\n",
    "\n",
    "# Drop duplicates so that each pair_key appears only once\n",
    "filtered_overlap = filtered_overlap.drop_duplicates(subset=\"pair_key\")\n",
    "\n",
    "# Optionally remove the pair_key column afterwards\n",
    "# filtered_overlap = filtered_overlap.drop(columns=[\"pair_key\"])\n",
    "print(len(filtered_overlap))\n",
    "filtered_overlap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38958\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromA</th>\n",
       "      <th>startA</th>\n",
       "      <th>endA</th>\n",
       "      <th>nameA</th>\n",
       "      <th>scoreA</th>\n",
       "      <th>strandA</th>\n",
       "      <th>chromB</th>\n",
       "      <th>startB</th>\n",
       "      <th>endB</th>\n",
       "      <th>nameB</th>\n",
       "      <th>scoreB</th>\n",
       "      <th>strandB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>11868</td>\n",
       "      <td>14409</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>12009</td>\n",
       "      <td>13670</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>14695</td>\n",
       "      <td>24886</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1</td>\n",
       "      <td>17368</td>\n",
       "      <td>17436</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chr1</td>\n",
       "      <td>29553</td>\n",
       "      <td>31109</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>30365</td>\n",
       "      <td>30503</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>chr1</td>\n",
       "      <td>57597</td>\n",
       "      <td>64116</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1</td>\n",
       "      <td>62948</td>\n",
       "      <td>63887</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chr1</td>\n",
       "      <td>89294</td>\n",
       "      <td>133566</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1</td>\n",
       "      <td>89550</td>\n",
       "      <td>91105</td>\n",
       "      <td>gene</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chromA  startA    endA nameA  scoreA strandA chromB  startB   endB nameB  \\\n",
       "1    chr1   11868   14409  gene       0       +   chr1   12009  13670  gene   \n",
       "4    chr1   14695   24886  gene       0       -   chr1   17368  17436  gene   \n",
       "9    chr1   29553   31109  gene       0       +   chr1   30365  30503  gene   \n",
       "15   chr1   57597   64116  gene       0       +   chr1   62948  63887  gene   \n",
       "19   chr1   89294  133566  gene       0       -   chr1   89550  91105  gene   \n",
       "\n",
       "    scoreB strandB  \n",
       "1        0       +  \n",
       "4        0       -  \n",
       "9        0       +  \n",
       "15       0       +  \n",
       "19       0       -  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_overlap = filtered_overlap.drop(columns=[\"pair_key\"])\n",
    "print(len(filtered_overlap))\n",
    "filtered_overlap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH = /home/virtuousrogue/Deep Learning Projects/venv/bin:/home/virtuousrogue/.vscode-server/bin/91fbdddc47bc9c09064bf7acf133d22631cbf083/bin/remote-cli:/home/virtuousrogue/.local/bin:/home/virtuousrogue/.local/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Program Files/Python310/Scripts/:/mnt/c/Program Files/Python310/:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/bin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/libnvvp:/mnt/c/Program Files/Python312/Scripts/:/mnt/c/Program Files/Python312/:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/Microsoft VS Code/bin:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/dotnet/:/mnt/c/Program Files/Microsoft SQL Server/150/Tools/Binn/:/mnt/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn/:/mnt/c/Program Files (x86)/Windows Kits/10/Windows Performance Toolkit/:/mnt/c/Program Files/NVIDIA Corporation/Nsight Compute 2024.3.2/:/mnt/c/Program Files/Tesseract-OCR:/mnt/c/poppler/poppler-24.08.0/Library/bin/:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA app/NvDLISR:/mnt/c/Program Files/Java/jdk-21/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Users/imcpa/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/imcpa/.dotnet/tools:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/bin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/include:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/lib/x64:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/libnvvp:/mnt/c/tools/cuda/bin:/mnt/c/Program Files/Tesseract-OCR:/snap/bin\n",
      "/usr/bin/bedtools\n",
      "bedtools v2.30.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pybedtools\n",
    "\n",
    "print(\"PATH =\", os.environ[\"PATH\"])\n",
    "!which bedtools\n",
    "!bedtools --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Uses a dictionary to encode bases. N's are masked as all zeroes\n",
    "'''\n",
    "n_base_encoder = {\n",
    "    'A': [1, 0, 0, 0],\n",
    "    'C': [0, 1, 0, 0],\n",
    "    'G': [0, 0, 1, 0],\n",
    "    'T': [0, 0, 0, 1],\n",
    "    'N': [0, 0, 0, 0]  \n",
    "}\n",
    "\n",
    "\n",
    "def one_hot_encode_with_N(sequence):\n",
    "    return [n_base_encoder.get(nuc, [0, 0, 0, 0]) for nuc in sequence]\n",
    "\n",
    "def pad_one_hot_sequence(encoded_seq, target_length):\n",
    "    \"\"\"\n",
    "    Pads sequence chunks so all are the same size\n",
    "    \"\"\"\n",
    "    seq_len = len(encoded_seq)\n",
    "    padding_length = target_length - seq_len\n",
    "    if padding_length > 0:\n",
    "        encoded_seq += [[0, 0, 0, 0]] * padding_length\n",
    "    return encoded_seq\n",
    "\n",
    "def label_sequence(sequence_length, annotations):\n",
    "    \"\"\"\n",
    "    Assigns categorical numeric labels to save memory:\n",
    "      0 = non-coding\n",
    "      1 = intron start\n",
    "      2 = intron end\n",
    "      3 = exon start\n",
    "      4 = exon end\n",
    "    \"\"\"\n",
    "    labels = [0] * sequence_length  # All non-coding by default\n",
    "    for _, row in annotations.iterrows():\n",
    "        start, end, feature = int(row['start']), int(row['end']), row['feature']\n",
    "        # Clip to chunk boundaries\n",
    "        start = max(0, start)\n",
    "        end   = min(sequence_length, end)\n",
    "\n",
    "        if feature == 'exon':\n",
    "            if start < sequence_length:\n",
    "                labels[start] = 3  # Exon start\n",
    "            if end - 1 < sequence_length and end - 1 >= 0:\n",
    "                labels[end - 1] = 4  # Exon end\n",
    "            \n",
    "        elif feature == 'intron':\n",
    "            if start < sequence_length:\n",
    "                labels[start] = 1  # Intron start\n",
    "            if end - 1 < sequence_length and end - 1 >= 0:\n",
    "                labels[end - 1] = 2  # Intron end\n",
    "\n",
    "    return labels\n",
    "\n",
    "def pad_one_hot_sequence(encoded_seq, target_len):\n",
    "    pad_size = target_len - len(encoded_seq)\n",
    "    if pad_size > 0:\n",
    "        encoded_seq += [[0, 0, 0, 0]] * pad_size\n",
    "    return encoded_seq\n",
    "\n",
    "def pad_labels(labels, target_length):\n",
    "    \"\"\"\n",
    "    Pads label array up to target_length with 0 (non-coding).\n",
    "    \"\"\"\n",
    "    padding_length = target_length - len(labels)\n",
    "    if padding_length > 0:\n",
    "        labels += [0] * padding_length\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def serialize_example(X, y):\n",
    "    \"\"\"\n",
    "    Convert (X, y) to a tf.train.Example for TFRecord.\n",
    "    X: shape (chunk_size, 4), float32\n",
    "    y: shape (chunk_size,), int\n",
    "    \"\"\"\n",
    "    # Flatten X to store as bytes or as a list of floats\n",
    "    X_flat = X.reshape(-1).tolist()\n",
    "    y_flat = y.tolist()\n",
    "\n",
    "    feature = {\n",
    "        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X_flat)),\n",
    "        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=y_flat)),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def _write_shard(\n",
    "    shard_id,\n",
    "    shard_indices,\n",
    "    fasta_file,\n",
    "    gtf_data,\n",
    "    chunk_size,\n",
    "    out_dir,\n",
    "    compression=\"GZIP\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Worker function that writes TFRecord for the assigned shard_indices.\n",
    "    Each shard goes into its own file: e.g. out_dir/shard_{shard_id:04d}.tfrecord.gz\n",
    "    \"\"\"\n",
    "    # We'll open the FASTA in each process so we can do random-access.\n",
    "    fa = Fasta(fasta_file)\n",
    "    \n",
    "    # Create TFRecord writer with compression\n",
    "    shard_filename = os.path.join(out_dir, f\"shard_{shard_id:04d}.tfrecord\")\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression)\n",
    "    with tf.io.TFRecordWriter(shard_filename, options=options) as writer:\n",
    "        for (record_id, start, end) in shard_indices:\n",
    "            seq = fa[record_id][start:end].seq\n",
    "            \n",
    "            # Filter relevant annotations\n",
    "            sub_anno = gtf_data[\n",
    "                (gtf_data['seqname'] == record_id) &\n",
    "                (gtf_data['start'] < end) &\n",
    "                (gtf_data['end'] > start)\n",
    "            ].copy()\n",
    "            # Shift coordinates relative to chunk\n",
    "            sub_anno['start'] -= start\n",
    "            sub_anno['end']   -= start\n",
    "\n",
    "            # One-hot encode\n",
    "            encoded = one_hot_encode_with_N(seq)\n",
    "            labels  = label_sequence(len(seq), sub_anno)\n",
    "\n",
    "            # Pad to chunk_size if needed\n",
    "            encoded = pad_one_hot_sequence(encoded, chunk_size)\n",
    "            labels  = pad_labels(labels, chunk_size)\n",
    "\n",
    "            X = np.array(encoded, dtype=np.float32)\n",
    "            y = np.array(labels,  dtype=np.int32)\n",
    "\n",
    "            example = serialize_example(X, y)\n",
    "            writer.write(example)\n",
    "\n",
    "    return shard_filename\n",
    "\n",
    "\n",
    "def parallel_write_tfrecords(\n",
    "    fasta_file,\n",
    "    gtf_file,\n",
    "    chunk_size,\n",
    "    out_dir,\n",
    "    num_shards=8,\n",
    "    compression=\"GZIP\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits chunk_indices into N shards and processes them in parallel.\n",
    "    Each shard is written to out_dir/shard_xxxx.tfrecord (compressed).\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Loading GTF...\")\n",
    "    gtf_data = load_gtf_annotations(gtf_file)\n",
    "\n",
    "    print(\"Computing chunk indices...\")\n",
    "    chunk_indices = compute_chunk_indices(fasta_file, chunk_size)\n",
    "    \n",
    "    # Shuffle if you want random distribution among shards\n",
    "    np.random.shuffle(chunk_indices)\n",
    "\n",
    "    # Split chunk_indices into num_shards subsets\n",
    "    shard_size = math.ceil(len(chunk_indices) / num_shards)\n",
    "    shard_splits = [\n",
    "        chunk_indices[i * shard_size : (i + 1) * shard_size]\n",
    "        for i in range(num_shards)\n",
    "    ]\n",
    "\n",
    "    # Launch parallel processes\n",
    "    futures = []\n",
    "    print(f\"Writing {len(chunk_indices)} chunks into {num_shards} shards...\")\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_shards) as executor:\n",
    "        for shard_id, shard_indices in enumerate(shard_splits):\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    _write_shard,\n",
    "                    shard_id,\n",
    "                    shard_indices,\n",
    "                    fasta_file,\n",
    "                    gtf_data,      # pass the entire DataFrame to each process\n",
    "                    chunk_size,\n",
    "                    out_dir,\n",
    "                    compression\n",
    "                )\n",
    "            )\n",
    "\n",
    "    results = []\n",
    "    for f in concurrent.futures.as_completed(futures):\n",
    "        results.append(f.result())\n",
    "\n",
    "    print(\"All shards written:\")\n",
    "    for r in results:\n",
    "        print(\"  \", r)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GTF...\n",
      "Computing chunk indices...\n",
      "Writing 658691 chunks into 8 shards...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# We'll produce 8 shards in parallel using GZIP compression\u001b[39;00m\n\u001b[1;32m      7\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfrecord_shards\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mparallel_write_tfrecords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfasta_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfasta_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgtf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgtf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# or however many parallel processes / shards you want\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGZIP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# or \"ZLIB\" or None for no compression\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone writing TFRecord!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 104\u001b[0m, in \u001b[0;36mparallel_write_tfrecords\u001b[0;34m(fasta_file, gtf_file, chunk_size, out_dir, num_shards, compression)\u001b[0m\n\u001b[1;32m    102\u001b[0m futures \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk_indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_shards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shards...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mnum_shards) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shard_id, shard_indices \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(shard_splits):\n\u001b[1;32m    106\u001b[0m         futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    107\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    108\u001b[0m                 _write_shard,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         )\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py:780\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread_wakeup\u001b[38;5;241m.\u001b[39mwakeup()\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fasta_file = \"genome.fa\"\n",
    "gtf_file   = \"annotations.gtf\"\n",
    "chunk_size = 5000\n",
    "\n",
    "\n",
    "# We'll produce 8 shards in parallel using GZIP compression\n",
    "output_dir = \"tfrecord_shards\"\n",
    "parallel_write_tfrecords(\n",
    "    fasta_file=fasta_file,\n",
    "    gtf_file=gtf_file,\n",
    "    chunk_size=chunk_size,\n",
    "    out_dir=output_dir,\n",
    "    num_shards=8,          # or however many parallel processes / shards you want\n",
    "    compression=\"GZIP\"     # or \"ZLIB\" or None for no compression\n",
    "    )\n",
    "\n",
    "print(\"Done writing TFRecord!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto, chunk_size=5000):\n",
    "    # Define the features\n",
    "    feature_spec = {\n",
    "        'X': tf.io.FixedLenSequenceFeature([], dtype=tf.float32, allow_missing=True),\n",
    "        'y': tf.io.FixedLenSequenceFeature([], dtype=tf.int64, allow_missing=True)\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "    # 'X' is shape [chunk_size*4], 'y' is shape [chunk_size]\n",
    "    X = parsed_features['X']\n",
    "    y = parsed_features['y']\n",
    "\n",
    "    # Reshape X back to [chunk_size, 4]\n",
    "    X = tf.reshape(X, [chunk_size, 4])\n",
    "    y = tf.reshape(y, [chunk_size])\n",
    "    \n",
    "    return X, tf.cast(y, tf.int32)\n",
    "\n",
    "def build_dataset_from_tfrecords(file_pattern, chunk_size=5000, batch_size=8):\n",
    "    files = tf.data.Dataset.list_files(file_pattern)\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(lambda x: parse_tfrecord(x, chunk_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Example usage:\n",
    "write_tfrecords(\"genome.fa\", \"annotations.gtf\", 5000, \"my_dataset.tfrecord\")\n",
    "dataset = build_dataset_from_tfrecords(\"my_dataset.tfrecord\", chunk_size=5000, batch_size=8)\n",
    "\n",
    "# Now this dataset is purely Tensor-based and can be saved or re-loaded if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Parses a single TFRecord example into (X, y) Tensors.\n",
    "    \"\"\"\n",
    "    feature_spec = {\n",
    "        'X': tf.io.FixedLenSequenceFeature([], dtype=tf.float32, allow_missing=True),\n",
    "        'y': tf.io.FixedLenSequenceFeature([], dtype=tf.int64, allow_missing=True),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "    \n",
    "    # 'X' is a 1D float array of length (chunk_size * 4)\n",
    "    X = parsed_features['X']\n",
    "    y = parsed_features['y']\n",
    "    \n",
    "    # Reshape X to [chunk_size, 4]\n",
    "    X = tf.reshape(X, [chunk_size, 4])\n",
    "    y = tf.reshape(y, [chunk_size])\n",
    "    \n",
    "    # Cast y to int32 if needed\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return X, y\n",
    "\n",
    "def build_dataset_from_tfrecords(file_pattern, chunk_size=5000, batch_size=8):\n",
    "    \"\"\"\n",
    "    Reads TFRecords, parses them, and returns a tf.data.Dataset of (X, y).\n",
    "    \"\"\"\n",
    "    # file_pattern can be a single file or a wildcard to multiple files\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda fp: tf.data.TFRecordDataset(fp, compression_type=None),\n",
    "        cycle_length=4,       # how many files to read in parallel\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    # Parse each record\n",
    "    dataset = dataset.map(\n",
    "        lambda x: parse_tfrecord(x, chunk_size),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    # Shuffle (buffer size is a hyperparameter) + batch + prefetch\n",
    "    dataset = dataset.shuffle(1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Usage:\n",
    "# after writing TFRecords (my_dataset.tfrecord),\n",
    "# read them back in a TF data pipeline:\n",
    "ds = build_dataset_from_tfrecords(\"my_dataset.tfrecord\", \n",
    "                                  chunk_size=5000,\n",
    "                                  batch_size=8)\n",
    "for X_batch, y_batch in ds.take(1):\n",
    "    print(X_batch.shape, y_batch.shape)  # (8, 5000, 4), (8, 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid CNN LSTM Serial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (sequence_length, 4)  # One-hot encoded sequence\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# 1D-CNN block\n",
    "cnn = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs)\n",
    "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "\n",
    "cnn = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(cnn)\n",
    "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "\n",
    "# LSTM block\n",
    "lstm = Bidirectional(LSTM(128, return_sequences=True))(cnn)\n",
    "lstm = Bidirectional(LSTM(128))(lstm)\n",
    "\n",
    "# Fully connected layers\n",
    "dense = Dense(128, activation='relu')(lstm)\n",
    "dense = Dropout(0.5)(dense)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(dense)  # For multi-class classification\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['f1_score'])  # might try adagrad if adam is bad\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN only with similar calculation number to hybrid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted 1D-CNN Model\n",
    "inputs = Input(shape=(sequence_length, 4))\n",
    "\n",
    "# Add more filters and layers to match the hybrid's complexity\n",
    "cnn = Conv1D(filters=96, kernel_size=5, activation='relu', padding='same')(inputs)\n",
    "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "\n",
    "cnn = Conv1D(filters=192, kernel_size=5, activation='relu', padding='same')(cnn)\n",
    "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "\n",
    "cnn = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(cnn)\n",
    "\n",
    "# Fully connected layers\n",
    "flatten = Flatten()(cnn)\n",
    "dense = Dense(128, activation='relu')(flatten)\n",
    "dense = Dropout(0.5)(dense)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(dense)\n",
    "\n",
    "cnn_model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile and summarize\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['f1_score'])  # might try adagrad if adam is bad\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM only with similar calculation number to hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted LSTM Model\n",
    "inputs = Input(shape=(sequence_length, 4))\n",
    "\n",
    "# Increase LSTM hidden size to match hybrid complexity\n",
    "lstm = Bidirectional(LSTM(192, return_sequences=True))(inputs)  # Larger hidden size\n",
    "lstm = Bidirectional(LSTM(192))(lstm)\n",
    "\n",
    "# Fully connected layers\n",
    "dense = Dense(128, activation='relu')(lstm)\n",
    "dense = Dropout(0.5)(dense)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(dense)\n",
    "\n",
    "lstm_model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile and summarize\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['f1_score'])  # might try adagrad if adam is bad\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid parallel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, Dropout, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (sequence_length, 4)  # One-hot encoded sequence\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# CNN Branch\n",
    "cnn = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs)\n",
    "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "cnn = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(cnn)\n",
    "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "cnn = Flatten()(cnn)  # Flatten for concatenation\n",
    "\n",
    "# LSTM Branch\n",
    "lstm = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
    "lstm = Bidirectional(LSTM(128))(lstm)  # Return a single vector\n",
    "\n",
    "# Concatenate the branches\n",
    "combined = Concatenate()([cnn, lstm])\n",
    "\n",
    "# Fully connected layers\n",
    "dense = Dense(128, activation='relu')(combined)\n",
    "dense = Dropout(0.5)(dense)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(dense)  # For multi-class classification\n",
    "\n",
    "# Create model\n",
    "parallel_hybrid_model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "parallel_hybrid_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['f1_score'])  # might try adagrad if adam is bad\n",
    "\n",
    "# Model summary\n",
    "parallel_hybrid_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time_seconds = 3600  # 1 hour\n",
    "batch_size = 32\n",
    "epochs = 100  # Set high enough to allow stopping by time\n",
    "\n",
    "time_limit_callback = TimeLimit(max_time_seconds=max_time_seconds)\n",
    "\n",
    "# Train 1D-CNN Model\n",
    "history_cnn = cnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[time_limit_callback]\n",
    ")\n",
    "\n",
    "# Train LSTM Model\n",
    "history_lstm = lstm_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[time_limit_callback]\n",
    ")\n",
    "\n",
    "# Train Hybrid Model\n",
    "history_hybrid = hybrid_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[time_limit_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(x_test, y_test)\n",
    "test_loss_lstm, test_acc_lstm = lstm_model.evaluate(x_test, y_test)\n",
    "test_loss_hybrid, test_acc_hybrid = hybrid_model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecords_for_test(fasta_file, gtf_file, chunk_size, output_path):\n",
    "    \"\"\"Writes chunks in the EXACT order they appear in chunk_indices (no shuffle).\"\"\"\n",
    "    import math\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from pyfaidx import Fasta\n",
    "    \n",
    "    gtf_data = load_gtf_annotations(gtf_file)\n",
    "    chunk_indices = compute_chunk_indices(fasta_file, chunk_size)  # <--- no shuffle\n",
    "    fa = Fasta(fasta_file)\n",
    "    \n",
    "    # For test, let's do no compression to keep it simple (you can add if you want)\n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        for (record_id, start, end) in chunk_indices:\n",
    "            seq = fa[record_id][start:end].seq\n",
    "            \n",
    "            # Filter relevant annotations\n",
    "            sub_anno = gtf_data[\n",
    "                (gtf_data['seqname'] == record_id) &\n",
    "                (gtf_data['start'] < end) &\n",
    "                (gtf_data['end'] > start)\n",
    "            ].copy()\n",
    "            sub_anno['start'] -= start\n",
    "            sub_anno['end']   -= start\n",
    "\n",
    "            # One-hot + label\n",
    "            encoded = one_hot_encode_with_N(seq)\n",
    "            labels  = label_sequence(len(seq), sub_anno)\n",
    "            \n",
    "            encoded = pad_one_hot_sequence(encoded, chunk_size)\n",
    "            labels  = pad_labels(labels, chunk_size)\n",
    "\n",
    "            X = np.array(encoded, dtype=np.float32)\n",
    "            y = np.array(labels, dtype=np.int32)\n",
    "\n",
    "            example = serialize_example(X, y)\n",
    "            writer.write(example)\n",
    "\n",
    "    print(\"Wrote TFRecord for test in same order as chunk_indices:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      4\u001b[0m test_tfrecord_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_ordered.tfrecord\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mwrite_tfrecords_for_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfasta_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtf_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_tfrecord_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mwrite_tfrecords_for_test\u001b[0;34m(fasta_file, gtf_file, chunk_size, output_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m seq \u001b[38;5;241m=\u001b[39m fa[record_id][start:end]\u001b[38;5;241m.\u001b[39mseq\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Filter relevant annotations\u001b[39;00m\n\u001b[1;32m     19\u001b[0m sub_anno \u001b[38;5;241m=\u001b[39m gtf_data[\n\u001b[0;32m---> 20\u001b[0m     (\u001b[43mgtf_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseqname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord_id\u001b[49m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     21\u001b[0m     (gtf_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m end) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     22\u001b[0m     (gtf_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m start)\n\u001b[1;32m     23\u001b[0m ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     24\u001b[0m sub_anno[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m     25\u001b[0m sub_anno[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m start\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fasta_file = \"genome.fa\"\n",
    "gtf_file   = \"annotations.gtf\"\n",
    "chunk_size = 5000\n",
    "test_tfrecord_path = \"test_ordered.tfrecord\"\n",
    "\n",
    "write_tfrecords_for_test(fasta_file, gtf_file, chunk_size, test_tfrecord_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #30 details (zero-based index = 29):\n",
      "  record_id: chr1\n",
      "  start: 145000\n",
      "  end:   150000\n"
     ]
    }
   ],
   "source": [
    "chunk_indices = compute_chunk_indices(fasta_file, chunk_size)  # same function\n",
    "# The \"30th\" chunk in zero-based indexing is chunk_indices[29],\n",
    "# or if you literally mean the 30th chunk in one-based counting, chunk_indices[30 - 1].\n",
    "# Let's assume zero-based for programming clarity:\n",
    "chunk_idx = 29  # zero-based\n",
    "record_id_30, start_30, end_30 = chunk_indices[chunk_idx]\n",
    "\n",
    "print(\"Chunk #30 details (zero-based index = 29):\")\n",
    "print(\"  record_id:\", record_id_30)\n",
    "print(\"  start:\", start_30)\n",
    "print(\"  end:  \", end_30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737078009.848599     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1737078010.001964     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1737078010.002015     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1737078010.005459     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1737078010.005509     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1737078010.005534     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1737078010.272699     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1737078010.272767     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 18:40:10.272779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1737078010.272826     745 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:04:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 18:40:10.274275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_30th_np shape: (5000, 4)\n",
      "y_30th_np shape: (5000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 18:40:10.809666: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def parse_tfrecord(example_proto, chunk_size=5000):\n",
    "    feature_spec = {\n",
    "        'X': tf.io.FixedLenSequenceFeature([], dtype=tf.float32, allow_missing=True),\n",
    "        'y': tf.io.FixedLenSequenceFeature([], dtype=tf.int64, allow_missing=True),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "    X = parsed_features['X']\n",
    "    y = parsed_features['y']\n",
    "    \n",
    "    X = tf.reshape(X, [chunk_size, 4])\n",
    "    y = tf.reshape(y, [chunk_size])\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(test_tfrecord_path)  # no compression\n",
    "dataset = dataset.map(lambda x: parse_tfrecord(x, chunk_size=chunk_size))\n",
    "\n",
    "# Skip 29 examples, then take 1\n",
    "dataset_30th = dataset.skip(chunk_idx).take(1)\n",
    "\n",
    "# Pull it into Python\n",
    "for X_30th, y_30th in dataset_30th:\n",
    "    X_30th_np = X_30th.numpy()\n",
    "    y_30th_np = y_30th.numpy()\n",
    "\n",
    "print(\"X_30th_np shape:\", X_30th_np.shape)\n",
    "print(\"y_30th_np shape:\", y_30th_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual shapes: (5000, 4) (5000,)\n"
     ]
    }
   ],
   "source": [
    "from pyfaidx import Fasta\n",
    "\n",
    "fasta = Fasta(fasta_file)\n",
    "seq_30 = fasta[record_id_30][start_30:end_30].seq\n",
    "\n",
    "gtf_data = load_gtf_annotations(gtf_file)\n",
    "sub_anno_30 = gtf_data[\n",
    "    (gtf_data['seqname'] == record_id_30) &\n",
    "    (gtf_data['start'] < end_30) &\n",
    "    (gtf_data['end'] > start_30)\n",
    "].copy()\n",
    "\n",
    "sub_anno_30['start'] -= start_30\n",
    "sub_anno_30['end']   -= start_30\n",
    "\n",
    "encoded_30 = one_hot_encode_with_N(seq_30)\n",
    "labels_30  = label_sequence(len(seq_30), sub_anno_30)\n",
    "\n",
    "encoded_30 = pad_one_hot_sequence(encoded_30, chunk_size)\n",
    "labels_30  = pad_labels(labels_30, chunk_size)\n",
    "\n",
    "X_30th_manual = np.array(encoded_30, dtype=np.float32)\n",
    "y_30th_manual = np.array(labels_30,  dtype=np.int32)\n",
    "\n",
    "print(\"Manual shapes:\", X_30th_manual.shape, y_30th_manual.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X array match? True\n",
      "y array match? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "same_X = np.array_equal(X_30th_np, X_30th_manual)\n",
    "same_y = np.array_equal(y_30th_np, y_30th_manual)\n",
    "\n",
    "print(f\"X array match? {same_X}\")\n",
    "print(f\"y array match? {same_y}\")\n",
    "\n",
    "# If you want a more flexible comparison (allow floating rounding errors):\n",
    "# np.allclose(X_30th_np, X_30th_manual, atol=1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import concurrent.futures\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "def load_gtf_annotations(gtf_file):\n",
    "    gtf_data = pd.read_csv(\n",
    "        gtf_file, sep='\\t', comment='#', header=None,\n",
    "        names=['seqname', 'source', 'feature', 'start', 'end', \n",
    "               'score', 'strand', 'frame', 'attribute']\n",
    "    )\n",
    "    gtf_data['start'] = gtf_data['start'] - 1  # zero-based\n",
    "    return gtf_data\n",
    "\n",
    "def compute_chunk_indices(fasta_file, chunk_size):\n",
    "    fa = Fasta(fasta_file)\n",
    "    chunk_indices = []\n",
    "    idx = 0\n",
    "    for record_id in fa.keys():\n",
    "        seq_len = len(fa[record_id])\n",
    "        for start in range(0, seq_len, chunk_size):\n",
    "            end = min(start + chunk_size, seq_len)\n",
    "            chunk_indices.append((idx, record_id, start, end))\n",
    "            idx += 1\n",
    "    return chunk_indices\n",
    "\n",
    "NUC_ENCODING = {\n",
    "    'A': [1, 0, 0, 0],\n",
    "    'C': [0, 1, 0, 0],\n",
    "    'G': [0, 0, 1, 0],\n",
    "    'T': [0, 0, 0, 1],\n",
    "    'N': [0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "def one_hot_encode_with_N(sequence):\n",
    "    return [NUC_ENCODING.get(n, [0,0,0,0]) for n in sequence]\n",
    "\n",
    "def label_sequence(seq_len, annotations):\n",
    "    labels = [0]*seq_len\n",
    "    for _, row in annotations.iterrows():\n",
    "        start = max(0, int(row['start']))\n",
    "        end   = min(seq_len, int(row['end']))\n",
    "        feat  = row['feature']\n",
    "        if feat == 'exon':\n",
    "            if start < seq_len:\n",
    "                labels[start] = 3\n",
    "            if (end - 1) < seq_len and (end-1) >= 0:\n",
    "                labels[end-1] = 4\n",
    "            for i in range(start+1, end-1):\n",
    "                if 0 <= i < seq_len:\n",
    "                    labels[i] = 6\n",
    "        elif feat == 'intron':\n",
    "            if start < seq_len:\n",
    "                labels[start] = 1\n",
    "            if (end - 1) < seq_len and (end-1) >= 0:\n",
    "                labels[end-1] = 2\n",
    "            for i in range(start+1, end-1):\n",
    "                if 0 <= i < seq_len:\n",
    "                    labels[i] = 5\n",
    "    return labels\n",
    "\n",
    "def pad_one_hot_sequence(encoded_seq, target_len):\n",
    "    pad_size = target_len - len(encoded_seq)\n",
    "    if pad_size > 0:\n",
    "        encoded_seq += [[0,0,0,0]]*pad_size\n",
    "    return encoded_seq\n",
    "\n",
    "def pad_labels(labels, target_len):\n",
    "    pad_size = target_len - len(labels)\n",
    "    if pad_size > 0:\n",
    "        labels += [0]*pad_size\n",
    "    return labels\n",
    "\n",
    "def serialize_example(X, y, chunk_idx, record_id, start, end):\n",
    "    \"\"\"\n",
    "    Store:\n",
    "      - X, y\n",
    "      - chunk_idx (int)\n",
    "      - record_id (string)\n",
    "      - start, end (int)\n",
    "    \"\"\"\n",
    "    # Flatten X for storage\n",
    "    X_flat = X.reshape(-1).tolist()\n",
    "    y_flat = y.tolist()\n",
    "\n",
    "    # Build feature dict\n",
    "    feature = {\n",
    "        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X_flat)),\n",
    "        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=y_flat)),\n",
    "        'chunk_idx': tf.train.Feature(int64_list=tf.train.Int64List(value=[chunk_idx])),\n",
    "        'record_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[record_id.encode()])),\n",
    "        'start': tf.train.Feature(int64_list=tf.train.Int64List(value=[start])),\n",
    "        'end':   tf.train.Feature(int64_list=tf.train.Int64List(value=[end])),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def _write_shard(\n",
    "    shard_id,\n",
    "    shard_indices,\n",
    "    fasta_file,\n",
    "    gtf_data,\n",
    "    chunk_size,\n",
    "    out_dir,\n",
    "    compression=\"GZIP\"\n",
    "):\n",
    "    fa = Fasta(fasta_file)\n",
    "    shard_fname = os.path.join(out_dir, f\"shard_{shard_id:04d}.tfrecord\")\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression)\n",
    "    with tf.io.TFRecordWriter(shard_fname, options=options) as writer:\n",
    "        for (chunk_idx, record_id, start, end) in shard_indices:\n",
    "            seq = fa[record_id][start:end].seq\n",
    "            sub_anno = gtf_data[\n",
    "                (gtf_data['seqname'] == record_id) &\n",
    "                (gtf_data['start'] < end) &\n",
    "                (gtf_data['end'] > start)\n",
    "            ].copy()\n",
    "            sub_anno['start'] -= start\n",
    "            sub_anno['end']   -= start\n",
    "\n",
    "            encoded = one_hot_encode_with_N(seq)\n",
    "            labels  = label_sequence(len(seq), sub_anno)\n",
    "\n",
    "            # pad\n",
    "            encoded = pad_one_hot_sequence(encoded, chunk_size)\n",
    "            labels  = pad_labels(labels, chunk_size)\n",
    "\n",
    "            X = np.array(encoded, dtype=np.float32)\n",
    "            y = np.array(labels,  dtype=np.int32)\n",
    "\n",
    "            example = serialize_example(X, y, chunk_idx, record_id, start, end)\n",
    "            writer.write(example)\n",
    "\n",
    "    return shard_fname\n",
    "\n",
    "\n",
    "def parallel_write_tfrecords(\n",
    "    fasta_file,\n",
    "    gtf_file,\n",
    "    chunk_size,\n",
    "    out_dir,\n",
    "    num_shards=4,\n",
    "    compression=\"GZIP\"\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    gtf_data = load_gtf_annotations(gtf_file)\n",
    "    chunk_indices = compute_chunk_indices(fasta_file, chunk_size)\n",
    "    np.random.shuffle(chunk_indices)  # optional\n",
    "\n",
    "    shard_size = math.ceil(len(chunk_indices) / num_shards)\n",
    "    shard_splits = [\n",
    "        chunk_indices[i * shard_size : (i+1)*shard_size]\n",
    "        for i in range(num_shards)\n",
    "    ]\n",
    "\n",
    "    futures = []\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_shards) as executor:\n",
    "        for shard_id, shard_idxs in enumerate(shard_splits):\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    _write_shard,\n",
    "                    shard_id,\n",
    "                    shard_idxs,\n",
    "                    fasta_file,\n",
    "                    gtf_data,\n",
    "                    chunk_size,\n",
    "                    out_dir,\n",
    "                    compression\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    results = []\n",
    "    for f in concurrent.futures.as_completed(futures):\n",
    "        results.append(f.result())\n",
    "\n",
    "    print(\"Shards written:\")\n",
    "    for r in results:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto, chunk_size=5000):\n",
    "    feature_spec = {\n",
    "        'X':         tf.io.FixedLenSequenceFeature([], dtype=tf.float32, allow_missing=True),\n",
    "        'y':         tf.io.FixedLenSequenceFeature([], dtype=tf.int64, allow_missing=True),\n",
    "        'chunk_idx': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'record_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'start':     tf.io.FixedLenFeature([], tf.int64),\n",
    "        'end':       tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "    \n",
    "    X = parsed['X']\n",
    "    y = parsed['y']\n",
    "    chunk_idx = parsed['chunk_idx']\n",
    "    record_id = parsed['record_id']\n",
    "    start     = parsed['start']\n",
    "    end       = parsed['end']\n",
    "\n",
    "    # Reshape X, y\n",
    "    X = tf.reshape(X, [chunk_size, 4])\n",
    "    y = tf.reshape(y, [chunk_size])\n",
    "\n",
    "    return (X, y, chunk_idx, record_id, start, end)\n",
    "\n",
    "\n",
    "def build_dataset_from_tfrecord_shards(\n",
    "    shard_pattern, \n",
    "    chunk_size=5000, \n",
    "    compression=\"GZIP\"\n",
    "):\n",
    "    dataset = tf.data.Dataset.list_files(shard_pattern, shuffle=True)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda fp: tf.data.TFRecordDataset(fp, compression_type=compression),\n",
    "        cycle_length=4,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        lambda x: parse_tfrecord(x, chunk_size),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: tfrecord_shards/shard_*.tfrecord'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m sharded_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataset_from_tfrecord_shards\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshard_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtfrecord_shards/shard_*.tfrecord\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGZIP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m target_chunk_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     23\u001b[0m result \u001b[38;5;241m=\u001b[39m find_chunk(sharded_ds, target_chunk_idx)\n",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m, in \u001b[0;36mbuild_dataset_from_tfrecord_shards\u001b[0;34m(shard_pattern, chunk_size, compression)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dataset_from_tfrecord_shards\u001b[39m(\n\u001b[1;32m     27\u001b[0m     shard_pattern, \n\u001b[1;32m     28\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, \n\u001b[1;32m     29\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGZIP\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m ):\n\u001b[0;32m---> 31\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39minterleave(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m fp: tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTFRecordDataset(fp, compression_type\u001b[38;5;241m=\u001b[39mcompression),\n\u001b[1;32m     34\u001b[0m         cycle_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     35\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: parse_tfrecord(x, chunk_size),\n\u001b[1;32m     39\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[1;32m     40\u001b[0m     )\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:1320\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[0;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[1;32m   1313\u001b[0m condition \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mgreater(array_ops\u001b[38;5;241m.\u001b[39mshape(matching_files)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1314\u001b[0m                              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_not_empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1316\u001b[0m message \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files matched pattern: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1318\u001b[0m     string_ops\u001b[38;5;241m.\u001b[39mreduce_join(file_pattern, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1320\u001b[0m assert_not_empty \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_assert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massert_not_empty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assert_not_empty]):\n\u001b[1;32m   1323\u001b[0m   matching_files \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(matching_files)\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Deep Learning Projects/venv/lib/python3.10/site-packages/tensorflow/python/ops/control_flow_assert.py:102\u001b[0m, in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    100\u001b[0m     xs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_n_to_tensor(data)\n\u001b[1;32m    101\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m [_summarize_eager(x, summarize) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m    103\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be true. Summarized data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    106\u001b[0m         (condition, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_str)))\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssert\u001b[39m\u001b[38;5;124m\"\u001b[39m, [condition, data]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: tfrecord_shards/shard_*.tfrecord'"
     ]
    }
   ],
   "source": [
    "def find_chunk(dataset, target_idx):\n",
    "    \"\"\"\n",
    "    Scans the dataset until it finds the record where chunk_idx == target_idx.\n",
    "    Returns (X, y, record_id, start, end).\n",
    "    \"\"\"\n",
    "    for X, y, chunk_idx, record_id, start, end in dataset:\n",
    "        # chunk_idx, record_id, etc., are tf.Tensor objects.\n",
    "        if int(chunk_idx.numpy()) == target_idx:\n",
    "            return (X.numpy(), y.numpy(),\n",
    "                    record_id.numpy().decode('utf-8'),\n",
    "                    int(start.numpy()),\n",
    "                    int(end.numpy()))\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "sharded_ds = build_dataset_from_tfrecord_shards(\n",
    "    shard_pattern=\"tfrecord_shards/shard_*.tfrecord\",\n",
    "    chunk_size=5000,\n",
    "    compression=\"GZIP\"\n",
    ")\n",
    "\n",
    "target_chunk_idx = 30\n",
    "result = find_chunk(sharded_ds, target_chunk_idx)\n",
    "if result is None:\n",
    "    print(f\"Could not find chunk_idx={target_chunk_idx}\")\n",
    "else:\n",
    "    X_array, y_array, rec_id_str, start_val, end_val = result\n",
    "    print(f\"Found chunk_idx={target_chunk_idx}, record_id={rec_id_str}, start={start_val}, end={end_val}\")\n",
    "    print(\"X_array shape:\", X_array.shape)  # (5000, 4)\n",
    "    print(\"y_array shape:\", y_array.shape)  # (5000,)\n",
    "\n",
    "    # For the \"first 1000 bases\"  that corresponds to X_array[:1000, :]\n",
    "    # It's a one-hot. Let's decode or compare to the original FASTA to confirm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfaidx import Fasta\n",
    "\n",
    "def compare_chunk(\n",
    "    X_array, y_array,   # from TFRecord\n",
    "    record_id, start, end,\n",
    "    fasta_file, gtf_data,\n",
    "    chunk_size=5000,\n",
    "    n_bases=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare the first n_bases in X_array to the actual FASTA substring.\n",
    "    Also check the labeling vs. GTF if desired.\n",
    "    \"\"\"\n",
    "    fa = Fasta(fasta_file)\n",
    "    seq = fa[record_id][start:end].seq  # original substring\n",
    "    # If we want only the first n_bases, slice:\n",
    "    seq_substring = seq[:n_bases]  \n",
    "    print(\"FASTA substring (first 1000 if that many):\", seq_substring)\n",
    "\n",
    "    # Convert X_array[:n_bases] from one-hot back to letters to see if they match\n",
    "    # your original substring.\n",
    "    one_hot_slice = X_array[:n_bases]  # shape (1000, 4)\n",
    "    decoded = []\n",
    "    for row in one_hot_slice:\n",
    "        # row is [A, C, G, T], find which index = 1\n",
    "        # but here we can handle partial or zeros\n",
    "        a, c, g, t = row\n",
    "        if a == 1.0 and c==0.0 and g==0.0 and t==0.0:\n",
    "            decoded.append('A')\n",
    "        elif a==0.0 and c==1.0 and g==0.0 and t==0.0:\n",
    "            decoded.append('C')\n",
    "        elif a==0.0 and c==0.0 and g==1.0 and t==0.0:\n",
    "            decoded.append('G')\n",
    "        elif a==0.0 and c==0.0 and g==0.0 and t==1.0:\n",
    "            decoded.append('T')\n",
    "        else:\n",
    "            decoded.append('N')  # or 'N' if all zeros\n",
    "\n",
    "    decoded_str = ''.join(decoded)\n",
    "    print(\"Decoded from one-hot (first 1000 bases):\", decoded_str)\n",
    "\n",
    "    # Check if it matches:\n",
    "    if decoded_str == seq_substring.upper():\n",
    "        print(\"First 1000 bases match perfectly!\")\n",
    "    else:\n",
    "        print(\"Mismatch found between TFRecord data and FASTA substring.\")\n",
    "\n",
    "\n",
    "# Suppose you found chunk_idx=30 in the TFRecord dataset, and got:\n",
    "# X_array, y_array, rec_id_str, start_val, end_val = ...\n",
    "gtf_data = load_gtf_annotations(\"annotations.gtf\")  # or reuse if you have it\n",
    "compare_chunk(\n",
    "    X_array, y_array,\n",
    "    rec_id_str, start_val, end_val,\n",
    "    fasta_file=\"genome.fa\", \n",
    "    gtf_data=gtf_data,\n",
    "    chunk_size=5000,\n",
    "    n_bases=1000\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
